

## Milestone 4: Model Training

---
# Text Processing Part

## Overview / Objective
This milestone details the implementation and initial experiments for a two-stage medical note synthesis system combining large and small language models. The objective is to develop an automated pipeline starting from doctor-patient textual interactions (after speech-to-text conversion), synthesizing both technical SOAP-style notes for clinicians and plain language summaries for patients. It covers the setup, training, optimization, and early performance analysis for the core modules responsible for clinical documentation and layperson-friendly summarization.

Link to previous milestone: Refer to previous documentation for data preprocessing details, architecture rationale, etc. The current milestone builds on the previously defined workflow, now introducing model training evaluation. (We have not spent much time yet on hyperparameter tuning)
## Dataset Details

Primary dataset consists of JSONL records, where each entry encodes the output of a simulated doctor-patient interaction (after ASR), a synthetic SOAP-form report, and a teacher-generated patient-friendly summary. The SOAP form output was generated by this model shared in Kaggle, and this model was used as is for this particular task: https://www.kaggle.com/code/srijitsengupta06/autosoap-ai-powered-clinical-documentation/ This model as such has not been described further in this document.
### Sources & Splits:
 - AutoSOAP uses a variety of sources, including real and simulated patient dialogues. These are not described further. The results are stored in a csv file with these fields: conversation_id,source,patient_input,doctor_response,soap_note,soap_components,completeness_scores,quality_metrics
 - Teacher model takes the output of the AutoSOAP module and in particular the soap_components field. It outputs an ‘Expert generated’ plain text summary.
After experimentation this version creates summaries for all available SOAP notes that pass some criteria. About 960 records are used.
 - Student model is trained with the outputs from the Teacher model. This version uses a 50:50 train test split.
### Input Types: 
	Text only (speech converted to text by a separate module).
### Preprocessing:
 - Extract assessment and plan from doctor's notes
 - Construct prompts (with different level of details) for both teacher and student model training
 - Standardized lowercasing, tokenization, and error-handling for missing values
### Handling Missing/Imbalanced Data:
 - Cases with missing assessment or plan are filtered out (this
   reduces data from 1000 samples to a bit above 900)
  - NaN or blank    predictions during experiments are logged for error analysis and
   excluded from metric calculations

## Model Architecture
An initial image is below. This would be refined further



<img width="877" height="571" alt="textmodel_architecture" src="https://github.com/user-attachments/assets/35b71e6f-f0db-4a86-9fd0-0e0d3e113d1b" />


### Modules

- Teacher Model: Mistral-7B, pretrained transformer, receives constructed prompt with SOAP content and generates quality labels for patient-friendly summary. This approach has been taken to create quality labels for the data.
- Student Model: TinyLlama-1.1B-Chat, LoRA-wrapped for parameter-efficient fine-tuning. This would be used for inference and is small enough to run as an endpoint
### Input/Output Shapes:
- For Teacher model: Input: Text prompt(“[INST] <<SYS>> You are a medical communication assistant. Your task is to combine the Assessment and Plan sections of a clinical note into a clear, patient-friendly summary. Use simple language, avoid jargon, and clearly explain the doctor's conclusions (Assessment) and recommendations (Plan). Keep it concise and empathetic. <</SYS>> ### Clinical Context: - **Patient says**: {subjective} **Tests/findings**: {objective} - **Doctor's assessment**: {assessment} - **Recommended plan**: {plan} Explain the doctor's assessment and plan in a way that a patient can understand. Use simple language, and describe what the doctor concluded, what the next steps are, and how they help. Limit to 8-10 sentences. [/INST] )
○
- For Student model: Input: Text prompt ("Assessment:\n...\nPlan:\n...\nRewrite the above...")
- Output: One-paragraph plain-language summary (Output from Teacher model are uses as labels to train Student model)

### Architecture Choice:
- LLMs chosen for their contextual reasoning capabilities, matching Milestone 3 rationale for generative document synthesis
### Pretrained Weights / Fine-Tuning:
- Teacher uses off-the-shelf Mistral-7B, tuned via prompt design
- Student is fine-tuned with LoRA adapters on ~470 teacher-labeled examples
### Training Setup
- Loss/Metric: Cross-entropy (causal LM) trained to match teacher summaries; evaluated with ROUGE-L, BERTScore, and medical term recall
- Optimizer/Schedule: AdamW optimizer (learning rate grid-searched); fixed vs scheduled learning rate explored
- Batch Size / Epochs / Hardware: Batch size: 1 (due to GPU memory); 3-5 epochs, Kaggle T4 GPU (16GB)
### Training Strategies: 
Regular checkpoint saving (more strategies to be listed)
## Hyperparameter Experiments
### Explored:
- Learning rate: To be described
- Batch size: Has been 1 due to memory limitation
- Dropout: 0.1
- Weight decay: 0.05

### Results Table: 
Performance improved with moderate dropout and clipped gradients. Smaller batch size reduced NaN outputs but affected throughput.
### Observation: 
Trade-off between context window/max tokens (memory) and prediction completeness. LoRA efficiently enabled smaller model to mimic teacher output.

## Regularization & Optimization
### Techniques Used:
- LoRA for efficient fine-tuning
- Dropout (0.1), to reduce overfitting in a small dataset
- Weight decay and norm clipping for stability
- 
### Effect: 
Prevented abrupt divergence in training loss. Reduced overfitting and improved generalization to validation/test data. Dropout especially helpful given synthetic, relatively homogeneous data.

## Initial Training Results
- Curves: Loss decreased steadily over steps
- (to be included)
- 
- Examples:
-- Technical output: "Assessment: Pain in the tooth is also caused due to periapical reasons like periapical cysts, abscess, granuloma, periodontal or gingival infection. I would suggest you to get an X RAY examination done for the affected tooth which will help in diagnosing the cause of pain in the tooth accordingly treatment can be carried out Plan: I would suggest you to get an X RAY examination done for the affected tooth which will help in diagnosing the cause of pain in the tooth accordingly treatment can be carried out. Approach a dentist and get the evaluation done, decayed teeth are treated with root canal treatment to relive pain permanently”
  
-- Patient summary: “Dear Patient, I'm sorry to hear about your tooth pain. It's common for people to experience pain in their teeth when they have a problem with the root or the crown. This is because the tooth is a vital part of your body, and it's essential to take care of it. To help you, I'd recommend seeing a dentist. A dentist can perform an X-ray examination to diagnose the cause of your tooth pain. This will help them determine the best treatment for you. If the dentist finds that your tooth is causing pain, they may recommend a root canal treatment. This is a procedure that involves removing the infected part of the tooth and cleaning it to prevent future infections. The root canal treatment is a painless procedure that can help relieve your tooth pain and prevent further damage to your tooth. I recommend seeing a dentist as soon as possible to get the root canal treatment. This will help you get back to your normal life and prevent further damage to your tooth. I hope this helps. Please let me know if you have any other questions."

### Observed Behavior: 
Some underfitting at lowest LR; slight overfitting with high LR and zero dropout. NaN predictions correlated with input length/truncation and batch size.

## Metrics - Initial
- For evaluation, both train and test sets were used
- Model was set to eval mode and inference done
- A major issue is the empty predictions
- For all metrics, there is not much difference between train and test sets
- This implies that the model is not overfitting
|Metric		| Train  	| 	Test |
| Total | 471.000000 |472.000000|
| Valid |279.000000 |265.000000 |
|NaN_rate| 0.407643 | 0.438559|
| ROUGE-L | 0.237754 | 0.232926 |
| BERTScore_F1 | 0.876159 | 0.875688 }
} Exact_Match | 0.000000 | 0.000000 |
| Medical_Retain  | 0.434783 | 0.645161 |
| Hallucination_rate | 0.007168 | 0.007547 |
|  |  |

## Model Artifacts
- Saved output from AutoSOAP model and used it for Teacher Training
- Saved TinyLlama-1.1B-Chat weights and LoRA adapter
- Mistral-7B summary scripts and teacher output cache
- Notebooks for data preparation, prompt construction, training, and evaluation
- Logs of batch-wise training and validation metrics
## Observations / Notes for Next Milestone
### Performance:
- High semantic similarity (BERTScore >0.88) but moderate ROUGE-L (0.27); 
- NaN rate reduced after batch size tuning and prompt handling, and needs further work
### Issues:
- Long inputs and aggressive prompt stripping caused blank outputs
- Some missing assessment/plan fields in raw data necessitated better filtering
- GPU memory limits constrained batch size and context, impacting completeness
### Next Steps:
- Further tuning to optimize context size and batch handling
- Implement more robust postprocessing to mitigate blank/NaN outputs

- Plan thorough error analysis, and other evaluation methods for Milestone 5

---
---

# Speech Processing Part

## Overview / Objective
This milestone clarifies the input and routing flow: inputs can be speech, text, or both, provided by either a doctor or a patient. When speech is present, Whisper Small is used to transcribe to text; otherwise text is used as-is. The resulting text is routed to an LLM that behaves as either a Doctor Assistant or a Patient Assistant based on the selected role. We establish the ASR pipeline and its integration points (dataset prep, preprocessing, model config, initial eval) to support this role-aware flow.

Flow:
- Input (speech and/or text) from Doctor or Patient
- If speech: Whisper → transcript; merge with any provided text
- Role selection: Doctor Assist or Patient Assist
- LLM generates role-specific output (SOAP/counseling for doctor; plain-language guidance for patient)

Link to previous context: Builds on the earlier data preprocessing and architecture rationale. Focus here is speech (ASR) with Whisper Small for practicality and deployability.

## Dataset Details

- Input type: Single-channel conversational clinical audio (doctor–patient style)
- Sampling: 16 kHz target (resampled where necessary)
- Formats: WAV/MP3 supported via soundfile/torchaudio
- Splits: Train/validation/test prepared in notebook as needed
- Notebook reference: notebooks/speech/data-prep.ipynb

### Sources & IO Stack
- Libraries: transformers, datasets, torchaudio, soundfile, accelerate, huggingface_hub, jiwer (for WER/CER), ffmpeg (I/O)
- Files organized for batched evaluation and logging

### Preprocessing
- Resampling to 16 kHz
- Waveform normalization and trimming (optional silence handling)
- Feature extraction via WhisperProcessor (log-Mel spectrograms)
- Label/normalization for evaluation (lowercasing, punctuation handling for WER/CER)

## Model Architecture

- Base Model: Whisper Small (openai/whisper-small)
- Components: WhisperForConditionalGeneration + WhisperProcessor
- Decoding: Beam search with temperature control; timestamps disabled for text-only downstream
- Configuration: Language set to English; task="transcribe"
- Output: Clean transcript per utterance for downstream text pipeline

### Rationale
- Whisper Small balances accuracy, speed, and resource usage (T4-class GPU) for demos and iterative experiments

## Training Setup

- Regime: Zero-shot baseline with optional light fine-tuning on in-domain clips
- Hardware: Single GPU (T4-class)
- Precision: FP16 where available
- Batch size / Epochs: Tuned to fit memory; gradient accumulation as needed
- Optimizer: AdamW
- Logging: Optional Weights & Biases; local logs saved

## Hyperparameter Experiments (Initial)

- Beam width: small grid (e.g., 1, 3, 5) to trade speed vs accuracy
- Temperature/top-p: tuned only if sampling used (default beam preferred for ASR)
- Chunking/stride: explored for long-form audio stability
- Normalization: multiple text normalization variants benchmarked for WER impact

### Results Snapshot (Qualitative)
- Clean audio: good readability
- Challenging audio: errors on drug names/medical terms; fast speech and overlaps reduce accuracy

## Regularization & Optimization

- Data augmentation (future): time/frequency masking, noise injection for robustness
- Grad norm clipping and small weight decay when fine-tuning

## Initial Training/Evaluation Results

- Baseline: Zero-shot Whisper Small on clinical-style snippets
- Metrics: WER and CER via jiwer with consistent text normalization
- Observed: domain terminology and overlapping speech are primary error sources

## Metrics – Initial

- WER/CER reported per split; store predictions and references for reproducibility
- Track effect of decoding settings (beam vs greedy) and normalization choices

## Model Artifacts

- Saved configs and processor settings for reproducibility
- Optional fine-tuned checkpoints (if run) and decoding configs
- Batch predictions and references CSV/JSON for metric computation
- Notebook: notebooks/speech/data-prep.ipynb

## Observations / Notes for Next Milestone

### Performance
- Whisper Small is adequate for demo; in-domain fine-tuning expected to reduce WER on medical vocabulary

### Issues
- Multi-speaker overlap and background noise
- Domain-specific terminology recognition (drug names, diagnoses)

### Next Steps
- Light fine-tuning on curated clinical clips
- Add domain lexicon/pronunciation hints and improved text normalization
- Evaluate diarization or turn-taking heuristics for cleaner role-aware transcripts
- Integrate ASR outputs with role-based prompts (Doctor Assist / Patient Assist) for end-to-end testing
