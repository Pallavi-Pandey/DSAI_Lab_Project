{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Inference code \n\n## inference on train set, and also test set\n## evaluate metrics","metadata":{}},{"cell_type":"code","source":"#first fix the device\n# inference is faster with GPU\n# obligatory device set up\n# Automatically select device: GPU 0 if available, else CPU\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() and torch.cuda.device_count() >= 1 else \"cpu\")\nprint(f\"Using devic\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# first the model\n\n\n\nfrom transformers import pipeline\n\n#infer_pipe = pipeline(\"text-generation\", model=\"distilled-student-peft-adapter\", \n#                      tokenizer=\"distilled-student-peft-adapter\", device=0)\n\n# need to use peft load  \nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import PeftModel\n\nmodel_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\nadapter_path = \"/kaggle/input/savedmodelsummarization/pytorch/default/1\"   # change as needed\n\ntokenizer = AutoTokenizer.from_pretrained(adapter_path)\nbase_model = AutoModelForCausalLM.from_pretrained(model_id)\n\n# IMPORTANT: Use is_trainable=True for proper adapter loading\nmodel = PeftModel.from_pretrained(base_model, adapter_path, is_trainable=True).to(device)\n\nprint(\"Trainable params after load:\")\nmodel.print_trainable_parameters()\nprint(\"Compare with previous data to check match...\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### sample inference\n### check reasonableness before batch inference","metadata":{}},{"cell_type":"code","source":"import torch\n# Inference example\n\nprompt = f\"\"\"Assessment: You have typhoiditis\nPlan: Take paracetamol and acetenomycin\nRewrite the above for a patient with no medical background.\"\"\"\n\n\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\nwith torch.no_grad():\n    gen_output = model.generate(\n        **inputs,\n        max_new_tokens=256,\n        do_sample=True,\n        top_p=0.95,\n        temperature=0.7,\n        eos_token_id=tokenizer.eos_token_id\n    )\n\n\n# Full output includes the prompt + generated text\nfull_output = tokenizer.decode(gen_output[0], skip_special_tokens=True)\n\n# Extract only the generated part (remove prompt)  - \ngenerated_text = full_output[len(prompt):].strip()\n\nprint(\"Prompt: \", prompt)\nprint(\"\\nGenerated summary:\")\nprint(generated_text)\n#print(\"\\n\\n\")\n#print(full_output)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Above code can be reused with any Assessment and Plan text\n","metadata":{}}]}