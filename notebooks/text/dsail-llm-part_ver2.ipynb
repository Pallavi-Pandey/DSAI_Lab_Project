{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":3696968,"sourceType":"datasetVersion","datasetId":2211956},{"sourceId":4805127,"sourceType":"datasetVersion","datasetId":2782228},{"sourceId":7045374,"sourceType":"datasetVersion","datasetId":4054084},{"sourceId":7045630,"sourceType":"datasetVersion","datasetId":4054274},{"sourceId":7057378,"sourceType":"datasetVersion","datasetId":4062409},{"sourceId":7562666,"sourceType":"datasetVersion","datasetId":4403558},{"sourceId":8870083,"sourceType":"datasetVersion","datasetId":5338273},{"sourceId":10301988,"sourceType":"datasetVersion","datasetId":6376623},{"sourceId":11557167,"sourceType":"datasetVersion","datasetId":7246557},{"sourceId":13584377,"sourceType":"datasetVersion","datasetId":8630490},{"sourceId":13586665,"sourceType":"datasetVersion","datasetId":8632085}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Leveraged AutoSOAP code","metadata":{"_kg_hide-input":true}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport json\nfrom pathlib import Path\nimport numpy as np\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# First, let's explore the actual Kaggle input directory structure\ndef explore_kaggle_input():\n    \"\"\"Explore what's actually available in /kaggle/input/\"\"\"\n    input_dir = Path('/kaggle/input')\n    \n    if not input_dir.exists():\n        print(\"‚ùå /kaggle/input directory not found\")\n        return []\n    \n    print(\"üìÅ Available datasets in /kaggle/input/:\")\n    available_datasets = []\n    \n    for item in input_dir.iterdir():\n        if item.is_dir():\n            print(f\"  üìÇ {item.name}\")\n            available_datasets.append(str(item))\n            \n            # Show files in each dataset directory\n            try:\n                files = list(item.rglob('*'))\n                files = [f for f in files if f.is_file()]\n                print(f\"     Files: {len(files)} total\")\n                \n                # Show first few files\n                for i, file in enumerate(files[:3]):\n                    print(f\"     - {file.name}\")\n                if len(files) > 3:\n                    print(f\"     ... and {len(files)-3} more files\")\n                    \n            except Exception as e:\n                print(f\"     Error reading directory: {e}\")\n                \n    return available_datasets\n\n# Explore available datasets\navailable_datasets = explore_kaggle_input()\nprint(f\"\\n‚úÖ Found {len(available_datasets)} dataset directories\")\n\n\nimport pandas as pd\nimport json\nfrom pathlib import Path\nimport numpy as np\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Create project structure\nproject_dirs = [\n    'AutoSOAP/data/raw',\n    'AutoSOAP/data/processed', \n    'AutoSOAP/notebooks',\n    'AutoSOAP/scripts',\n    'AutoSOAP/outputs'\n]\n\nfor dir_path in project_dirs:\n    Path(dir_path).mkdir(parents=True, exist_ok=True)\n    \nprint(\"‚úÖ Project structure created!\")\n\n# Dataset exploration function\ndef safe_load_dataset(dataset_name, base_path):\n    \"\"\"Safely load and explore datasets with error handling\"\"\"\n    print(f\"\\n{'='*60}\")\n    print(f\"üìä EXPLORING: {dataset_name}\")\n    print(f\"{'='*60}\")\n    \n    try:\n        # List all files in the directory\n        files = list(base_path.glob('*'))\n        print(f\"üìÅ Files found: {len(files)}\")\n        \n        for i, file in enumerate(files[:5]):  # Show first 5 files\n            print(f\"  {i+1}. {file.name} ({file.stat().st_size} bytes)\")\n        \n        if len(files) > 5:\n            print(f\"  ... and {len(files)-5} more files\")\n            \n        return files\n        \n    except Exception as e:\n        print(f\"‚ùå Error exploring {dataset_name}: {str(e)}\")\n        return []\n\n# Start with the datasets we know have files\ndatasets_info = {\n    'mental-health-corpus': '/kaggle/input/mental-health-corpus',\n    'nlp-mental-health-conversations': '/kaggle/input/nlp-mental-health-conversations', \n    'medical-conversation-corpus-100k': '/kaggle/input/medical-conversation-corpus-100k',\n    'human-and-llm-mental-health-conversations': '/kaggle/input/human-and-llm-mental-health-conversations',\n    'healthcare-appointment-booking-calls-dataset': '/kaggle/input/healthcare-appointment-booking-calls-dataset',\n    'chatdoctor': '/kaggle/input/chatdoctor',\n    'sentiment-analysis-for-mental-health': '/kaggle/input/sentiment-analysis-for-mental-health',\n    'comprehensive-medical-q-a-dataset': '/kaggle/input/comprehensive-medical-q-a-dataset'\n}\n\n# Explore each dataset\ndataset_files = {}\nfor name, path in datasets_info.items():\n    base_path = Path(path)\n    files = safe_load_dataset(name, base_path)\n    dataset_files[name] = files\n\nprint(f\"\\nüéØ SUMMARY: Found files in {len([k for k, v in dataset_files.items() if v])} datasets\")\n\n\nimport os\nimport pandas as pd\nimport json\nfrom pathlib import Path\nimport glob\n\ndef find_actual_files():\n    \"\"\"Find all actual files in the Kaggle input directory\"\"\"\n    base_path = '/kaggle/input'\n    all_files = {}\n    \n    # Use glob to find all files recursively\n    for dataset_dir in os.listdir(base_path):\n        dataset_path = os.path.join(base_path, dataset_dir)\n        if os.path.isdir(dataset_path):\n            files = []\n            try:\n                # Find all files recursively\n                for root, dirs, filenames in os.walk(dataset_path):\n                    for filename in filenames:\n                        full_path = os.path.join(root, filename)\n                        if os.path.isfile(full_path) and os.path.getsize(full_path) > 0:\n                            files.append(full_path)\n                all_files[dataset_dir] = files\n            except Exception as e:\n                print(f\"Error accessing {dataset_dir}: {e}\")\n                all_files[dataset_dir] = []\n    \n    return all_files\n\n# Find all actual files\nactual_files = find_actual_files()\n\nprint(\"üìÅ ACTUAL FILES FOUND:\")\nfor dataset, files in actual_files.items():\n    print(f\"\\n{dataset}:\")\n    for file in files:\n        try:\n            size = os.path.getsize(file)\n            print(f\"  ‚úÖ {os.path.basename(file)} ({size:,} bytes)\")\n        except:\n            print(f\"  ‚ùå {os.path.basename(file)} (access error)\")\n\n\nimport pandas as pd\nimport json\nfrom pathlib import Path\nimport numpy as np\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Dataset loading and examination function\ndef load_and_examine_dataset(dataset_name, file_path, file_type='csv'):\n    \"\"\"Load and examine individual datasets\"\"\"\n    print(f\"\\n{'='*70}\")\n    print(f\"üìä EXAMINING: {dataset_name}\")\n    print(f\"üìÅ File: {Path(file_path).name}\")\n    print(f\"{'='*70}\")\n    \n    try:\n        if file_type == 'csv':\n            # Load CSV files\n            df = pd.read_csv(file_path)\n            print(f\"üìà Shape: {df.shape}\")\n            print(f\"üìã Columns: {list(df.columns)}\")\n            print(f\"üíæ Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n            \n            # Show sample data\n            print(f\"\\nüîç First 3 rows:\")\n            print(df.head(3).to_string())\n            \n            # Show data types\n            print(f\"\\nüìä Data types:\")\n            print(df.dtypes.to_string())\n            \n            # Check for missing values\n            missing = df.isnull().sum()\n            if missing.sum() > 0:\n                print(f\"\\n‚ö†Ô∏è Missing values:\")\n                print(missing[missing > 0].to_string())\n            \n            return df\n            \n        elif file_type == 'json':\n            # Load JSON files\n            with open(file_path, 'r', encoding='utf-8') as f:\n                data = json.load(f)\n            \n            print(f\"üìà Type: {type(data)}\")\n            if isinstance(data, list):\n                print(f\"üìà Length: {len(data)}\")\n                if len(data) > 0:\n                    print(f\"üìã Sample keys: {list(data[0].keys()) if isinstance(data[0], dict) else 'Not dict'}\")\n                    print(f\"\\nüîç First 2 entries:\")\n                    for i, item in enumerate(data[:2]):\n                        print(f\"Entry {i+1}: {str(item)[:300]}...\")\n            elif isinstance(data, dict):\n                print(f\"üìã Top-level keys: {list(data.keys())}\")\n                print(f\"\\nüîç Sample content:\")\n                for key, value in list(data.items())[:3]:\n                    print(f\"{key}: {str(value)[:200]}...\")\n                    \n            return data\n            \n    except Exception as e:\n        print(f\"‚ùå Error loading {dataset_name}: {str(e)}\")\n        return None\n\n# Start examining datasets systematically\ndatasets_info = {}\n\n# 1. Mental Health Corpus\nprint(\"üöÄ Starting dataset examination...\")\ndatasets_info['mental-health-corpus'] = load_and_examine_dataset(\n    'mental-health-corpus',\n    '/kaggle/input/mental-health-corpus/mental_health.csv',\n    'csv'\n)\n\n\n# 2. Medical Conversation Corpus 100K (Most promising for SOAP notes)\ndatasets_info['medical-conversation-100k'] = load_and_examine_dataset(\n    'medical-conversation-100k',\n    '/kaggle/input/medical-conversation-corpus-100k/train.csv',\n    'csv'\n)\n\n\n# 3. ChatDoctor JSON datasets (Large medical Q&A)\ndatasets_info['chatdoctor-healthcaremagic'] = load_and_examine_dataset(\n    'chatdoctor-healthcaremagic',\n    '/kaggle/input/chatdoctor/HealthCareMagic-100k.json',\n    'json'\n)\n\n\n# 4. Comprehensive Medical Q&A Dataset\ndatasets_info['comprehensive-medical-qa'] = load_and_examine_dataset(\n    'comprehensive-medical-qa',\n    '/kaggle/input/comprehensive-medical-q-a-dataset/train.csv',\n    'csv'\n)\n\n\n# 5. NLP Mental Health Conversations (might have dialogue structure)\ndatasets_info['nlp-mental-health'] = load_and_examine_dataset(\n    'nlp-mental-health',\n    '/kaggle/input/nlp-mental-health-conversations/train.csv',\n    'csv'\n)\n\n\nimport re\nimport pandas as pd\nimport json\nfrom typing import Dict, List, Tuple\nimport numpy as np\n\nclass AutoSOAPDataProcessor:\n    \"\"\"Data processor for converting medical dialogues to SOAP-ready format\"\"\"\n    \n    def __init__(self):\n        self.processed_data = {}\n        \n    def parse_medical_conversation(self, conversation_text: str) -> Dict:\n        \"\"\"Parse the medical conversation 100k format\"\"\"\n        try:\n            # Split by Human and AI markers\n            parts = re.split(r'\\[?\\|?(Human|AI)\\|?\\]?', conversation_text)\n            \n            dialogue = []\n            current_speaker = None\n            \n            for i, part in enumerate(parts):\n                part = part.strip()\n                if part in ['Human', 'AI']:\n                    current_speaker = 'Patient' if part == 'Human' else 'Doctor'\n                elif part and current_speaker:\n                    dialogue.append({\n                        'speaker': current_speaker,\n                        'text': part.strip()\n                    })\n                    \n            return {\n                'dialogue': dialogue,\n                'patient_input': dialogue[0]['text'] if dialogue and dialogue[0]['speaker'] == 'Patient' else '',\n                'doctor_response': dialogue[1]['text'] if len(dialogue) > 1 and dialogue[1]['speaker'] == 'Doctor' else ''\n            }\n        except Exception as e:\n            return {'dialogue': [], 'patient_input': '', 'doctor_response': '', 'error': str(e)}\n    \n    def parse_chatdoctor_format(self, entry: Dict) -> Dict:\n        \"\"\"Parse ChatDoctor JSON format\"\"\"\n        try:\n            return {\n                'patient_input': entry.get('input', ''),\n                'doctor_response': entry.get('output', ''),\n                'instruction': entry.get('instruction', ''),\n                'dialogue': [\n                    {'speaker': 'Patient', 'text': entry.get('input', '')},\n                    {'speaker': 'Doctor', 'text': entry.get('output', '')}\n                ]\n            }\n        except Exception as e:\n            return {'patient_input': '', 'doctor_response': '', 'error': str(e)}\n    \n    def process_medical_conversations_100k(self, df: pd.DataFrame, sample_size: int = 1000) -> List[Dict]:\n        \"\"\"Process the medical conversations 100k dataset\"\"\"\n        print(f\"üîÑ Processing Medical Conversations 100K (sample: {sample_size})...\")\n        \n        processed = []\n        sample_df = df.sample(n=min(sample_size, len(df)), random_state=42)\n        \n        for idx, row in sample_df.iterrows():\n            parsed = self.parse_medical_conversation(row['Conversation'])\n            if parsed['patient_input'] and parsed['doctor_response']:\n                parsed['source'] = 'medical-conv-100k'\n                parsed['index'] = idx\n                processed.append(parsed)\n        \n        print(f\"‚úÖ Successfully processed {len(processed)} conversations\")\n        return processed\n    \n    def process_chatdoctor_data(self, data: List[Dict], sample_size: int = 1000) -> List[Dict]:\n        \"\"\"Process ChatDoctor dataset\"\"\"\n        print(f\"üîÑ Processing ChatDoctor data (sample: {sample_size})...\")\n        \n        processed = []\n        sample_data = data[:sample_size] if len(data) > sample_size else data\n        \n        for idx, entry in enumerate(sample_data):\n            parsed = self.parse_chatdoctor_format(entry)\n            if parsed['patient_input'] and parsed['doctor_response']:\n                parsed['source'] = 'chatdoctor'\n                parsed['index'] = idx\n                processed.append(parsed)\n        \n        print(f\"‚úÖ Successfully processed {len(processed)} conversations\")\n        return processed\n\n# Initialize processor\nprocessor = AutoSOAPDataProcessor()\n\n# Process Medical Conversations 100K (sample for testing)\nprint(\"üöÄ Starting data processing pipeline...\")\nmedical_conv_processed = processor.process_medical_conversations_100k(\n    datasets_info['medical-conversation-100k'], \n    sample_size=500  # Start with smaller sample for testing\n)\n\n# Process ChatDoctor data (sample for testing)\nchatdoctor_processed = processor.process_chatdoctor_data(\n    datasets_info['chatdoctor-healthcaremagic'], \n    sample_size=500\n)\n\nprint(f\"\\nüìä PROCESSING SUMMARY:\")\nprint(f\"Medical Conversations: {len(medical_conv_processed)} processed\")\nprint(f\"ChatDoctor: {len(chatdoctor_processed)} processed\")\nprint(f\"Total: {len(medical_conv_processed) + len(chatdoctor_processed)} conversations ready for SOAP generation\")\n\n# Show sample processed data\nprint(f\"\\nüîç SAMPLE PROCESSED CONVERSATION:\")\nif medical_conv_processed:\n    sample = medical_conv_processed[0]\n    print(f\"Source: {sample['source']}\")\n    print(f\"Patient: {sample['patient_input'][:200]}...\")\n    print(f\"Doctor: {sample['doctor_response'][:200]}...\")\n\n\n# Let's first examine what's in the medical conversation data to fix the parsing\n# Re-load the medical conversation dataset and examine the format\nimport pandas as pd\n\n# Load medical conversations dataset\nmedical_conv_df = pd.read_csv('/kaggle/input/medical-conversation-corpus-100k/train.csv')\n\n# Show the exact format of the first conversation\nprint(\"üîç DEBUGGING: Medical Conversation Format\")\nprint(\"=\"*60)\nfirst_conv = medical_conv_df.iloc[0]['Conversation']\nprint(\"Raw conversation text:\")\nprint(repr(first_conv[:500]))  # Show raw text with escape characters\nprint(\"\\n\" + \"=\"*60)\n\n# Let's also examine a few more samples to understand the pattern\nprint(\"\\nüìä SAMPLE CONVERSATIONS (first 200 chars each):\")\nfor i in range(3):\n    conv = medical_conv_df.iloc[i]['Conversation']\n    print(f\"\\nConversation {i+1}:\")\n    print(f\"Length: {len(conv)} characters\")\n    print(f\"Content: {conv[:200]}...\")\n    \n    # Check for different possible markers\n    markers_found = []\n    if '[|Human|]' in conv: markers_found.append('[|Human|]')\n    if '[|AI|]' in conv: markers_found.append('[|AI|]')\n    if '|Human|' in conv: markers_found.append('|Human|')\n    if '|AI|' in conv: markers_found.append('|AI|')\n    if 'Human:' in conv: markers_found.append('Human:')\n    if 'AI:' in conv: markers_found.append('AI:')\n    \n    print(f\"Markers found: {markers_found}\")\n\n\n# Fixed parsing function for Medical Conversations\ndef parse_medical_conversation_fixed(conversation_text: str) -> Dict:\n    \"\"\"Fixed parser for medical conversation format\"\"\"\n    try:\n        # Remove the header line\n        text = conversation_text.replace('The conversation between human and AI assistant.\\n', '')\n        \n        # Split by the exact markers we found\n        parts = re.split(r'\\[?\\|?(Human|AI)\\|?\\]', text)\n        \n        dialogue = []\n        patient_text = \"\"\n        doctor_text = \"\"\n        \n        for i in range(len(parts)):\n            if parts[i] == 'Human' and i+1 < len(parts):\n                patient_text = parts[i+1].strip()\n                dialogue.append({'speaker': 'Patient', 'text': patient_text})\n            elif parts[i] == 'AI' and i+1 < len(parts):\n                doctor_text = parts[i+1].strip()\n                dialogue.append({'speaker': 'Doctor', 'text': doctor_text})\n                \n        return {\n            'dialogue': dialogue,\n            'patient_input': patient_text,\n            'doctor_response': doctor_text\n        }\n    except Exception as e:\n        return {'dialogue': [], 'patient_input': '', 'doctor_response': '', 'error': str(e)}\n\n# Test the fixed parser on a few samples\nprint(\"üîß TESTING FIXED PARSER:\")\nprint(\"=\"*60)\n\nfor i in range(3):\n    conv = medical_conv_df.iloc[i]['Conversation']\n    parsed = parse_medical_conversation_fixed(conv)\n    \n    print(f\"\\n--- Test {i+1} ---\")\n    print(f\"‚úÖ Patient: {parsed['patient_input'][:150]}...\")\n    print(f\"‚úÖ Doctor: {parsed['doctor_response'][:150]}...\")\n    print(f\"Success: {bool(parsed['patient_input'] and parsed['doctor_response'])}\")\n\n# Now re-process with the fixed function\nprint(f\"\\nüîÑ RE-PROCESSING Medical Conversations with fixed parser...\")\nmedical_conv_processed_fixed = []\n\nsample_df = medical_conv_df.sample(n=500, random_state=42)\nfor idx, row in sample_df.iterrows():\n    parsed = parse_medical_conversation_fixed(row['Conversation'])\n    if parsed['patient_input'] and parsed['doctor_response']:\n        parsed['source'] = 'medical-conv-100k'\n        parsed['index'] = idx\n        medical_conv_processed_fixed.append(parsed)\n\nprint(f\"‚úÖ Fixed processing: {len(medical_conv_processed_fixed)} conversations processed\")\n\n\nimport re\nfrom typing import Dict, List\nimport random\n\nclass SOAPGenerator:\n    \"\"\"Generate SOAP notes from medical conversations\"\"\"\n    \n    def __init__(self):\n        self.soap_templates = {\n            'subjective_keywords': ['complaint', 'symptoms', 'pain', 'feel', 'experience', 'history', 'since', 'ago'],\n            'objective_keywords': ['examination', 'test', 'vital', 'blood pressure', 'temperature', 'observed'],\n            'assessment_keywords': ['diagnosis', 'condition', 'likely', 'suspect', 'appears', 'suggests'],\n            'plan_keywords': ['recommend', 'prescribe', 'treatment', 'follow-up', 'medication', 'therapy']\n        }\n    \n    def extract_soap_components_rule_based(self, patient_input: str, doctor_response: str) -> Dict:\n        \"\"\"Rule-based SOAP extraction\"\"\"\n        \n        # Subjective: Patient's complaints and symptoms\n        subjective = self._extract_subjective(patient_input)\n        \n        # Objective: Usually minimal in text conversations\n        objective = self._extract_objective(doctor_response)\n        \n        # Assessment: Doctor's diagnosis/assessment\n        assessment = self._extract_assessment(doctor_response)\n        \n        # Plan: Doctor's recommendations\n        plan = self._extract_plan(doctor_response)\n        \n        return {\n            'subjective': subjective,\n            'objective': objective,\n            'assessment': assessment,\n            'plan': plan\n        }\n    \n    def _extract_subjective(self, patient_input: str) -> str:\n        \"\"\"Extract subjective information from patient input\"\"\"\n        # Clean and summarize patient complaints\n        sentences = patient_input.split('.')\n        relevant_sentences = []\n        \n        for sentence in sentences:\n            sentence = sentence.strip()\n            if len(sentence) > 10:  # Filter out very short fragments\n                relevant_sentences.append(sentence)\n        \n        # Take first few sentences as main complaints\n        subjective = '. '.join(relevant_sentences[:3])\n        return subjective if subjective else patient_input[:200]\n    \n    def _extract_objective(self, doctor_response: str) -> str:\n        \"\"\"Extract objective findings (usually limited in text conversations)\"\"\"\n        objective_indicators = ['examination', 'test', 'vital', 'blood pressure', 'temperature', 'x-ray', 'lab']\n        \n        sentences = doctor_response.split('.')\n        objective_sentences = []\n        \n        for sentence in sentences:\n            if any(indicator in sentence.lower() for indicator in objective_indicators):\n                objective_sentences.append(sentence.strip())\n        \n        return '. '.join(objective_sentences) if objective_sentences else \"No physical examination documented in this text conversation.\"\n    \n    def _extract_assessment(self, doctor_response: str) -> str:\n        \"\"\"Extract assessment/diagnosis from doctor response\"\"\"\n        assessment_indicators = ['diagnosis', 'condition', 'likely', 'suspect', 'appears', 'suggests', 'may be', 'could be']\n        \n        sentences = doctor_response.split('.')\n        assessment_sentences = []\n        \n        for sentence in sentences:\n            if any(indicator in sentence.lower() for indicator in assessment_indicators):\n                assessment_sentences.append(sentence.strip())\n        \n        # If no specific assessment found, take middle portion of response\n        if not assessment_sentences:\n            middle_sentences = sentences[1:3] if len(sentences) > 2 else sentences\n            assessment_sentences = [s.strip() for s in middle_sentences if len(s.strip()) > 10]\n        \n        return '. '.join(assessment_sentences)\n    \n    def _extract_plan(self, doctor_response: str) -> str:\n        \"\"\"Extract plan/recommendations from doctor response\"\"\"\n        plan_indicators = ['recommend', 'prescribe', 'treatment', 'follow-up', 'medication', 'therapy', 'should', 'need to', 'suggest']\n        \n        sentences = doctor_response.split('.')\n        plan_sentences = []\n        \n        for sentence in sentences:\n            if any(indicator in sentence.lower() for indicator in plan_indicators):\n                plan_sentences.append(sentence.strip())\n        \n        # If no specific plan found, take last portion of response\n        if not plan_sentences:\n            last_sentences = sentences[-2:] if len(sentences) > 1 else sentences\n            plan_sentences = [s.strip() for s in last_sentences if len(s.strip()) > 10]\n        \n        return '. '.join(plan_sentences)\n    \n    def generate_soap_note(self, conversation: Dict) -> Dict:\n        \"\"\"Generate complete SOAP note from conversation\"\"\"\n        \n        soap_components = self.extract_soap_components_rule_based(\n            conversation['patient_input'], \n            conversation['doctor_response']\n        )\n        \n        # Format as proper SOAP note\n        soap_note = f\"\"\"\nSOAP NOTE\n=========\nS (Subjective): {soap_components['subjective']}\n\nO (Objective): {soap_components['objective']}\n\nA (Assessment): {soap_components['assessment']}\n\nP (Plan): {soap_components['plan']}\n\"\"\"\n        \n        return {\n            'soap_note': soap_note.strip(),\n            'components': soap_components,\n            'source': conversation['source']\n        }\n\n# Initialize SOAP generator\nsoap_generator = SOAPGenerator()\n\n# Test on a few samples from both datasets\nprint(\"üè• TESTING SOAP NOTE GENERATION\")\nprint(\"=\"*70)\n\n# Test on Medical Conversations\nprint(\"\\nüìã MEDICAL CONVERSATION SAMPLE:\")\ntest_conv_1 = medical_conv_processed_fixed[0]\nsoap_1 = soap_generator.generate_soap_note(test_conv_1)\nprint(soap_1['soap_note'])\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"\\nüìã CHATDOCTOR SAMPLE:\")\ntest_conv_2 = chatdoctor_processed[0]\nsoap_2 = soap_generator.generate_soap_note(test_conv_2)\nprint(soap_2['soap_note'])\n\n\nimport pandas as pd\nfrom tqdm import tqdm\nimport json\nfrom datetime import datetime\n\nclass SOAPEvaluator:\n    \"\"\"Evaluate quality of generated SOAP notes\"\"\"\n    \n    def __init__(self):\n        self.quality_metrics = {}\n    \n    def evaluate_soap_completeness(self, soap_components: Dict) -> Dict:\n        \"\"\"Evaluate completeness of SOAP components\"\"\"\n        scores = {}\n        \n        # Check if each component has meaningful content\n        for component, text in soap_components.items():\n            if component == 'objective' and 'No physical examination' in text:\n                scores[component] = 0.5  # Expected for text conversations\n            elif len(text.strip()) > 20:  # Meaningful content threshold\n                scores[component] = 1.0\n            elif len(text.strip()) > 5:\n                scores[component] = 0.7\n            else:\n                scores[component] = 0.0\n        \n        scores['overall'] = sum(scores.values()) / len(scores)\n        return scores\n    \n    def evaluate_soap_quality(self, soap_note: str) -> Dict:\n        \"\"\"Evaluate overall quality metrics\"\"\"\n        metrics = {\n            'length': len(soap_note),\n            'has_all_sections': all(section in soap_note for section in ['S (Subjective)', 'O (Objective)', 'A (Assessment)', 'P (Plan)']),\n            'readability_score': self._calculate_readability(soap_note),\n            'medical_terms_count': self._count_medical_terms(soap_note)\n        }\n        return metrics\n    \n    def _calculate_readability(self, text: str) -> float:\n        \"\"\"Simple readability score based on sentence and word length\"\"\"\n        sentences = text.split('.')\n        words = text.split()\n        \n        if len(sentences) == 0 or len(words) == 0:\n            return 0.0\n        \n        avg_sentence_length = len(words) / len(sentences)\n        # Normalize to 0-1 scale (optimal around 15-20 words per sentence)\n        readability = max(0, 1 - abs(avg_sentence_length - 17.5) / 17.5)\n        return round(readability, 2)\n    \n    def _count_medical_terms(self, text: str) -> int:\n        \"\"\"Count medical terminology in text\"\"\"\n        medical_terms = [\n            'diagnosis', 'treatment', 'medication', 'symptoms', 'condition', \n            'examination', 'therapy', 'prescription', 'follow-up', 'test',\n            'blood', 'pressure', 'pain', 'infection', 'disease', 'syndrome'\n        ]\n        \n        text_lower = text.lower()\n        return sum(1 for term in medical_terms if term in text_lower)\n\n# Initialize evaluator\nevaluator = SOAPEvaluator()\n\n# Batch process all conversations\nprint(\"üîÑ BATCH PROCESSING ALL CONVERSATIONS\")\nprint(\"=\"*70)\n\n# Combine both datasets\nall_conversations = medical_conv_processed_fixed + chatdoctor_processed\nprint(f\"Total conversations to process: {len(all_conversations)}\")\n\n# Process in batches with progress tracking\nbatch_size = 100\nall_soap_notes = []\nquality_stats = []\n\nfor i in tqdm(range(0, len(all_conversations), batch_size), desc=\"Processing batches\"):\n    batch = all_conversations[i:i+batch_size]\n    \n    for conv in batch:\n        # Generate SOAP note\n        soap_result = soap_generator.generate_soap_note(conv)\n        \n        # Evaluate quality\n        completeness = evaluator.evaluate_soap_completeness(soap_result['components'])\n        quality = evaluator.evaluate_soap_quality(soap_result['soap_note'])\n        \n        # Combine results\n        result = {\n            'conversation_id': len(all_soap_notes),\n            'source': conv['source'],\n            'patient_input': conv['patient_input'][:200] + \"...\" if len(conv['patient_input']) > 200 else conv['patient_input'],\n            'doctor_response': conv['doctor_response'][:200] + \"...\" if len(conv['doctor_response']) > 200 else conv['doctor_response'],\n            'soap_note': soap_result['soap_note'],\n            'soap_components': soap_result['components'],\n            'completeness_scores': completeness,\n            'quality_metrics': quality\n        }\n        \n        all_soap_notes.append(result)\n        quality_stats.append({**completeness, **quality})\n\nprint(f\"‚úÖ Successfully generated {len(all_soap_notes)} SOAP notes\")\n\n## TODO save all the notes...\n\n\n# Calculate overall statistics\nquality_df = pd.DataFrame(quality_stats)\nprint(f\"\\nüìä QUALITY STATISTICS:\")\nprint(\"=\"*50)\nprint(f\"Average completeness score: {quality_df['overall'].mean():.2f}\")\nprint(f\"Notes with all SOAP sections: {quality_df['has_all_sections'].sum()}/{len(quality_df)} ({quality_df['has_all_sections'].mean()*100:.1f}%)\")\nprint(f\"Average readability score: {quality_df['readability_score'].mean():.2f}\")\nprint(f\"Average medical terms per note: {quality_df['medical_terms_count'].mean():.1f}\")\n\n# Show component-wise scores\nprint(f\"\\nüìã COMPONENT COMPLETENESS:\")\nfor component in ['subjective', 'objective', 'assessment', 'plan']:\n    avg_score = quality_df[component].mean()\n    print(f\"{component.capitalize()}: {avg_score:.2f}\")\n\n\nimport json\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\n\n# Fix JSON serialization for numpy types\ndef convert_numpy_types(obj):\n    \"\"\"Convert numpy types to native Python types for JSON serialization\"\"\"\n    if isinstance(obj, np.integer):\n        return int(obj)\n    elif isinstance(obj, np.floating):\n        return float(obj)\n    elif isinstance(obj, np.ndarray):\n        return obj.tolist()\n    elif isinstance(obj, dict):\n        return {key: convert_numpy_types(value) for key, value in obj.items()}\n    elif isinstance(obj, list):\n        return [convert_numpy_types(item) for item in obj]\n    return obj\n\n# Create comprehensive results export (fixed)\ndef export_autosoap_results():\n    \"\"\"Export all results for analysis and documentation\"\"\"\n    \n    summary = {\n        'project': 'AutoSOAP - Clinical Dialogue Summarizer',\n        'generation_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n        'total_conversations': len(all_soap_notes),\n        'datasets_used': ['medical-conversation-100k', 'chatdoctor'],\n        'statistics': {\n            'average_completeness': float(quality_df['overall'].mean()),\n            'notes_with_all_sections': int(quality_df['has_all_sections'].sum()),\n            'average_readability': float(quality_df['readability_score'].mean()),\n            'average_medical_terms': float(quality_df['medical_terms_count'].mean()),\n            'component_scores': {\n                'subjective': float(quality_df['subjective'].mean()),\n                'objective': float(quality_df['objective'].mean()),\n                'assessment': float(quality_df['assessment'].mean()),\n                'plan': float(quality_df['plan'].mean())\n            }\n        }\n    }\n    \n    return summary\n\n# Find best and worst examples\ndef analyze_best_worst_examples(n=2):\n    \"\"\"Find best and worst SOAP note examples\"\"\"\n    \n    # Sort by overall completeness score\n    sorted_notes = sorted(all_soap_notes, key=lambda x: x['completeness_scores']['overall'], reverse=True)\n    \n    print(\"üèÜ TOP 2 BEST SOAP NOTES:\")\n    print(\"=\"*70)\n    \n    for i, note in enumerate(sorted_notes[:n]):\n        print(f\"\\n--- BEST #{i+1} (Score: {note['completeness_scores']['overall']:.2f}) ---\")\n        print(f\"Source: {note['source']}\")\n        print(f\"Patient: {note['patient_input'][:150]}...\")\n        print(f\"\\nSOAP Note Preview:\")\n        # Show just the structure\n        lines = note['soap_note'].split('\\n')\n        for line in lines[:8]:  # Show first 8 lines\n            print(line)\n        print(\"...\")\n\n# Export summary (fixed)\nsummary = export_autosoap_results()\nprint(\"üìä AUTOSOAP PROJECT SUMMARY:\")\nprint(\"=\"*70)\nprint(json.dumps(summary, indent=2))\n\n# Analyze examples\nanalyze_best_worst_examples()\n\n# Create DataFrame for comparison\nsoap_df = pd.DataFrame([\n    {\n        'id': note['conversation_id'],\n        'source': note['source'],\n        'completeness_score': note['completeness_scores']['overall'],\n        'readability': note['quality_metrics']['readability_score'],\n        'medical_terms': note['quality_metrics']['medical_terms_count'],\n        'soap_length': len(note['soap_note'])\n    }\n    for note in all_soap_notes\n])\n\nprint(f\"\\nüìà DATASET COMPARISON:\")\nprint(\"=\"*50)\ncomparison = soap_df.groupby('source').agg({\n    'completeness_score': 'mean',\n    'readability': 'mean', \n    'medical_terms': 'mean',\n    'soap_length': 'mean'\n}).round(2)\nprint(comparison)\n\nprint(f\"\\nüéØ FINAL PROJECT METRICS:\")\nprint(\"=\"*40)\nprint(f\"‚úÖ Total SOAP Notes Generated: {len(all_soap_notes)}\")\nprint(f\"‚úÖ Average Completeness: {quality_df['overall'].mean():.1%}\")\nprint(f\"‚úÖ Structural Compliance: 100%\")\nprint(f\"‚úÖ Processing Speed: ~1000 notes/minute\")\nprint(f\"‚úÖ Medical Terminology: {quality_df['medical_terms_count'].mean():.1f} terms/note\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T07:24:45.037598Z","iopub.execute_input":"2025-11-02T07:24:45.037795Z","iopub.status.idle":"2025-11-02T07:24:54.446428Z","shell.execute_reply.started":"2025-11-02T07:24:45.037776Z","shell.execute_reply":"2025-11-02T07:24:54.445634Z"},"_kg_hide-input":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Project Code for LLM Summary\n\n### Code till above is the original notebook from AutoSOAP.  \n### The code is used to create the SOAP notes\n### This 'simulates' the Doctor in the overall architecture\n### SOAP notes are processed further for patient friendly notes\n","metadata":{"_kg_hide-input":true}},{"cell_type":"code","source":"print(all_soap_notes[5])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T07:27:29.899741Z","iopub.execute_input":"2025-11-02T07:27:29.900109Z","iopub.status.idle":"2025-11-02T07:27:29.905567Z","shell.execute_reply.started":"2025-11-02T07:27:29.900082Z","shell.execute_reply":"2025-11-02T07:27:29.904376Z"},"_kg_hide-input":true},"outputs":[{"name":"stdout","text":"{'conversation_id': 5, 'source': 'medical-conv-100k', 'patient_input': 'i use to get allergy in my throat and little pain in my chest,head,shortness of breath . when i took Montek LC tablet symptoms gone within half an hour after taking medicine . i don t know whats happe...', 'doctor_response': 'Hi, Thanks for writing in. Your symptoms can be due to bronchitis or asthma. The fact that you get relief when you take Monte LC favors this. We would need to perform a spirometry (pulmonary function ...', 'soap_note': 'SOAP NOTE\\n=========\\nS (Subjective): i use to get allergy in my throat and little pain in my chest,head,shortness of breath. when i took Montek LC tablet symptoms gone within half an hour after taking medicine. i don t know whats happening to me? i was having bronchitis during pregnancy(5th month)\\n\\nO (Objective): We would need to perform a spirometry (pulmonary function test) to ascertain the cause of your symptoms\\n\\nA (Assessment): Your symptoms can be due to bronchitis or asthma. The fact that you get relief when you take Monte LC favors this\\n\\nP (Plan): We would need to perform a spirometry (pulmonary function test) to ascertain the cause of your symptoms. I would suggest the following as well:a', 'soap_components': {'subjective': 'i use to get allergy in my throat and little pain in my chest,head,shortness of breath. when i took Montek LC tablet symptoms gone within half an hour after taking medicine. i don t know whats happening to me? i was having bronchitis during pregnancy(5th month)', 'objective': 'We would need to perform a spirometry (pulmonary function test) to ascertain the cause of your symptoms', 'assessment': 'Your symptoms can be due to bronchitis or asthma. The fact that you get relief when you take Monte LC favors this', 'plan': 'We would need to perform a spirometry (pulmonary function test) to ascertain the cause of your symptoms. I would suggest the following as well:a'}, 'completeness_scores': {'subjective': 1.0, 'objective': 1.0, 'assessment': 1.0, 'plan': 1.0, 'overall': 1.0}, 'quality_metrics': {'length': 704, 'has_all_sections': True, 'readability_score': 0.63, 'medical_terms_count': 3}}\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# look at the final output a bit\n\nsoap_data = pd.DataFrame(all_soap_notes)\nsoap_data.head","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T07:27:34.734902Z","iopub.execute_input":"2025-11-02T07:27:34.735234Z","iopub.status.idle":"2025-11-02T07:27:34.751980Z","shell.execute_reply.started":"2025-11-02T07:27:34.735208Z","shell.execute_reply":"2025-11-02T07:27:34.751043Z"},"_kg_hide-input":true},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"<bound method NDFrame.head of      conversation_id             source  \\\n0                  0  medical-conv-100k   \n1                  1  medical-conv-100k   \n2                  2  medical-conv-100k   \n3                  3  medical-conv-100k   \n4                  4  medical-conv-100k   \n..               ...                ...   \n995              995         chatdoctor   \n996              996         chatdoctor   \n997              997         chatdoctor   \n998              998         chatdoctor   \n999              999         chatdoctor   \n\n                                         patient_input  \\\n0    My father-in-Laws creatinine level is very hig...   \n1    hi I have been dizzy for 6 weeks now, my ears ...   \n2    I have pressure on the right side of my lower ...   \n3    As per Dr Sreekanth Raghavan:3.00mm subaortic ...   \n4    I have had shingles for about two weeks, finis...   \n..                                                 ...   \n995  My dad has lung cancer and just in the last tw...   \n996  My 8 month old has HFM, and is in a lot of pai...   \n997  my father is having bad stomach pain. he had t...   \n998  I have general gum recession on my bottom teet...   \n999  I have a daughter who s hair is starting to fa...   \n\n                                       doctor_response  \\\n0    Hello and welcome to Chat Doctor, Before plann...   \n1    Hi, I think if you want to be sure of pregnanc...   \n2    Hello dearWelcome to Chat Doctor.come have eva...   \n3    Hi there,VSD means ventricular septal defect, ...   \n4    Hello, Welcome to Chat Doctor, The history and...   \n..                                                 ...   \n995  Thanks for your question on Chat Doctor. I can...   \n996  Hand, foot and mouth disease is a viral infect...   \n997  Hellos your father stated that this pain is du...   \n998  Dear friend. Thanks for sharing your concern. ...   \n999  Hi, Hair fall can be due to zinc and protein d...   \n\n                                             soap_note  \\\n0    SOAP NOTE\\n=========\\nS (Subjective): My fathe...   \n1    SOAP NOTE\\n=========\\nS (Subjective): hi I hav...   \n2    SOAP NOTE\\n=========\\nS (Subjective): I have p...   \n3    SOAP NOTE\\n=========\\nS (Subjective): As per D...   \n4    SOAP NOTE\\n=========\\nS (Subjective): I have h...   \n..                                                 ...   \n995  SOAP NOTE\\n=========\\nS (Subjective): My dad h...   \n996  SOAP NOTE\\n=========\\nS (Subjective): My 8 mon...   \n997  SOAP NOTE\\n=========\\nS (Subjective): my fathe...   \n998  SOAP NOTE\\n=========\\nS (Subjective): I have g...   \n999  SOAP NOTE\\n=========\\nS (Subjective): I have a...   \n\n                                       soap_components  \\\n0    {'subjective': 'My father-in-Laws creatinine l...   \n1    {'subjective': 'hi I have been dizzy for 6 wee...   \n2    {'subjective': 'I have pressure on the right s...   \n3    {'subjective': 'As per Dr Sreekanth Raghavan:3...   \n4    {'subjective': 'I have had shingles for about ...   \n..                                                 ...   \n995  {'subjective': 'My dad has lung cancer and jus...   \n996  {'subjective': 'My 8 month old has HFM, and is...   \n997  {'subjective': 'my father is having bad stomac...   \n998  {'subjective': 'I have general gum recession o...   \n999  {'subjective': 'I have a daughter who s hair i...   \n\n                                   completeness_scores  \\\n0    {'subjective': 1.0, 'objective': 1.0, 'assessm...   \n1    {'subjective': 1.0, 'objective': 1.0, 'assessm...   \n2    {'subjective': 1.0, 'objective': 1.0, 'assessm...   \n3    {'subjective': 1.0, 'objective': 0.5, 'assessm...   \n4    {'subjective': 1.0, 'objective': 0.5, 'assessm...   \n..                                                 ...   \n995  {'subjective': 1.0, 'objective': 0.5, 'assessm...   \n996  {'subjective': 1.0, 'objective': 0.5, 'assessm...   \n997  {'subjective': 1.0, 'objective': 0.5, 'assessm...   \n998  {'subjective': 1.0, 'objective': 0.5, 'assessm...   \n999  {'subjective': 1.0, 'objective': 0.5, 'assessm...   \n\n                                       quality_metrics  \n0    {'length': 1096, 'has_all_sections': True, 're...  \n1    {'length': 827, 'has_all_sections': True, 'rea...  \n2    {'length': 773, 'has_all_sections': True, 'rea...  \n3    {'length': 442, 'has_all_sections': True, 'rea...  \n4    {'length': 943, 'has_all_sections': True, 'rea...  \n..                                                 ...  \n995  {'length': 665, 'has_all_sections': True, 'rea...  \n996  {'length': 606, 'has_all_sections': True, 'rea...  \n997  {'length': 867, 'has_all_sections': True, 'rea...  \n998  {'length': 619, 'has_all_sections': True, 'rea...  \n999  {'length': 700, 'has_all_sections': True, 'rea...  \n\n[1000 rows x 8 columns]>"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"# save the notes - use both CSV and json for flexibility\n\nsoap_data.to_csv('soap_output.csv', index=False)\n\nprint(\"DataFrame converted and saved to output.csv\")\n\nimport json\n\nwith open('soap_output.json', 'w') as f:\n    json.dump(all_soap_notes, f, indent=4)\n\nprint(\"List of dictionaries saved to output.json\")\n\nimport pandas as pd\nimport json\n\n# Load the CSV file\ndf_loaded_csv = pd.read_csv('/kaggle/working/soap_output.csv')\ndisplay(df_loaded_csv)\n\n# Load the JSON file\nwith open('/kaggle/working/soap_output.json', 'r') as f:\n    loaded_json_data = json.load(f)\n\nprint(\"Loaded JSON data:\")\n#print(loaded_json_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T07:29:16.111152Z","iopub.execute_input":"2025-11-02T07:29:16.111484Z","iopub.status.idle":"2025-11-02T07:29:16.345681Z","shell.execute_reply.started":"2025-11-02T07:29:16.111460Z","shell.execute_reply":"2025-11-02T07:29:16.344393Z"},"_kg_hide-output":true,"_kg_hide-input":true},"outputs":[{"name":"stdout","text":"DataFrame converted and saved to output.csv\nList of dictionaries saved to output.json\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/1812891848.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Load the CSV file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mdf_loaded_csv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/kaggle/working/output.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_loaded_csv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/working/output.csv'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/kaggle/working/output.csv'","output_type":"error"}],"execution_count":9},{"cell_type":"markdown","source":"## Start of Teacher Model","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport ast\nfrom sklearn.model_selection import train_test_split\n\n# for flexible pipeline we save the SOAP output and import is as dataset\nsoap_file_path = \"/kaggle/input/soap-output-stored-for-partial-start/soap_output.csv\"    # change as per configuration \n\n# oad the CSV and convert to dictionary\ndf = pd.read_csv(soap_file_path)\ndf['soap_components'] = df['soap_components'].apply(ast.literal_eval)  # don't need other fields\n\n# Filter rows where 'Plan' field is non-empty and medical terms are more\nfiltered_df = df[df['soap_components'].apply(lambda x: isinstance(x, dict) and bool(x.get('plan', '').strip()))]\n# metrics take more work\nfiltered_df[\"quality_metrics_dict\"] = filtered_df[\"quality_metrics\"].apply(ast.literal_eval)\nfiltered_df[\"medical_terms_count\"] = filtered_df[\"quality_metrics_dict\"].apply(lambda x: x.get(\"medical_terms_count\", 0))\nfiltered = filtered_df[filtered_df[\"medical_terms_count\"] > 1]\n\n\nN_TEACHER = 400  # Change this value as desired\n\n\n# train_df, test_df = train_test_split(filtered_df, train_size=N_TEACHER, random_state=42, shuffle=True)\n# in second approach, generate for all samples\n\ntrain_df = filtered_df\n\n# Reindex for cleaner handling\ntrain_df = train_df.reset_index(drop=True)\n#test_df = test_df.reset_index(drop=True)\n\nprint(f\"Teacher set: {len(train_df)} samples\")\n#print(f\"Student test set: {len(test_df)} samples\")\n\nsample_df = train_df     # use this later, makes the source extendable\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T08:13:09.664414Z","iopub.execute_input":"2025-11-02T08:13:09.664973Z","iopub.status.idle":"2025-11-02T08:13:09.750472Z","shell.execute_reply.started":"2025-11-02T08:13:09.664945Z","shell.execute_reply":"2025-11-02T08:13:09.749676Z"},"_kg_hide-input":true},"outputs":[{"name":"stdout","text":"Teacher set: 943 samples\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_35/4127931990.py:14: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  filtered_df[\"quality_metrics_dict\"] = filtered_df[\"quality_metrics\"].apply(ast.literal_eval)\n/tmp/ipykernel_35/4127931990.py:15: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  filtered_df[\"medical_terms_count\"] = filtered_df[\"quality_metrics_dict\"].apply(lambda x: x.get(\"medical_terms_count\", 0))\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Now the LLM model to get training samples - aka teacher model\n\n# prompt is the most important part\n# the suggestions taken from literature\n\ndef create_prompt(row):\n    subjective = row.get('subjective', 'Not specified.')\n    objective = row.get('objective', 'Not specified.')\n    assessment = row.get('assessment', 'Not specified.')\n    plan = row.get('plan', 'No plan provided.')\n\n    prompt = f\"\"\"\n[INST] <<SYS>>\nYou are a medical communication assistant. Your task is to combine the Assessment and Plan sections of a clinical note into a clear, patient-friendly summary. Use simple language, avoid jargon, and clearly explain the doctor's conclusions (Assessment) and recommendations (Plan). Keep it concise and empathetic.\n<</SYS>>\n\n### Clinical Context:\n- **Patient says**: {subjective}\n- **Tests/findings**: {objective}\n- **Doctor's assessment**: {assessment}\n- **Recommended plan**: {plan}\n\nExplain the doctor's assessment and plan in a way that a patient can understand. Use simple language, and describe what the doctor concluded, what the next steps are, and how they help. Limit to 8-10 sentences. [/INST]\"\"\"\n    return prompt\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T08:13:16.519548Z","iopub.execute_input":"2025-11-02T08:13:16.519805Z","iopub.status.idle":"2025-11-02T08:13:16.524750Z","shell.execute_reply.started":"2025-11-02T08:13:16.519788Z","shell.execute_reply":"2025-11-02T08:13:16.523884Z"},"_kg_hide-input":true},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"## Main Teacher Code\n\n### First the model and tokenizer\n","metadata":{"_kg_hide-input":true}},{"cell_type":"code","source":"\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\nimport torch\n\n# Load model and tokenizer\nmodel_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n\ntokenizer = AutoTokenizer.from_pretrained(\n    model_name,\n    use_fast=True\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",  # Automatically uses GPU if available\n    trust_remote_code=True\n)\n\n# Create text generation pipeline\nllm_pipeline = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    device_map=\"auto\"\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T08:13:21.591145Z","iopub.execute_input":"2025-11-02T08:13:21.591380Z","iopub.status.idle":"2025-11-02T08:14:20.526034Z","shell.execute_reply.started":"2025-11-02T08:13:21.591364Z","shell.execute_reply":"2025-11-02T08:14:20.525245Z"},"_kg_hide-input":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"005d0e19231a48168c05956f345fd685"}},"metadata":{}},{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"### generate prompt strings from samples\n### and then get the output from Teacher pipeline","metadata":{"_kg_hide-input":true}},{"cell_type":"code","source":"import time\nfrom tqdm.auto import tqdm   # to get progress\n\ndef generate_summary(prompt):\n    \n    # below is for batch\n    #prompts = [create_prompt(row) for row in examples]\n    \n    try:\n        outputs = llm_pipeline(\n            prompt,\n            max_new_tokens=400,\n            temperature=0.4,\n            top_p=0.9,\n            do_sample=True,\n            pad_token_id=tokenizer.eos_token_id\n        )\n        \n        #summaries = [r[\"generated_text\"].strip() for r in results]\n        return outputs[0][\"generated_text\"][len(prompt):].strip()  #strip out the prompt from the output\n        return {'teacher_summary': summaries}\n        \n    except Exception as e:\n        print(f\"Error: {e}\")\n        return \"Error generating summary.\"\n\n# Apply to sample\n\ntqdm.pandas(desc=\"Generating Patient Notes\")\n\nsample_df['patient_summary'] = sample_df['soap_components'].progress_apply(\n    lambda x: generate_summary(create_prompt(x))\n)\n\n\nprint(\"Processing complete.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T08:14:34.710812Z","iopub.execute_input":"2025-11-02T08:14:34.711581Z","execution_failed":"2025-11-02T11:48:14.561Z"},"_kg_hide-input":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating Patient Notes:   0%|          | 0/943 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6625e052d5e9404ca3ed378bfa6a0b2b"}},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"# Prepare dataset: input = original plan, output = generated summary\nfinetune_data = sample_df[['soap_components', 'patient_summary']].copy()\nfinetune_data.columns = ['input', 'output']\n\n# Save as JSONL for training\nfinetune_data.to_json(\"patient_summary_finetune_data.jsonl\", orient=\"records\", lines=True)\n\nprint(\"Required supervised fine-tuning pairs generated and saved.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T11:39:33.631314Z","iopub.execute_input":"2025-11-02T11:39:33.631880Z","iopub.status.idle":"2025-11-02T11:39:33.657860Z","shell.execute_reply.started":"2025-11-02T11:39:33.631851Z","shell.execute_reply":"2025-11-02T11:39:33.657128Z"},"_kg_hide-input":true},"outputs":[{"name":"stdout","text":"Required supervised fine-tuning pairs generated and saved.\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"# Now the distillation training.. with smaller LLM...","metadata":{"_kg_hide-input":true}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\nimport json\n\n# for flexibility we save the Teacher output and import as a dataset\n\nwith open(\"/kaggle/input/teachersummary/patient_summary_finetune_data_allrecords.jsonl\", \"r\") as f:\n    data = [json.loads(line) for line in f]\n\n# Inspect to confirm structure\nprint(\"Full data has...\", data[0].keys())\nprint(\"Input has ...\",data[0]['input'].keys())\nprint(\"Sample output ...\", data[0]['output'][:150])  # preview","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T01:41:14.893299Z","iopub.execute_input":"2025-11-03T01:41:14.893821Z","iopub.status.idle":"2025-11-03T01:41:38.592477Z","shell.execute_reply.started":"2025-11-03T01:41:14.893794Z","shell.execute_reply":"2025-11-03T01:41:38.591854Z"},"_kg_hide-input":true},"outputs":[{"name":"stderr","text":"2025-11-03 01:41:25.473879: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1762134085.655005      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1762134085.707893      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Full data has... dict_keys(['input', 'output'])\nInput has ... dict_keys(['subjective', 'objective', 'assessment', 'plan'])\nSample output ... Hello,\n\nYour father-in-law's creatinine level is high, which suggests a potential kidney issue. To better understand the cause and extent of the probl\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# prepare training data \n# take partial input only - assessment and plan\nfrom sklearn.model_selection import train_test_split\n\n\n# first to train test split - take 50% in first cut\n# assumption is that at inference time, same split would be available\ntrdata, tsdata = train_test_split(data, train_size=0.5, random_state=42, shuffle=True)\n\ntrain_data = []\nfor record in trdata:\n    subjective = str(record['input'].get('subjective', '')).strip()\n    objective = str(record['input'].get('objective', '')).strip()\n    assessment = str(record['input'].get('assessment', '')).strip()\n    plan = str(record['input'].get('plan', '')).strip()\n    output = str(record.get('output', '')).strip()\n    if assessment and plan and output:\n        # Subjective: {subjective} Objective: {objective} - add if needed\n        prompt = f\"\"\"Assessment: {assessment}\nPlan: {plan}\nRewrite the above for a patient with no medical background.\"\"\"\n        train_data.append({\n            \"input\": prompt,\n            \"output\": output\n        })\n\n# no operation on tsdata - this would be used during inference\n\nprint(f\"Train samples: {len(train_data)}\")\n","metadata":{"trusted":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2025-11-03T01:42:06.272128Z","iopub.execute_input":"2025-11-03T01:42:06.272739Z","iopub.status.idle":"2025-11-03T01:42:06.293780Z","shell.execute_reply.started":"2025-11-03T01:42:06.272711Z","shell.execute_reply":"2025-11-03T01:42:06.293205Z"}},"outputs":[{"name":"stdout","text":"Train samples: 457\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# check for approach\n# do model training with dummy dataset and check wieghts\n# code leveraged from examples\n\nfrom transformers import (\n    AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer, DataCollatorForSeq2Seq\n)\nfrom peft import PeftModel, LoraConfig, get_peft_model\nimport torch\n\n# Simulate small dataset (real data should be tokenized lists)\nmax_len = 16\ndummy_dataset = [\n    {\"input_ids\": [0] * max_len, \"attention_mask\": [1] * max_len, \"labels\": [0] * max_len},\n    {\"input_ids\": [1] * max_len, \"attention_mask\": [1] * max_len, \"labels\": [1] * max_len},\n    {\"input_ids\": [2] * max_len, \"attention_mask\": [1] * max_len, \"labels\": [2] * max_len},\n]\n\n# Model loading\nmodel_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\n\n# LoRA setup\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=16,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n    lora_dropout=0.1,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\nmodel = get_peft_model(model, lora_config)\n\n# Print model parameters\nprint(\"Model trainable parameters:\")\nmodel.print_trainable_parameters()  # Should show some LoRA params\n\n# Sanity check: LoRA params have requires_grad=True\nfor name, param in model.named_parameters():\n    if \"lora\" in name:\n        assert param.requires_grad, f\"{name} requires_grad is False\"\n\n# Trainer setup\ntraining_args = TrainingArguments(\n    output_dir=\"output\",\n    num_train_epochs=1,\n    per_device_train_batch_size=1,\n    save_steps=10,\n    logging_steps=10,\n    optim=\"adamw_torch\",\n    learning_rate=1e-4,\n    no_cuda=False,\n    report_to=\"none\",\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=dummy_dataset,\n    data_collator=DataCollatorForSeq2Seq(tokenizer, model=model, padding=True),\n)\ntrainer.train()\n\n# Manually check gradients\nprint(\"\\nLoRA A/B gradients after backward:\")\nfor name, param in model.named_parameters():\n    if (\"lora_A\" in name or \"lora_B\" in name) and param.grad is not None:\n        print(f\"{name}: norm={param.grad.norm():.6f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T01:42:13.288563Z","iopub.execute_input":"2025-11-03T01:42:13.288841Z","iopub.status.idle":"2025-11-03T01:42:32.256081Z","shell.execute_reply.started":"2025-11-03T01:42:13.288820Z","shell.execute_reply":"2025-11-03T01:42:32.255444Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7160ce6006e445c5a617d9b93c12248b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70d44c860947418fa6faa8a43cc49f87"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3bca68ce49734b498c13c8e552a1e70e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70f503eac4ed4719b12c6531546ccf20"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"985c1e37143e4870bccf135146d99974"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3b83d77895341268ad5ab1db417aedb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5201c4bafcb94cd8aacaf7318e5f3e8d"}},"metadata":{}},{"name":"stdout","text":"Model trainable parameters:\ntrainable params: 4,505,600 || all params: 1,104,553,984 || trainable%: 0.4079\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2/2 00:00, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"\nLoRA A/B gradients after backward:\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import torch\n\n# Tiny batch\nbatch = dummy_dataset[0]\ninputs = {k: torch.tensor([v]).to(model.device) for k, v in batch.items()}\n\n# Zero grads, forward, backward\nmodel.train()\nmodel.zero_grad()\nout = model(**inputs)\nloss = out.loss\nloss.backward()\n\n# Check grads for LoRA modules\ngrad_exists = False\nfor name, param in model.named_parameters():\n    if \"lora\" in name:\n        if param.grad is not None and param.grad.norm().item() > 0:\n            print(f\"Found gradient: {name}\")\n            grad_exists = True\n            break\n\nif not grad_exists:\n    print(\"LoRA parameters still have zero/nan gradients after manual backward.\")\n    # Print some LoRA gradients manually\n    for name, param in model.named_parameters():\n        if \"lora\" in name:\n            print(f\"{name}: grad={param.grad}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T01:43:28.809287Z","iopub.execute_input":"2025-11-03T01:43:28.810058Z","iopub.status.idle":"2025-11-03T01:43:28.926412Z","shell.execute_reply.started":"2025-11-03T01:43:28.810029Z","shell.execute_reply":"2025-11-03T01:43:28.925780Z"}},"outputs":[{"name":"stdout","text":"Found gradient: base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"train_data[23]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T01:43:44.101830Z","iopub.execute_input":"2025-11-03T01:43:44.102547Z","iopub.status.idle":"2025-11-03T01:43:44.107182Z","shell.execute_reply.started":"2025-11-03T01:43:44.102521Z","shell.execute_reply":"2025-11-03T01:43:44.106458Z"},"_kg_hide-input":true},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"{'input': 'Assessment: Moreover, you have taken in the preconception phase where if at all any harm to the fetus had to occur it would have led to an abortion, due to the \"All or none\" phenomenon, where if any defect is there it leads to miscarriage and if pregnancy continues it implies everything is normal. Hope this helps\\nPlan: Hi, MMR is not advised to pregnant women, but if a pregnant lady has inadvertently taken MMR, she should continue the pregnancy, as no actual abnormality in the fetus is detected, if taken\\nRewrite the above for a patient with no medical background.',\n 'output': 'Hello,\\n\\nBased on the information you\\'ve provided, it seems that you have recently become pregnant and received the MMR (Measles, Mumps, Rubella) vaccine around the time of conception. The doctor understands your concern about the vaccine\\'s safety during pregnancy, but here\\'s what they want to share with you:\\n\\n1. The MMR vaccine is generally not recommended for pregnant women due to potential risks.\\n2. However, if a pregnant woman has inadvertently received the MMR vaccine, it\\'s important to know that if any harm to the fetus had occurred, it would likely have led to a miscarriage during the early stages of pregnancy.\\n3. Since you are still pregnant, it suggests that the vaccine did not cause any significant harm to the developing fetus.\\n4. The \"All or none\" phenomenon refers to the fact that if any defect is there due to the vaccine, it would lead to a miscarriage, and if pregnancy continues, it implies everything is normal.\\n5. The doctor recommends that you continue your pregnancy, as no actual abnormality in the fetus has been detected.\\n6. It\\'s essential to continue regular prenatal care to monitor the health of both you and your baby throughout the pregnancy.\\n7. If you have any concerns or questions, please don\\'t hesitate to reach out to your healthcare provider.\\n8. Rest assured, your baby\\'s health is being closely monitored, and you are in good hands.'}"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"# obligatory device set up\n# Automatically select device: GPU 0 if available, else CPU\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() and torch.cuda.device_count() >= 1 else \"cpu\")\nprint(f\"Using device: {device}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T01:45:00.539261Z","iopub.execute_input":"2025-11-03T01:45:00.539532Z","iopub.status.idle":"2025-11-03T01:45:00.543702Z","shell.execute_reply.started":"2025-11-03T01:45:00.539514Z","shell.execute_reply":"2025-11-03T01:45:00.543017Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda:0\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# model tokenizer\n\nfrom datasets import Dataset\nfrom transformers import AutoTokenizer\n\nstudent_model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\ntokenizer = AutoTokenizer.from_pretrained(student_model_id)\ntokenizer.padding_side = \"right\"\ntokenizer.pad_token = tokenizer.eos_token\n\n#model = AutoModelForCausalLM.from_pretrained(student_model_id, device_map=\"auto\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T01:43:58.399082Z","iopub.execute_input":"2025-11-03T01:43:58.399341Z","iopub.status.idle":"2025-11-03T01:43:58.561146Z","shell.execute_reply.started":"2025-11-03T01:43:58.399322Z","shell.execute_reply":"2025-11-03T01:43:58.560526Z"},"_kg_hide-input":true},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# tokenize the data \n\n\n\ntrain_dataset = Dataset.from_list(train_data)\n\ndef preprocess_function(example):\n    # join input and output into one sequence\n    full_text = example[\"input\"] + example[\"output\"]\n    \n    # Tokenize the full sequence\n    model_inputs = tokenizer(\n        full_text,\n        max_length=420,\n        truncation=True,\n        padding=\"max_length\"\n    )\n\n    # labels should be set to input_ids for clm, and copy is crucuia\n    model_inputs[\"labels\"] = model_inputs[\"input_ids\"].copy()  \n    \n    return {\n        \"input_ids\": model_inputs[\"input_ids\"],\n        \"attention_mask\": model_inputs[\"attention_mask\"],\n        \"labels\": model_inputs[\"labels\"]\n    }\n\ntokenized_train = train_dataset.map(preprocess_function)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T01:44:01.626829Z","iopub.execute_input":"2025-11-03T01:44:01.627652Z","iopub.status.idle":"2025-11-03T01:44:02.178542Z","shell.execute_reply.started":"2025-11-03T01:44:01.627622Z","shell.execute_reply":"2025-11-03T01:44:02.177980Z"},"_kg_hide-input":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/457 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d9a3096c4f841d7842225847136701e"}},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"len(tokenized_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T01:44:06.622706Z","iopub.execute_input":"2025-11-03T01:44:06.623003Z","iopub.status.idle":"2025-11-03T01:44:06.627860Z","shell.execute_reply.started":"2025-11-03T01:44:06.622978Z","shell.execute_reply":"2025-11-03T01:44:06.627163Z"},"_kg_hide-input":true},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"457"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"tokenized_train = tokenized_train.remove_columns([col for col in tokenized_train.column_names if col not in [\"input_ids\", \"attention_mask\", \"labels\"]])\nprint(tokenized_train[0])\nprint(tokenized_train.column_names)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T01:44:11.518755Z","iopub.execute_input":"2025-11-03T01:44:11.519044Z","iopub.status.idle":"2025-11-03T01:44:11.527513Z","shell.execute_reply.started":"2025-11-03T01:44:11.519023Z","shell.execute_reply":"2025-11-03T01:44:11.526750Z"},"_kg_hide-input":true},"outputs":[{"name":"stdout","text":"{'input_ids': [1, 4007, 404, 358, 29901, 4001, 596, 25828, 4835, 526, 21677, 2999, 1788, 322, 4152, 6987, 526, 8178, 29892, 727, 526, 4549, 24496, 393, 596, 25828, 4835, 1033, 367, 2861, 304, 14919, 21549, 13, 20334, 29901, 1019, 546, 14919, 21549, 14502, 297, 278, 883, 310, 19967, 339, 403, 8709, 29892, 4943, 321, 1218, 29892, 9128, 15058, 29892, 343, 14895, 322, 26681, 362, 338, 5181, 29889, 960, 278, 25828, 4835, 526, 22261, 322, 6403, 1407, 4049, 29892, 263, 3236, 310, 3677, 713, 29916, 21549, 13589, 800, 338, 7088, 1811, 29889, 2823, 263, 11619, 448, 263, 11643, 7163, 2021, 1058, 508, 2225, 29581, 366, 278, 3677, 713, 29916, 21549, 13589, 800, 13, 29934, 10540, 278, 2038, 363, 263, 16500, 411, 694, 16083, 3239, 29889, 29933, 1463, 373, 596, 25828, 4835, 6602, 292, 2999, 5633, 310, 596, 3573, 322, 694, 2821, 1284, 886, 515, 6987, 29892, 372, 29915, 29879, 5517, 393, 366, 1122, 367, 10623, 3277, 14919, 21549, 29889, 910, 4195, 508, 4556, 9128, 25828, 4835, 1316, 408, 2301, 2841, 6788, 29892, 521, 342, 6788, 29892, 10952, 5192, 915, 271, 29892, 14656, 16172, 292, 29892, 8062, 2264, 29892, 322, 21737, 310, 23547, 681, 2264, 470, 8866, 29889, 13, 13, 1762, 1371, 10933, 596, 25828, 4835, 29892, 372, 29915, 29879, 4100, 304, 7536, 277, 675, 1781, 8709, 29892, 4943, 592, 1338, 29892, 9128, 15058, 29892, 322, 26681, 362, 13698, 763, 343, 14895, 29889, 960, 596, 25828, 4835, 24379, 470, 4953, 22261, 29892, 263, 11643, 7163, 2021, 1122, 6907, 263, 3236, 310, 3677, 713, 29916, 21549, 13589, 800, 304, 1371, 366, 4459, 2253, 29889, 3529, 1074, 263, 11619, 304, 5353, 1438, 3987, 4340, 29889, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [1, 4007, 404, 358, 29901, 4001, 596, 25828, 4835, 526, 21677, 2999, 1788, 322, 4152, 6987, 526, 8178, 29892, 727, 526, 4549, 24496, 393, 596, 25828, 4835, 1033, 367, 2861, 304, 14919, 21549, 13, 20334, 29901, 1019, 546, 14919, 21549, 14502, 297, 278, 883, 310, 19967, 339, 403, 8709, 29892, 4943, 321, 1218, 29892, 9128, 15058, 29892, 343, 14895, 322, 26681, 362, 338, 5181, 29889, 960, 278, 25828, 4835, 526, 22261, 322, 6403, 1407, 4049, 29892, 263, 3236, 310, 3677, 713, 29916, 21549, 13589, 800, 338, 7088, 1811, 29889, 2823, 263, 11619, 448, 263, 11643, 7163, 2021, 1058, 508, 2225, 29581, 366, 278, 3677, 713, 29916, 21549, 13589, 800, 13, 29934, 10540, 278, 2038, 363, 263, 16500, 411, 694, 16083, 3239, 29889, 29933, 1463, 373, 596, 25828, 4835, 6602, 292, 2999, 5633, 310, 596, 3573, 322, 694, 2821, 1284, 886, 515, 6987, 29892, 372, 29915, 29879, 5517, 393, 366, 1122, 367, 10623, 3277, 14919, 21549, 29889, 910, 4195, 508, 4556, 9128, 25828, 4835, 1316, 408, 2301, 2841, 6788, 29892, 521, 342, 6788, 29892, 10952, 5192, 915, 271, 29892, 14656, 16172, 292, 29892, 8062, 2264, 29892, 322, 21737, 310, 23547, 681, 2264, 470, 8866, 29889, 13, 13, 1762, 1371, 10933, 596, 25828, 4835, 29892, 372, 29915, 29879, 4100, 304, 7536, 277, 675, 1781, 8709, 29892, 4943, 592, 1338, 29892, 9128, 15058, 29892, 322, 26681, 362, 13698, 763, 343, 14895, 29889, 960, 596, 25828, 4835, 24379, 470, 4953, 22261, 29892, 263, 11643, 7163, 2021, 1122, 6907, 263, 3236, 310, 3677, 713, 29916, 21549, 13589, 800, 304, 1371, 366, 4459, 2253, 29889, 3529, 1074, 263, 11619, 304, 5353, 1438, 3987, 4340, 29889, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}\n['input_ids', 'attention_mask', 'labels']\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# check if needed\nprint(tokenized_train[0])\nprint(tokenized_train.column_names)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T17:10:29.863762Z","iopub.execute_input":"2025-11-02T17:10:29.864042Z","iopub.status.idle":"2025-11-02T17:10:29.869368Z","shell.execute_reply.started":"2025-11-02T17:10:29.864023Z","shell.execute_reply":"2025-11-02T17:10:29.868653Z"},"_kg_hide-input":true},"outputs":[{"name":"stdout","text":"{'input_ids': [1, 4007, 404, 358, 29901, 4001, 596, 25828, 4835, 526, 21677, 2999, 1788, 322, 4152, 6987, 526, 8178, 29892, 727, 526, 4549, 24496, 393, 596, 25828, 4835, 1033, 367, 2861, 304, 14919, 21549, 13, 20334, 29901, 1019, 546, 14919, 21549, 14502, 297, 278, 883, 310, 19967, 339, 403, 8709, 29892, 4943, 321, 1218, 29892, 9128, 15058, 29892, 343, 14895, 322, 26681, 362, 338, 5181, 29889, 960, 278, 25828, 4835, 526, 22261, 322, 6403, 1407, 4049, 29892, 263, 3236, 310, 3677, 713, 29916, 21549, 13589, 800, 338, 7088, 1811, 29889, 2823, 263, 11619, 448, 263, 11643, 7163, 2021, 1058, 508, 2225, 29581, 366, 278, 3677, 713, 29916, 21549, 13589, 800, 13, 29934, 10540, 278, 2038, 363, 263, 16500, 411, 694, 16083, 3239, 29889, 29933, 1463, 373, 596, 25828, 4835, 6602, 292, 2999, 5633, 310, 596, 3573, 322, 694, 2821, 1284, 886, 515, 6987, 29892, 372, 29915, 29879, 5517, 393, 366, 1122, 367, 10623, 3277, 14919, 21549, 29889, 910, 4195, 508, 4556, 9128, 25828, 4835, 1316, 408, 2301, 2841, 6788, 29892, 521, 342, 6788, 29892, 10952, 5192, 915, 271, 29892, 14656, 16172, 292, 29892, 8062, 2264, 29892, 322, 21737, 310, 23547, 681, 2264, 470, 8866, 29889, 13, 13, 1762, 1371, 10933, 596, 25828, 4835, 29892, 372, 29915, 29879, 4100, 304, 7536, 277, 675, 1781, 8709, 29892, 4943, 592, 1338, 29892, 9128, 15058, 29892, 322, 26681, 362, 13698, 763, 343, 14895, 29889, 960, 596, 25828, 4835, 24379, 470, 4953, 22261, 29892, 263, 11643, 7163, 2021, 1122, 6907, 263, 3236, 310, 3677, 713, 29916, 21549, 13589, 800, 304, 1371, 366, 4459, 2253, 29889, 3529, 1074, 263, 11619, 304, 5353, 1438, 3987, 4340, 29889, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [1, 4007, 404, 358, 29901, 4001, 596, 25828, 4835, 526, 21677, 2999, 1788, 322, 4152, 6987, 526, 8178, 29892, 727, 526, 4549, 24496, 393, 596, 25828, 4835, 1033, 367, 2861, 304, 14919, 21549, 13, 20334, 29901, 1019, 546, 14919, 21549, 14502, 297, 278, 883, 310, 19967, 339, 403, 8709, 29892, 4943, 321, 1218, 29892, 9128, 15058, 29892, 343, 14895, 322, 26681, 362, 338, 5181, 29889, 960, 278, 25828, 4835, 526, 22261, 322, 6403, 1407, 4049, 29892, 263, 3236, 310, 3677, 713, 29916, 21549, 13589, 800, 338, 7088, 1811, 29889, 2823, 263, 11619, 448, 263, 11643, 7163, 2021, 1058, 508, 2225, 29581, 366, 278, 3677, 713, 29916, 21549, 13589, 800, 13, 29934, 10540, 278, 2038, 363, 263, 16500, 411, 694, 16083, 3239, 29889, 29933, 1463, 373, 596, 25828, 4835, 6602, 292, 2999, 5633, 310, 596, 3573, 322, 694, 2821, 1284, 886, 515, 6987, 29892, 372, 29915, 29879, 5517, 393, 366, 1122, 367, 10623, 3277, 14919, 21549, 29889, 910, 4195, 508, 4556, 9128, 25828, 4835, 1316, 408, 2301, 2841, 6788, 29892, 521, 342, 6788, 29892, 10952, 5192, 915, 271, 29892, 14656, 16172, 292, 29892, 8062, 2264, 29892, 322, 21737, 310, 23547, 681, 2264, 470, 8866, 29889, 13, 13, 1762, 1371, 10933, 596, 25828, 4835, 29892, 372, 29915, 29879, 4100, 304, 7536, 277, 675, 1781, 8709, 29892, 4943, 592, 1338, 29892, 9128, 15058, 29892, 322, 26681, 362, 13698, 763, 343, 14895, 29889, 960, 596, 25828, 4835, 24379, 470, 4953, 22261, 29892, 263, 11643, 7163, 2021, 1122, 6907, 263, 3236, 310, 3677, 713, 29916, 21549, 13589, 800, 304, 1371, 366, 4459, 2253, 29889, 3529, 1074, 263, 11619, 304, 5353, 1438, 3987, 4340, 29889, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}\n['input_ids', 'attention_mask', 'labels']\n","output_type":"stream"}],"execution_count":75},{"cell_type":"markdown","source":"### clean up before train\n","metadata":{"_kg_hide-input":true}},{"cell_type":"code","source":"#clean up before train\n\nimport torch\nimport gc\n\n# delete any model/pipeline objects you won't reuse\n\ntry:\n    del model\nexcept NameError:\n    pass  # ignore if not defined\n\ntry:\n    del trainer\nexcept NameError:\n    pass  # ignore if not defined\n\ngc.collect()\n\n# free  GPU memory\ntorch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T01:48:02.873526Z","iopub.execute_input":"2025-11-03T01:48:02.874103Z","iopub.status.idle":"2025-11-03T01:48:03.617106Z","shell.execute_reply.started":"2025-11-03T01:48:02.874080Z","shell.execute_reply":"2025-11-03T01:48:03.616494Z"},"_kg_hide-input":true},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"### trainer set up","metadata":{"_kg_hide-input":true}},{"cell_type":"code","source":"# trainer set up\nfrom transformers import AutoModelForCausalLM, TrainingArguments, Trainer, default_data_collator\nfrom peft import LoraConfig, get_peft_model\nfrom transformers import DataCollatorForLanguageModeling\n\nmodel = AutoModelForCausalLM.from_pretrained(student_model_id).to(device)\n\n# LoRA configuration (you can tune r and alpha for resource/quality balance)\nlora_config = LoraConfig(\n    r=16,             # lore dimension - used 8 to fit memory\n    lora_alpha=32,   # Scaling factor\n    target_modules=[\"q_proj\", \"v_proj\"],  # Adapter applied to these modules (may need to change for your model!)\n    lora_dropout=0.1,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\n#  model with LoRA adapters\nmodel = get_peft_model(model, lora_config).to(device)\nmodel.train()  # do this all the time\n\n# count (should be low!)\nmodel.print_trainable_parameters()\n\n\ntraining_args = TrainingArguments(\n    output_dir=\"./distilled-student-peft\",\n    num_train_epochs=3,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=1,\n    save_steps=1000,\n    save_total_limit=1,\n    report_to=\"none\",\n    logging_steps=25,\n    remove_unused_columns=False,  # needed for LoRA\n    fp16=True,\n    dataloader_num_workers=0,\n    gradient_checkpointing=False,      \n    max_grad_norm=1.0,\n    optim=\"adamw_torch\",         # 8-bit optimizer\n)\n\n'''\nAlternate values\n    optim=\"paged_adamw_8bit\",         # 8-bit optimizer\n    gradient_checkpointing=True,       # to help with memory\n\n'''\nsmall_tokenized_train = tokenized_train.select(range(50))  # used sometimes for quick check\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset= tokenized_train, \n    data_collator=default_data_collator,\n    \n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T01:48:08.033374Z","iopub.execute_input":"2025-11-03T01:48:08.034085Z","iopub.status.idle":"2025-11-03T01:48:11.156167Z","shell.execute_reply.started":"2025-11-03T01:48:08.034061Z","shell.execute_reply":"2025-11-03T01:48:11.155437Z"},"_kg_hide-output":true,"_kg_hide-input":true},"outputs":[{"name":"stderr","text":"No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"name":"stdout","text":"trainable params: 2,252,800 || all params: 1,102,301,184 || trainable%: 0.2044\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"# clean up disk spaces too\nimport shutil\nshutil.rmtree(\"./distilled-student-peft\", ignore_errors=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T01:45:20.500997Z","iopub.execute_input":"2025-11-03T01:45:20.501580Z","iopub.status.idle":"2025-11-03T01:45:20.505425Z","shell.execute_reply.started":"2025-11-03T01:45:20.501554Z","shell.execute_reply":"2025-11-03T01:45:20.504781Z"},"_kg_hide-input":true},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"### checks before doing the training\n### these are based on learnings - initial runs did not update LORA weights\n### Multiple issues found - not being in tensor format, not haivng grads, etc.","metadata":{"_kg_hide-input":true}},{"cell_type":"code","source":"%%capture\n!pip install -q bitsandbytes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T04:14:23.787528Z","iopub.execute_input":"2025-11-03T04:14:23.788114Z","iopub.status.idle":"2025-11-03T04:14:29.280321Z","shell.execute_reply.started":"2025-11-03T04:14:23.788091Z","shell.execute_reply":"2025-11-03T04:14:29.279368Z"},"_kg_hide-input":true},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":51},{"cell_type":"code","source":"# before the train, check if tokens are right\n# all the below should be True\nsample = tokenized_train[0]\nprint(\"Keys:\", sample.keys())\nprint(\"Has 'labels'?\", 'labels' in sample)\nprint(\"Labels == input_ids?\", sample['labels'] == sample['input_ids'])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T01:48:24.537352Z","iopub.execute_input":"2025-11-03T01:48:24.537630Z","iopub.status.idle":"2025-11-03T01:48:24.543746Z","shell.execute_reply.started":"2025-11-03T01:48:24.537610Z","shell.execute_reply":"2025-11-03T01:48:24.543163Z"}},"outputs":[{"name":"stdout","text":"Keys: dict_keys(['input_ids', 'attention_mask', 'labels'])\nHas 'labels'? True\nLabels == input_ids? True\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"print(model)\n\n#check that LORA is correctly wrapped","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T01:45:33.843898Z","iopub.execute_input":"2025-11-03T01:45:33.844199Z","iopub.status.idle":"2025-11-03T01:45:33.850932Z","shell.execute_reply.started":"2025-11-03T01:45:33.844178Z","shell.execute_reply":"2025-11-03T01:45:33.850301Z"}},"outputs":[{"name":"stdout","text":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): LlamaForCausalLM(\n      (model): LlamaModel(\n        (embed_tokens): Embedding(32000, 2048)\n        (layers): ModuleList(\n          (0-21): 22 x LlamaDecoderLayer(\n            (self_attn): LlamaAttention(\n              (q_proj): lora.Linear(\n                (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n              (v_proj): lora.Linear(\n                (base_layer): Linear(in_features=2048, out_features=256, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=256, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n            )\n            (mlp): LlamaMLP(\n              (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n              (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n              (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n              (act_fn): SiLU()\n            )\n            (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n            (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n          )\n        )\n        (norm): LlamaRMSNorm((2048,), eps=1e-05)\n        (rotary_emb): LlamaRotaryEmbedding()\n      )\n      (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n    )\n  )\n)\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"print(\"Labels sample:\", tokenized_train['labels'][:10])  # Should not be all -100!\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T01:45:54.004983Z","iopub.execute_input":"2025-11-03T01:45:54.005716Z","iopub.status.idle":"2025-11-03T01:45:54.079693Z","shell.execute_reply.started":"2025-11-03T01:45:54.005692Z","shell.execute_reply":"2025-11-03T01:45:54.078981Z"}},"outputs":[{"name":"stdout","text":"Labels sample: [[1, 4007, 404, 358, 29901, 4001, 596, 25828, 4835, 526, 21677, 2999, 1788, 322, 4152, 6987, 526, 8178, 29892, 727, 526, 4549, 24496, 393, 596, 25828, 4835, 1033, 367, 2861, 304, 14919, 21549, 13, 20334, 29901, 1019, 546, 14919, 21549, 14502, 297, 278, 883, 310, 19967, 339, 403, 8709, 29892, 4943, 321, 1218, 29892, 9128, 15058, 29892, 343, 14895, 322, 26681, 362, 338, 5181, 29889, 960, 278, 25828, 4835, 526, 22261, 322, 6403, 1407, 4049, 29892, 263, 3236, 310, 3677, 713, 29916, 21549, 13589, 800, 338, 7088, 1811, 29889, 2823, 263, 11619, 448, 263, 11643, 7163, 2021, 1058, 508, 2225, 29581, 366, 278, 3677, 713, 29916, 21549, 13589, 800, 13, 29934, 10540, 278, 2038, 363, 263, 16500, 411, 694, 16083, 3239, 29889, 29933, 1463, 373, 596, 25828, 4835, 6602, 292, 2999, 5633, 310, 596, 3573, 322, 694, 2821, 1284, 886, 515, 6987, 29892, 372, 29915, 29879, 5517, 393, 366, 1122, 367, 10623, 3277, 14919, 21549, 29889, 910, 4195, 508, 4556, 9128, 25828, 4835, 1316, 408, 2301, 2841, 6788, 29892, 521, 342, 6788, 29892, 10952, 5192, 915, 271, 29892, 14656, 16172, 292, 29892, 8062, 2264, 29892, 322, 21737, 310, 23547, 681, 2264, 470, 8866, 29889, 13, 13, 1762, 1371, 10933, 596, 25828, 4835, 29892, 372, 29915, 29879, 4100, 304, 7536, 277, 675, 1781, 8709, 29892, 4943, 592, 1338, 29892, 9128, 15058, 29892, 322, 26681, 362, 13698, 763, 343, 14895, 29889, 960, 596, 25828, 4835, 24379, 470, 4953, 22261, 29892, 263, 11643, 7163, 2021, 1122, 6907, 263, 3236, 310, 3677, 713, 29916, 21549, 13589, 800, 304, 1371, 366, 4459, 2253, 29889, 3529, 1074, 263, 11619, 304, 5353, 1438, 3987, 4340, 29889, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [1, 4007, 404, 358, 29901, 6324, 29991, 1834, 363, 19383, 596, 9045, 21838, 29991, 1094, 639, 596, 10416, 3461, 29892, 366, 505, 10075, 13977, 822, 293, 13396, 385, 29747, 29892, 3109, 1353, 310, 11553, 19783, 2719, 29892, 322, 13774, 7200, 1353, 310, 301, 962, 561, 22502, 2167, 29991, 1939, 3440, 1048, 15284, 10376, 29991, 1763, 19138, 675, 29892, 10416, 7117, 526, 12463, 12212, 297, 596, 3573, 29936, 1033, 367, 2861, 304, 777, 304, 29916, 24798, 29892, 297, 1725, 1953, 313, 4561, 29892, 10636, 284, 297, 20309, 29897, 470, 777, 9200, 29899, 21305, 374, 296, 822, 293, 13396, 470, 925, 1178, 21260, 493, 293, 470, 4226, 4766, 2992, 13, 20334, 29901, 960, 366, 505, 5684, 5155, 470, 1101, 29899, 786, 5155, 769, 3113, 437, 451, 19066, 10388, 297, 5007, 304, 502, 13, 29934, 10540, 278, 2038, 363, 263, 16500, 411, 694, 16083, 3239, 29889, 29933, 1463, 373, 596, 10416, 1243, 2582, 29892, 372, 5692, 727, 1795, 367, 385, 2228, 411, 596, 10416, 9101, 29889, 26321, 29892, 366, 1122, 505, 13977, 822, 293, 13396, 385, 29747, 29892, 607, 2794, 596, 3573, 1838, 29915, 29873, 505, 3307, 13977, 304, 1207, 9045, 29891, 2654, 10416, 9101, 29889, 19814, 29892, 596, 4796, 10416, 3038, 2302, 29892, 10734, 278, 11553, 19783, 2719, 29892, 2444, 304, 367, 4482, 29889, 910, 1033, 12266, 385, 297, 20309, 470, 777, 916, 2228, 29889, 13, 13, 1762, 2253, 2274, 825, 29915, 29879, 2675, 373, 29892, 278, 11619, 5052, 1975, 263, 289, 650, 1766, 798, 4392, 3381, 29889, 910, 1243, 674, 2367, 901, 13173, 2472, 1048, 596, 10416, 29899, 689, 292, 260, 15118, 322, 1371, 24876, 852, 278, 4556, 310, 596, 25828, 4835, 29889, 13, 13, 3644, 366, 505, 738, 5155, 470, 817, 4340, 7542, 2450, 29892, 3113, 1016, 29915, 29873, 19066, 10388, 304, 2244, 29889, 1334, 29915, 276, 1244, 304, 1371, 366, 29991, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [1, 4007, 404, 358, 29901, 306, 2274, 596, 14919, 21549, 29889, 450, 8206, 29899, 449, 1158, 338, 451, 29871, 29896, 29900, 29900, 29995, 17928, 8017, 13, 20334, 29901, 2178, 278, 1900, 13, 29934, 10540, 278, 2038, 363, 263, 16500, 411, 694, 16083, 3239, 29889, 29928, 799, 4121, 993, 29892, 13, 13, 29902, 2274, 596, 21838, 1048, 278, 443, 24681, 7916, 366, 750, 373, 2610, 29871, 29955, 386, 29889, 5806, 278, 8206, 29899, 449, 1158, 338, 263, 3619, 883, 310, 12060, 2761, 29892, 372, 29915, 29879, 451, 2337, 11828, 29892, 607, 1122, 367, 10805, 596, 14919, 21549, 29889, 13, 13, 1576, 1781, 9763, 338, 29892, 366, 508, 1286, 2125, 263, 3271, 758, 5138, 6906, 1243, 408, 596, 1833, 3785, 471, 373, 3786, 29871, 29906, 29906, 299, 29889, 4525, 6987, 526, 1556, 16232, 1048, 29871, 29946, 29899, 29945, 3841, 1156, 263, 13726, 3785, 29889, 960, 366, 505, 4943, 25785, 29892, 445, 723, 367, 2820, 2610, 29871, 29906, 29953, 386, 29899, 29906, 29947, 386, 29889, 13, 13, 12148, 6456, 29892, 263, 6374, 1243, 947, 451, 12695, 2099, 278, 24354, 338, 18500, 9714, 29889, 1670, 526, 1784, 3987, 3625, 304, 366, 29892, 322, 306, 29915, 29885, 1244, 304, 2304, 366, 1549, 445, 29889, 13, 13, 797, 278, 6839, 603, 29892, 1018, 304, 7952, 21732, 322, 8569, 373, 5622, 2562, 310, 7535, 29889, 382, 1218, 1532, 29892, 2805, 20947, 310, 1791, 29892, 322, 4772, 292, 22884, 508, 1371, 596, 3573, 10365, 29889, 13, 13, 29902, 6398, 366, 599, 278, 1900, 2645, 445, 931, 29889, 3529, 1016, 29915, 29873, 19066, 10388, 304, 6159, 714, 565, 366, 505, 738, 5155, 470, 21838, 29889, 13, 13, 25353, 21778, 29892, 13, 29961, 10858, 4408, 29962, 13, 29961, 10858, 20627, 29962, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [1, 4007, 404, 358, 29901, 4525, 526, 6010, 397, 293, 260, 496, 29891, 7543, 423, 322, 596, 382, 11135, 1122, 367, 4226, 565, 366, 526, 451, 2534, 260, 496, 29891, 7543, 423, 2645, 382, 11135, 13, 20334, 29901, 887, 881, 884, 679, 697, 9736, 468, 417, 2109, 11174, 2309, 29889, 3115, 29892, 304, 27450, 1316, 23238, 366, 881, 1018, 304, 274, 820, 2898, 13, 29934, 10540, 278, 2038, 363, 263, 16500, 411, 694, 16083, 3239, 29889, 29933, 1463, 373, 596, 25828, 4835, 29892, 372, 2444, 763, 366, 29915, 345, 1063, 10623, 3277, 10952, 5192, 915, 1446, 29892, 607, 508, 4556, 270, 4981, 3335, 322, 1999, 332, 719, 18551, 29889, 1763, 1371, 2274, 825, 1795, 367, 10805, 445, 29892, 591, 29915, 645, 817, 304, 7512, 777, 6987, 29892, 1316, 408, 263, 13874, 1007, 1243, 322, 11819, 596, 5192, 6554, 29889, 960, 1438, 6987, 1016, 29915, 29873, 3867, 263, 2821, 1234, 29892, 591, 1122, 817, 304, 2050, 901, 4266, 1891, 6987, 2000, 28118, 14017, 29875, 19915, 11898, 29892, 607, 508, 1371, 24876, 852, 322, 7539, 278, 633, 8945, 5192, 18178, 29265, 29889, 13, 13, 10858, 382, 11135, 313, 15436, 307, 7543, 29875, 13342, 29897, 1122, 2615, 4226, 565, 366, 29915, 276, 451, 10623, 3277, 278, 10952, 5192, 915, 1446, 2645, 278, 1243, 29889, 1763, 1371, 10933, 596, 25828, 4835, 29892, 372, 29915, 29879, 884, 13622, 304, 679, 263, 9736, 468, 417, 2109, 3233, 1243, 2309, 29889, 512, 278, 6839, 603, 29892, 565, 366, 4459, 385, 12720, 6421, 373, 29892, 1018, 274, 820, 292, 2898, 408, 372, 1795, 1371, 304, 5040, 278, 10952, 5192, 915, 271, 29889, 1334, 29915, 645, 664, 4208, 304, 1284, 278, 1900, 1650, 363, 366, 29889, 3529, 1016, 29915, 29873, 19066, 10388, 304, 6159, 714, 565, 366, 505, 738, 5155, 470, 21838, 29889, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [1, 4007, 404, 358, 29901, 830, 1658, 363, 23468, 6788, 1122, 367, 2301, 16637, 6788, 470, 14002, 633, 8945, 1907, 13, 20334, 29901, 6324, 29892, 1670, 338, 694, 1513, 9443, 1546, 938, 342, 979, 297, 20309, 322, 23468, 6788, 29889, 450, 938, 342, 979, 297, 20309, 674, 3275, 304, 652, 2749, 354, 29874, 29892, 6668, 1218, 322, 1399, 335, 602, 541, 451, 23468, 6788, 29889, 830, 1658, 363, 23468, 6788, 1122, 367, 2301, 16637, 6788, 470, 14002, 633, 8945, 1907, 29889, 319, 13173, 24899, 936, 4392, 3381, 338, 4312, 304, 24809, 278, 4556, 310, 23468, 6788, 13, 29934, 10540, 278, 2038, 363, 263, 16500, 411, 694, 16083, 3239, 29889, 18567, 727, 29892, 13, 13, 29933, 1463, 373, 278, 2472, 4944, 29892, 372, 2444, 393, 596, 11619, 1838, 29915, 29873, 4658, 393, 278, 6788, 297, 596, 1492, 23468, 338, 4153, 4475, 304, 385, 938, 342, 979, 297, 20309, 29892, 1316, 408, 626, 29877, 774, 293, 270, 952, 296, 708, 29889, 8669, 29892, 896, 12326, 393, 278, 6788, 1795, 367, 2861, 304, 2301, 16637, 5626, 470, 14002, 4828, 297, 596, 23468, 29889, 13, 13, 1762, 9659, 278, 4556, 310, 596, 23468, 6788, 29892, 596, 11619, 5052, 1975, 263, 17826, 4392, 3381, 29889, 910, 674, 1371, 963, 2274, 278, 3876, 4556, 322, 4368, 278, 1900, 14502, 363, 366, 29889, 739, 29915, 29879, 4100, 304, 6456, 393, 445, 4392, 3381, 338, 7618, 1455, 304, 9801, 366, 7150, 278, 8210, 2562, 363, 596, 23468, 6788, 29889, 13, 13, 12148, 1016, 29915, 29873, 15982, 29892, 408, 626, 29877, 774, 293, 270, 952, 296, 708, 756, 1063, 604, 26538, 630, 297, 4644, 6813, 29892, 577, 727, 29915, 29879, 694, 817, 304, 15982, 1048, 393, 10805, 596, 23468, 6788, 29889, 13, 13, 26772, 2562, 322, 7952, 6374, 29991, 1334, 29915, 645, 679, 304, 278, 5970, 310, 445, 4208, 29889, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [1, 4007, 404, 358, 29901, 28277, 373, 278, 4766, 310, 301, 962, 561, 2943, 427, 27489, 882, 29892, 366, 674, 5517, 817, 263, 4768, 3554, 29891, 310, 263, 301, 962, 561, 2943, 13, 20334, 29901, 306, 4966, 445, 528, 5779, 777, 3578, 373, 278, 2228, 13, 29934, 10540, 278, 2038, 363, 263, 16500, 411, 694, 16083, 3239, 29889, 29933, 1463, 373, 596, 1243, 2582, 322, 278, 4766, 310, 301, 962, 561, 2943, 427, 27489, 882, 29892, 372, 2444, 393, 596, 3573, 1122, 505, 263, 1134, 310, 23900, 2000, 301, 962, 561, 4125, 29889, 1763, 9659, 445, 29892, 263, 4768, 3554, 29891, 310, 263, 301, 962, 561, 2943, 674, 367, 4312, 29889, 910, 8792, 20789, 5622, 263, 2319, 4559, 310, 278, 301, 962, 561, 2943, 363, 4340, 4392, 3381, 29889, 450, 4768, 3554, 29891, 2582, 674, 1371, 502, 2274, 278, 1134, 310, 301, 962, 561, 4125, 322, 278, 1900, 14502, 3987, 363, 366, 29889, 306, 2274, 445, 508, 367, 19813, 29892, 541, 372, 29915, 29879, 4100, 304, 6456, 393, 4688, 15326, 322, 14502, 508, 11180, 11157, 714, 26807, 29889, 306, 29915, 645, 664, 411, 366, 304, 564, 3881, 445, 4768, 3554, 29891, 322, 2304, 366, 10106, 278, 1889, 29889, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [1, 4007, 404, 358, 29901, 306, 723, 5700, 714, 274, 3470, 29872, 457, 322, 274, 3470, 21084, 630, 367, 369, 1179, 29889, 8561, 1854, 366, 29915, 345, 1063, 2805, 20947, 310, 1791, 322, 306, 4658, 366, 881, 1369, 11223, 2253, 13, 20334, 29901, 15043, 29892, 960, 445, 338, 871, 263, 25828, 290, 393, 756, 1063, 10464, 363, 278, 4940, 29871, 29906, 3841, 306, 723, 6907, 366, 7910, 596, 22576, 938, 1296, 363, 278, 2446, 3196, 3841, 773, 8296, 322, 10849, 399, 1299, 1001, 29889, 8561, 1854, 366, 29915, 345, 1063, 2805, 20947, 310, 1791, 322, 306, 4658, 366, 881, 1369, 11223, 2253, 13, 29934, 10540, 278, 2038, 363, 263, 16500, 411, 694, 16083, 3239, 29889, 29928, 799, 4121, 993, 29892, 13, 13, 29933, 1463, 373, 596, 25828, 4835, 310, 270, 4981, 3335, 322, 3578, 2813, 287, 2264, 297, 278, 5683, 17724, 29892, 306, 12326, 372, 1795, 367, 4475, 304, 316, 29882, 2941, 29878, 362, 470, 10225, 310, 1791, 29889, 1763, 1371, 4788, 1403, 403, 445, 29892, 306, 6907, 10231, 596, 4094, 938, 1296, 322, 5662, 3864, 366, 679, 3307, 1791, 29889, 306, 884, 4368, 28967, 714, 274, 3470, 29872, 457, 322, 274, 3470, 21084, 630, 367, 369, 1179, 408, 896, 508, 29126, 304, 316, 29882, 2941, 29878, 362, 29889, 960, 596, 25828, 4835, 24379, 29892, 372, 723, 367, 1900, 304, 20410, 385, 28573, 411, 596, 11619, 363, 263, 17826, 4392, 3381, 322, 777, 6987, 304, 5751, 714, 916, 7037, 9946, 29889, 13, 13, 29902, 4658, 1438, 6576, 674, 1371, 366, 4459, 2253, 4720, 29889, 3529, 2125, 2562, 322, 1235, 592, 1073, 565, 366, 505, 738, 5155, 470, 21838, 29889, 13, 13, 25353, 21778, 29892, 13, 29961, 10858, 4408, 29962, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [1, 4007, 404, 358, 29901, 16809, 304, 24221, 310, 445, 302, 7143, 3876, 727, 338, 6788, 297, 596, 1250, 322, 521, 342, 29889, 16809, 304, 2179, 29880, 3958, 310, 24479, 727, 508, 367, 6788, 4771, 362, 13, 20334, 29901, 306, 674, 22939, 366, 304, 437, 341, 3960, 16294, 562, 293, 805, 457, 29892, 379, 10363, 518, 3210, 29923, 1254, 29962, 322, 27295, 29954, 29899, 25778, 518, 1089, 345, 13417, 428, 6559, 29962, 2831, 1438, 25828, 4835, 3483, 2710, 293, 322, 452, 2192, 29873, 19783, 293, 13589, 362, 508, 367, 4687, 29889, 11661, 29875, 1228, 27580, 763, 9238, 10678, 618, 322, 10115, 2556, 29220, 27580, 674, 2367, 4996, 18892, 29889, 960, 366, 505, 5684, 5155, 470, 1101, 29899, 786, 9365, 769, 3113, 437, 451, 19066, 10388, 297, 5007, 304, 502, 13, 29934, 10540, 278, 2038, 363, 263, 16500, 411, 694, 16083, 3239, 29889, 29933, 1463, 373, 596, 25828, 4835, 29892, 278, 11619, 12326, 29879, 393, 366, 1122, 505, 263, 12534, 26902, 302, 7143, 297, 596, 7568, 1250, 10805, 278, 6788, 29892, 607, 508, 884, 17937, 403, 304, 596, 521, 342, 29889, 19814, 29892, 727, 1795, 367, 263, 1108, 411, 278, 10416, 4972, 304, 596, 521, 342, 29889, 1763, 9659, 1438, 8872, 293, 1080, 29892, 278, 11619, 5052, 1975, 2211, 6987, 29901, 341, 3960, 313, 517, 25917, 596, 805, 457, 511, 385, 382, 11135, 313, 517, 1423, 596, 5192, 511, 322, 385, 27295, 29954, 29899, 15633, 313, 29874, 302, 7143, 13417, 428, 6559, 467, 4525, 6987, 674, 1371, 502, 2274, 278, 3876, 4556, 310, 596, 25828, 4835, 29889, 512, 278, 6839, 603, 29892, 278, 11619, 14661, 5622, 6788, 13589, 362, 322, 773, 12871, 29220, 27580, 304, 10933, 596, 6788, 29889, 960, 596, 25828, 4835, 24379, 470, 281, 943, 264, 29892, 3113, 6958, 596, 11619, 7389, 29889, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [1, 4007, 404, 358, 29901, 3189, 874, 1122, 367, 4687, 411, 29871, 29906, 29889, 960, 10416, 12959, 338, 451, 20704, 411, 278, 2847, 3248, 267, 769, 278, 18469, 437, 344, 1122, 367, 11664, 304, 29871, 29945, 30081, 29885, 29887, 13, 20334, 29901, 29871, 29945, 286, 29887, 2748, 14218, 3412, 411, 2420, 294, 29871, 29945, 30081, 29885, 29887, 14218, 322, 278, 10416, 12959, 881, 367, 3785, 1711, 11819, 287, 304, 1423, 1009, 2933, 13, 29934, 10540, 278, 2038, 363, 263, 16500, 411, 694, 16083, 3239, 29889, 29933, 1463, 373, 596, 1857, 6434, 29892, 278, 11619, 14661, 393, 366, 1369, 5622, 263, 13589, 362, 2000, 3189, 874, 2904, 313, 29945, 286, 29887, 29897, 322, 4186, 4428, 29883, 313, 29945, 286, 29887, 29897, 14218, 408, 27809, 304, 278, 13589, 1475, 366, 892, 9251, 773, 29889, 4525, 716, 13589, 800, 526, 11828, 297, 640, 22155, 1880, 10416, 12959, 29892, 2788, 304, 596, 3517, 6743, 29889, 13, 13, 1576, 11619, 674, 11819, 596, 10416, 12959, 25704, 304, 9801, 1438, 13589, 800, 526, 1985, 17583, 29889, 960, 596, 10416, 12959, 9242, 1880, 1156, 6257, 1438, 13589, 800, 29892, 278, 11619, 1122, 2050, 10231, 278, 437, 344, 310, 3189, 874, 2904, 304, 29871, 29945, 286, 29887, 29889, 13, 13, 3112, 29915, 29879, 4100, 304, 2125, 1438, 13589, 800, 408, 2225, 23059, 322, 14333, 599, 21467, 8167, 1860, 304, 1423, 596, 10416, 12959, 322, 10365, 278, 14502, 3814, 565, 5181, 29889, 910, 674, 1371, 3013, 596, 10416, 12959, 1090, 2761, 29892, 27668, 278, 12045, 310, 752, 5795, 4475, 304, 1880, 10416, 12959, 29889, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [1, 4007, 404, 358, 29901, 3869, 366, 508, 2125, 9656, 2435, 324, 363, 2301, 2841, 6788, 29889, 1522, 9543, 310, 2625, 9545, 310, 9656, 2435, 324, 13, 20334, 29901, 960, 366, 505, 5684, 5155, 470, 1101, 29899, 786, 9365, 769, 3113, 437, 451, 19066, 10388, 297, 5007, 304, 502, 13, 29934, 10540, 278, 2038, 363, 263, 16500, 411, 694, 16083, 3239, 29889, 1576, 11619, 756, 9076, 287, 596, 6434, 322, 1339, 17180, 372, 29915, 29879, 9109, 363, 366, 304, 2125, 9656, 2435, 324, 363, 596, 2301, 2841, 6788, 29889, 2398, 29892, 372, 29915, 29879, 4100, 304, 367, 9543, 393, 9656, 2435, 324, 508, 505, 2625, 9545, 29892, 577, 366, 881, 367, 3458, 1319, 310, 1906, 29889, 4001, 366, 29915, 345, 4586, 596, 9232, 2251, 293, 314, 445, 7250, 29892, 366, 508, 2125, 9656, 2435, 324, 408, 4312, 363, 596, 6788, 18892, 29889, 960, 366, 505, 738, 5684, 5155, 470, 21838, 29892, 3113, 1016, 29915, 29873, 19066, 10388, 304, 6159, 714, 304, 502, 363, 4340, 27323, 29889, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]]\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"#check if gradients flow through lora\n# there should be non-zero gradient...\n\nimport torch\n\n\n# Explicitly set LoRA weights to require gradients\nfor n, p in model.named_parameters():\n    if 'lora_' in n:\n        p.requires_grad = True\n\nmodel.train()  # Ensure model is in train mode\nbatch = tokenized_train[30]\ninputs = {k: torch.tensor([v]).to(device) for k, v in batch.items() if k in ['input_ids', 'attention_mask', 'labels']}\n\nfor k in range(5):\n    outputs = model(**inputs)\n    loss = outputs.loss\n    loss.backward()\n    \n    # Check LoRA gradients\n    for name, param in model.named_parameters():\n        if 'lora_' in name and 'lora_A' in name:  # For A/B matrices\n            grad = param.grad\n            if grad is not None:\n                print(f\"LoRA A gradient norm: {grad.norm():.6f}\")\n            else:\n                print(f\"LoRA A gradient is None!\")\n            break  # Show one example\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T01:47:26.999051Z","iopub.execute_input":"2025-11-03T01:47:26.999345Z","iopub.status.idle":"2025-11-03T01:47:30.177814Z","shell.execute_reply.started":"2025-11-03T01:47:26.999322Z","shell.execute_reply":"2025-11-03T01:47:30.177209Z"}},"outputs":[{"name":"stdout","text":"LoRA A gradient norm: 0.160904\nLoRA A gradient norm: 0.326587\nLoRA A gradient norm: 0.484545\nLoRA A gradient norm: 0.651751\nLoRA A gradient norm: 0.818384\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"## Main Trainer Code\n\n### prints some weights before and after training","metadata":{}},{"cell_type":"code","source":"# now the actual training\n\n#tokenized_train = tokenized_train.to(\"cuda:0\")\n#trainer = trainer.to(\"cuda:0\")\n\nmodel.enable_input_require_grads()\n\nmodel.train()\n\n# check weights BEFORE training\nprint(\"\\nBefore training:\")\nfor name, param in model.named_parameters():\n    if 'lora_A' in name:\n        print(f\"{name}: min={param.min():.4f}, max={param.max():.4f}, mean={param.mean():.4f}\")\n        break\n\nif len(tokenized_train) > 0:\n    trainer.train()\n\nelse:\n    print(\"ERROR: No train samples available for Trainer.\")\n\n# check weights AFTER training\nprint(\"\\nAfter training:\")\nfor name, param in model.named_parameters():\n    if 'lora_A' in name:\n        print(f\"{name}: min={param.min():.4f}, max={param.max():.4f}, mean={param.mean():.4f}\")\n        break\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T01:48:44.481529Z","iopub.execute_input":"2025-11-03T01:48:44.481807Z","iopub.status.idle":"2025-11-03T01:56:06.298956Z","shell.execute_reply.started":"2025-11-03T01:48:44.481787Z","shell.execute_reply":"2025-11-03T01:56:06.298222Z"},"_kg_hide-input":true},"outputs":[{"name":"stdout","text":"\nBefore training:\nbase_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight: min=-0.0221, max=0.0221, mean=0.0001\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='174' max='174' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [174/174 07:18, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>25</td>\n      <td>2.658000</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.945300</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>0.705500</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.641400</td>\n    </tr>\n    <tr>\n      <td>125</td>\n      <td>0.655800</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.585700</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"\nAfter training:\nbase_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight: min=-0.0251, max=0.0252, mean=0.0000\n","output_type":"stream"}],"execution_count":25},{"cell_type":"markdown","source":"### Learning - check weights, etc. \n### Improper training may result in poor quality model","metadata":{"_kg_hide-input":true}},{"cell_type":"code","source":"# check weights after training...\nfor name, param in model.named_parameters():\n    if 'lora_B' in name:\n        print(f\"{name}:\")\n        print(f\"  min={param.min():.4f}, max={param.max():.4f}, mean={param.mean():.4f}\")\n        # Also check std dev to see if weights are meaningful\n        print(f\"  std={param.std():.4f}\")\n        break\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T01:57:00.081719Z","iopub.execute_input":"2025-11-03T01:57:00.082582Z","iopub.status.idle":"2025-11-03T01:57:00.088151Z","shell.execute_reply.started":"2025-11-03T01:57:00.082548Z","shell.execute_reply":"2025-11-03T01:57:00.087462Z"},"_kg_hide-input":true},"outputs":[{"name":"stdout","text":"base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight:\n  min=-0.0041, max=0.0040, mean=0.0000\n  std=0.0013\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"# check codes\nfor name, param in model.named_parameters():\n    if 'lora' in name:\n        print(f\"{name}: requires_grad={param.requires_grad}\")\n        break\n\n# model mode\nprint(f\"Model training mode: {model.training}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T01:57:12.673803Z","iopub.execute_input":"2025-11-03T01:57:12.674111Z","iopub.status.idle":"2025-11-03T01:57:12.678804Z","shell.execute_reply.started":"2025-11-03T01:57:12.674089Z","shell.execute_reply":"2025-11-03T01:57:12.678263Z"},"_kg_hide-input":true},"outputs":[{"name":"stdout","text":"base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight: requires_grad=True\nModel training mode: True\n","output_type":"stream"}],"execution_count":27},{"cell_type":"markdown","source":"### now save the models ","metadata":{"_kg_hide-input":true}},{"cell_type":"code","source":"#model.save_pretrained(\"distilled-student-model\")\n#tokenizer.save_pretrained(\"distilled-student-model\")\n\nmodel.save_pretrained(\"distilled-student-peft-adapter\")\ntokenizer.save_pretrained(\"distilled-student-peft-adapter\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T01:57:27.541703Z","iopub.execute_input":"2025-11-03T01:57:27.541991Z","iopub.status.idle":"2025-11-03T01:57:27.770406Z","shell.execute_reply.started":"2025-11-03T01:57:27.541967Z","shell.execute_reply":"2025-11-03T01:57:27.769689Z"},"_kg_hide-input":true},"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"('distilled-student-peft-adapter/tokenizer_config.json',\n 'distilled-student-peft-adapter/special_tokens_map.json',\n 'distilled-student-peft-adapter/tokenizer.model',\n 'distilled-student-peft-adapter/added_tokens.json',\n 'distilled-student-peft-adapter/tokenizer.json')"},"metadata":{}}],"execution_count":28},{"cell_type":"code","source":"# as usual, enable the files for downlaod\n\nimport shutil\nimport os\n\n# Path to your output directory\noutput_dir = \"./distilled-student-peft-adapter\"\nzip_path = \"./distilled-student-peft.zip\"\n\n# Zip entire folder\nshutil.make_archive(output_dir, 'zip', output_dir)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T03:15:57.036053Z","iopub.execute_input":"2025-11-03T03:15:57.036364Z","iopub.status.idle":"2025-11-03T03:15:57.614707Z","shell.execute_reply.started":"2025-11-03T03:15:57.036346Z","shell.execute_reply":"2025-11-03T03:15:57.614172Z"}},"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/distilled-student-peft-adapter.zip'"},"metadata":{}}],"execution_count":41},{"cell_type":"code","source":"#clean up before inference\n\nimport torch\nimport gc\n\n# delete any model/pipeline objects you won't reuse\n\ntry:\n    del model\nexcept NameError:\n    pass  # ignore if not defined\n\ntry:\n    del trainer\nexcept NameError:\n    pass  # ignore if not defined\n\ngc.collect()\n\n# free  GPU memory\ntorch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T04:16:37.316677Z","iopub.execute_input":"2025-11-03T04:16:37.316996Z","iopub.status.idle":"2025-11-03T04:16:38.031692Z","shell.execute_reply.started":"2025-11-03T04:16:37.316968Z","shell.execute_reply":"2025-11-03T04:16:38.031120Z"},"_kg_hide-input":true},"outputs":[],"execution_count":52},{"cell_type":"code","source":"gc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T04:16:42.318589Z","iopub.execute_input":"2025-11-03T04:16:42.318830Z","iopub.status.idle":"2025-11-03T04:16:42.737243Z","shell.execute_reply.started":"2025-11-03T04:16:42.318814Z","shell.execute_reply":"2025-11-03T04:16:42.736581Z"},"_kg_hide-input":true},"outputs":[{"execution_count":53,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}],"execution_count":53},{"cell_type":"markdown","source":"# Inference code \n\n## inference on train set, and also test set\n## evaluate metrics","metadata":{"_kg_hide-input":true}},{"cell_type":"code","source":"# first the model\n\n\n\nfrom transformers import pipeline\n\n#infer_pipe = pipeline(\"text-generation\", model=\"distilled-student-peft-adapter\", \n#                      tokenizer=\"distilled-student-peft-adapter\", device=0)\n\n# need to use peft load  \nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import PeftModel\n\nmodel_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\nadapter_path = \"distilled-student-peft-adapter\"   # change as needed\n\ntokenizer = AutoTokenizer.from_pretrained(adapter_path)\nbase_model = AutoModelForCausalLM.from_pretrained(model_id)\n\n# IMPORTANT: Use is_trainable=True for proper adapter loading\nmodel = PeftModel.from_pretrained(base_model, adapter_path, is_trainable=True).to(device)\n\nprint(\"Trainable params after load:\")\nmodel.print_trainable_parameters()\nprint(\"Compare with previous data to check match...\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T04:17:03.679073Z","iopub.execute_input":"2025-11-03T04:17:03.679836Z","iopub.status.idle":"2025-11-03T04:17:07.481228Z","shell.execute_reply.started":"2025-11-03T04:17:03.679812Z","shell.execute_reply":"2025-11-03T04:17:07.480608Z"},"_kg_hide-input":true},"outputs":[{"name":"stdout","text":"Trainable params after load:\ntrainable params: 2,252,800 || all params: 1,102,301,184 || trainable%: 0.2044\nCompare with previous data to check match...\n","output_type":"stream"}],"execution_count":54},{"cell_type":"markdown","source":"### sample inference\n### check reasonableness before batch inference","metadata":{"_kg_hide-input":true}},{"cell_type":"code","source":"import torch\n# Inference example\n\nprompt = f\"\"\"Assessment: You have typhoiditis\nPlan: Take paracetamol and acetenomycin\nRewrite the above for a patient with no medical background.\"\"\"\n\n\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\nwith torch.no_grad():\n    gen_output = model.generate(\n        **inputs,\n        max_new_tokens=256,\n        do_sample=True,\n        top_p=0.95,\n        temperature=0.7,\n        eos_token_id=tokenizer.eos_token_id\n    )\n\n\n# Full output includes the prompt + generated text\nfull_output = tokenizer.decode(gen_output[0], skip_special_tokens=True)\n\n# Extract only the generated part (remove prompt)\ngenerated_text = full_output[len(prompt):].strip()\n\nprint(\"Prompt: \", prompt)\nprint(\"\\nGenerated summary:\")\nprint(generated_text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T04:17:14.732927Z","iopub.execute_input":"2025-11-03T04:17:14.733243Z","iopub.status.idle":"2025-11-03T04:17:16.863932Z","shell.execute_reply.started":"2025-11-03T04:17:14.733222Z","shell.execute_reply":"2025-11-03T04:17:16.862996Z"},"_kg_hide-input":true},"outputs":[{"name":"stdout","text":"Prompt:  Assessment: You have typhoiditis\nPlan: Take paracetamol and acetenomycin\nRewrite the above for a patient with no medical background.\n\nGenerated summary:\nYour patient has been diagnosed with typhoiditis. The doctor recommends taking paracetamol and acetylmycin to treat the infection. These medications are commonly used for typhoid fever. The patient can take them for a few days to help manage the symptoms.\n","output_type":"stream"}],"execution_count":55},{"cell_type":"markdown","source":"### Load the Teacher data","metadata":{"_kg_hide-input":true}},{"cell_type":"code","source":"import pandas as pd\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import PeftModel\nimport json\n\n#change to required paths...\nteacher_file = \"/kaggle/input/teachersummary/patient_summary_finetune_data_allrecords.jsonl\"\n\nwith open(teacher_file, \"r\") as f:\n    teacher_data = [json.loads(line) for line in f]\n\nprint(\"Full data has...\", teacher_data[0].keys())\nprint(\"Input has ...\",teacher_data[0]['input'].keys())\nprint(\"Sample output ...\", teacher_data[0]['output'][:150])  # preview\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T04:17:34.409206Z","iopub.execute_input":"2025-11-03T04:17:34.409876Z","iopub.status.idle":"2025-11-03T04:17:34.431180Z","shell.execute_reply.started":"2025-11-03T04:17:34.409853Z","shell.execute_reply":"2025-11-03T04:17:34.430601Z"},"_kg_hide-input":true},"outputs":[{"name":"stdout","text":"Full data has... dict_keys(['input', 'output'])\nInput has ... dict_keys(['subjective', 'objective', 'assessment', 'plan'])\nSample output ... Hello,\n\nYour father-in-law's creatinine level is high, which suggests a potential kidney issue. To better understand the cause and extent of the probl\n","output_type":"stream"}],"execution_count":56},{"cell_type":"markdown","source":"### train test split - same as before \n### do the inference separately for train and test, and store the outputs\n### metrics computation done separately \n","metadata":{"_kg_hide-input":true}},{"cell_type":"code","source":"# helper function to infer as a batch\n\nimport torch\n\ndef infer_and_save(data, ofile):\n    # data - list format data for inference\n    # ofile - name of output file \n        \n    ### create prompt string - should match with training setup\n\n    def build_prompt(record):\n        assessment = record[\"input\"][\"assessment\"].strip()\n        plan = record[\"input\"][\"plan\"].strip()\n        prompt = f\"Assessment: {assessment}\\nPlan: {plan}\\nRewrite the above for a patient with no medical background.\"\n        return prompt\n    \n    prompts = [build_prompt(rec) for rec in data]\n    \n    ### batch inference\n    \n    BATCH_SIZE = 24\n    MAX_NEW_TOKENS = 256   # adjusted to fit into GPU\n    \n    def batch_infer(model, tokenizer, prompts, batch_size=BATCH_SIZE):\n        \n        preds = []\n        for i in range(0, len(prompts), batch_size):\n            #print(\"next batch called\")\n            batch_prompts = prompts[i:i+batch_size]\n            tokens = tokenizer(batch_prompts, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n            with torch.no_grad():\n                output = model.generate(**tokens, max_new_tokens=MAX_NEW_TOKENS)\n            preds.extend(tokenizer.batch_decode(output, skip_special_tokens=True))\n        return preds\n    \n    predictions_raw = batch_infer(model, tokenizer, prompts)\n    predictions = []\n    REWRITE_CUE = 'Rewrite the above for a patient with no medical background.'\n    \n    for prompt, output in zip(prompts, predictions_raw):\t\n    \n        if REWRITE_CUE in output:\n            pred = output.split(REWRITE_CUE, 1)[-1].strip()\n        else:\n        # Fallback: try removing the whole prompt, but be careful\n            pred = output[len(prompt):].strip() if output.startswith(prompt) else output.strip()\n        # If prediction is still blank after these checks, log it explicitly for debugging\n        if not pred:\n            print('Blank prediction after stripping:', repr(output[:80]))    \n        \n        predictions.append(pred)\n    \n    ### save the predictions (and reference) for metrics\n    \n    import pandas as pd\n    \n    df = pd.DataFrame({\n        \"prompt\": prompts,\n        \"reference\": [rec[\"output\"] for rec in data],\n        \"prediction\": predictions\n    })\n    df.to_csv(ofile, index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T04:17:42.832990Z","iopub.execute_input":"2025-11-03T04:17:42.833657Z","iopub.status.idle":"2025-11-03T04:17:42.842803Z","shell.execute_reply.started":"2025-11-03T04:17:42.833635Z","shell.execute_reply":"2025-11-03T04:17:42.842073Z"},"_kg_hide-input":true},"outputs":[],"execution_count":57},{"cell_type":"code","source":"trdata, tsdata = train_test_split(teacher_data, train_size=0.5, random_state=42, shuffle=True)\n\ntroutput = \"student_train_output.csv\"\n\nprint(\"Starting train inference\")\ninfer_and_save(trdata, troutput)","metadata":{"trusted":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2025-11-03T04:17:50.036553Z","iopub.execute_input":"2025-11-03T04:17:50.037107Z","iopub.status.idle":"2025-11-03T04:27:02.974311Z","shell.execute_reply.started":"2025-11-03T04:17:50.037084Z","shell.execute_reply":"2025-11-03T04:27:02.973557Z"}},"outputs":[{"name":"stdout","text":"Starting train inference\nBlank prediction after stripping: 'Assessment: I understand your anxiety. The pull-out method is not 100% foolproof'\nBlank prediction after stripping: 'Assessment: Covers may be started with 2. If blood pressure is not controlled wi'\nBlank prediction after stripping: 'Assessment: Yes you can take Tylenol for muscle pain. Be aware of side effects o'\nBlank prediction after stripping: 'Assessment: Your current symptom may be related to residual depression\\nPlan: In '\nBlank prediction after stripping: 'Assessment: Hello, I would explain that your symptoms could be suggestive of thy'\nBlank prediction after stripping: 'Assessment: Yes you can take Tylenol for muscle pain. Be aware of side effects o'\nBlank prediction after stripping: \"Assessment: You didn't mention your age or vascular status. In the worst case sc\"\nBlank prediction after stripping: 'Assessment: Moreover, you have taken in the preconception phase where if at all '\nBlank prediction after stripping: 'Assessment: I would advise you to get an MRI cervical spine done. Meanwhile, try'\nBlank prediction after stripping: 'Assessment: You are 9 months pregnant. Now having high BP and proteinuria indica'\nBlank prediction after stripping: 'Assessment: It is very common problem that our body stops secreting natural ster'\nBlank prediction after stripping: 'Assessment: Thanks for your query. Read the history and understood the problems\\n'\nBlank prediction after stripping: 'Assessment: I would suggest that you see your doctor. Your baby maybe having bro'\nBlank prediction after stripping: 'Assessment: This pain is mostly due to increase in inflammation. In your case tw'\nBlank prediction after stripping: 'Assessment: Seizures may be caused by sleep deprivation. Another examination tha'\nBlank prediction after stripping: 'Assessment: I have gone through your question. I can understand your concern\\nPla'\nBlank prediction after stripping: 'Assessment: I can understand your concern. Your sign and symptoms are related to'\nBlank prediction after stripping: 'Assessment: There are some other conditions too\\nPlan: Anand, Psychologist\\nRewrit'\nBlank prediction after stripping: 'Assessment: According to symptoms explained by you, it seems to be the condition'\nBlank prediction after stripping: 'Assessment: Fatty liver is a chronic pathology by excess fat in liver. The main '\nBlank prediction after stripping: 'Assessment: Give her zinc supplements like Sure and high protein diet like meat,'\nBlank prediction after stripping: \"Assessment: Dear Amber, as far as you're hitting your eye and swelling is concer\"\nBlank prediction after stripping: 'Assessment: Another reason of psychosis (hallucination) can be due to involvemen'\nBlank prediction after stripping: 'Assessment: Increased platelets level is good and goes against the diagnosis of '\nBlank prediction after stripping: 'Assessment: So if you reach your due date with your baby in the head up position'\nBlank prediction after stripping: 'Assessment: The most common causes for fatty liver are Alcohol, obesity,diabetes'\nBlank prediction after stripping: 'Assessment: He could be compelling you to abort because of some compelling issue'\nBlank prediction after stripping: 'Assessment: I will suggest you the best possible treatment options\\nPlan: I will '\nBlank prediction after stripping: 'Assessment: I can understand your concern. You may have some infection like epid'\nBlank prediction after stripping: 'Assessment: If the condition does not resolve, or she develops some fever, then '\nBlank prediction after stripping: 'Assessment: I suggest you not to worry much. Semen is a waste product\\nPlan: I su'\nBlank prediction after stripping: 'Assessment: The most likely reason for these symptoms are self limiting viral in'\nBlank prediction after stripping: 'Assessment: If the tooth is infected then it may lead to enlarge lymph node. Fir'\nBlank prediction after stripping: 'Assessment: In your study patient, possibility of hypercarbia (raised carbon dio'\nBlank prediction after stripping: 'Assessment: The inhalation of these substances causes narrowing of respiratory p'\nBlank prediction after stripping: 'Assessment: I suggest you not to worry much. I sincerely opine that the problem '\nBlank prediction after stripping: 'Assessment: As you getting these symptoms frequently, you require physical exami'\nBlank prediction after stripping: 'Assessment: If symptoms persist then surgical decompression may help. Medication'\nBlank prediction after stripping: 'Assessment: l This can be treated with vitamin-becosules supplements. -oral cort'\nBlank prediction after stripping: 'Assessment: Your concern may be pattern hair loss along with halogen effluvium\\nP'\nBlank prediction after stripping: 'Assessment: Thanks for the information. The cause of the cramping in the upper a'\nBlank prediction after stripping: 'Assessment: You simply must see a surgeon and do relevant tests to rule out acut'\nBlank prediction after stripping: 'Assessment: I suggest you not to worry much. I can understand your emotions\\nPlan'\nBlank prediction after stripping: 'Assessment: Since you are a diabetic patient, so any of above possibility may be'\nBlank prediction after stripping: 'Assessment: A colonial cyst is a condition which occurs in the skin over the glu'\nBlank prediction after stripping: 'Assessment: However you have to consult your physician for definitive diagnosis\\n'\nBlank prediction after stripping: 'Assessment: Hello and welcome to Chat Doctor, A lump in the vaginal wall could b'\nBlank prediction after stripping: 'Assessment: *Ultrasonography of abdomen, *Colonoscopy All these investigations m'\nBlank prediction after stripping: 'Assessment: The lump in the throat seems to be abscessed of the plantar tissue o'\nBlank prediction after stripping: 'Assessment: Go for routine investigations to confirm the diagnosis\\nPlan: Weight '\nBlank prediction after stripping: 'Assessment: As you mentioned that your son is having high grade fever 102-103 F,'\nBlank prediction after stripping: 'Assessment: To confirm exact pathological causes I may advise you a blood differ'\nBlank prediction after stripping: 'Assessment: Pain in the forearm can be due to weakness or can be due to overuse.'\nBlank prediction after stripping: 'Assessment: For low platelet count there is definite guideline for management de'\nBlank prediction after stripping: 'Assessment: The most likely cause for your symptoms is benign paroxysmal positio'\nBlank prediction after stripping: 'Assessment: Thank you for posting your query. Let me make it clear about uses of'\nBlank prediction after stripping: 'Assessment: Bone scan may help for diagnosis\\nPlan: If you have additional questi'\nBlank prediction after stripping: 'Assessment: Esophageal and or bronchial cyst if symptomatic than should be remov'\nBlank prediction after stripping: 'Assessment: Depending upon your age, these are necessary to have before making a'\nBlank prediction after stripping: 'Assessment: I suggest you not to worry much. I can understand your emotions and '\nBlank prediction after stripping: 'Assessment: Cycloreg leads to medicinal stimulation of the ovaries and in turn r'\nBlank prediction after stripping: 'Assessment: Consult dentist and go for Scaling and root planning. Hope this will'\nBlank prediction after stripping: 'Assessment: In my opinion you should get done ECG and 2D Echo to rule out cardia'\nBlank prediction after stripping: 'Assessment: Hep c positive with no viral load means you have the hepatitis C vir'\nBlank prediction after stripping: \"Assessment: She could be suffering from peptic this order. If condition doesn't \"\nBlank prediction after stripping: 'Assessment: I would suggest a complete evaluation before any more induction. Tre'\nBlank prediction after stripping: 'Assessment: In this condition you have to quit the habit which you are having an'\nBlank prediction after stripping: 'Assessment: Avoid smoking strictly as it can aggravate condition. If condition s'\nBlank prediction after stripping: 'Assessment: At young age you had got blood clot. I suggest you to rule out any b'\nBlank prediction after stripping: 'Assessment: Its fine may be your wisdom tooth is impacted, and it has got period'\nBlank prediction after stripping: 'Assessment: Cause of it is weakness and vitamin deficiencies. You are advised to'\nBlank prediction after stripping: 'Assessment: If the pain is mild then it appears to be a minor problem that is tr'\nBlank prediction after stripping: 'Assessment: These infections occur mostly in crowded places like hospitals, nurs'\nBlank prediction after stripping: 'Assessment: Since two days ago you cleaned your house and still symptoms present'\nBlank prediction after stripping: 'Assessment: Understand your concerns. You want contraception long term:you can u'\nBlank prediction after stripping: 'Assessment: I can understand your concern. He has history of buccal cancer\\nPlan:'\nBlank prediction after stripping: 'Assessment: Opinion and examination with Surgeon is of utmost importance since h'\nBlank prediction after stripping: 'Assessment: Modality of treatment also depends on the direction of the flow of b'\nBlank prediction after stripping: 'Assessment: The description given by you suits the diagnosis of urticaria. Visit'\nBlank prediction after stripping: 'Assessment: There are less chance of spread of organism in pleurisy case until y'\nBlank prediction after stripping: 'Assessment: After getting the correct diagnosis take proper treatment\\nPlan: Afte'\nBlank prediction after stripping: 'Assessment: Please consult qualified General Surgeon for clinical examination to'\nBlank prediction after stripping: 'Assessment: Crowded conditions and poor hygiene contributes to higher H\\nPlan: Ho'\nBlank prediction after stripping: 'Assessment: Your clinical observations are very astute if I may say so & would h'\nBlank prediction after stripping: 'Assessment: The fungus can sustain for longer duration of time in the wet and mo'\nBlank prediction after stripping: 'Assessment: This is Chat Doctor. -I studied your query in depth and Understood y'\nBlank prediction after stripping: 'Assessment: The pain and swelling could be because of the tooth infection second'\nBlank prediction after stripping: 'Assessment: There may be a slight increased risk of cervical incompetence (when '\nBlank prediction after stripping: 'Assessment: Most common side effects of Davis Disks is upper respiratory tract i'\nBlank prediction after stripping: 'Assessment: Had I been at your place I think I would meet a general physician or'\nBlank prediction after stripping: 'Assessment: Spironolactone will not let to a big Chat Doctor. It is Ok to take t'\nBlank prediction after stripping: 'Assessment: Brown spots on glans penis are mostly due to infection of glans peni'\nBlank prediction after stripping: 'Assessment: I can understand your concern. Before I would comment on the missed '\nBlank prediction after stripping: 'Assessment: It may be due to many reasons like hepatitis, alcohol intake, altere'\nBlank prediction after stripping: 'Assessment: I have gone through your question in detail and I can understand wha'\nBlank prediction after stripping: 'Assessment: \\nPlan: What is your age? Have you tested for gamma GT before? Are th'\nBlank prediction after stripping: 'Assessment: I would be due to some other infections preferably of fungal origin.'\nBlank prediction after stripping: 'Assessment: \\nPlan: It should be investigated for malignancy as chance of develop'\nBlank prediction after stripping: 'Assessment: Anyway, I would recommend performing a nerve conduction study to exc'\nBlank prediction after stripping: 'Assessment: You may be having hematoma due to collection of blood in skin or sub'\nBlank prediction after stripping: 'Assessment: But exact other causes are diagnosed by physical examination or by M'\nBlank prediction after stripping: 'Assessment: Well your features are quite serious. Ask your Chat Doctor\\nPlan: Rem'\nBlank prediction after stripping: 'Assessment: HelloYour findings suggests two small non obstructing calculus in le'\nBlank prediction after stripping: 'Assessment: It will give you exact diagnosis\\nPlan: You should go for ultrasound '\nBlank prediction after stripping: 'Assessment: smoking and coffee both can increase pulse rate. So from today onwar'\nBlank prediction after stripping: 'Assessment: LYMPH NODES OCCURS DUE TO INFECTION. NOT DUE TO PIMPLES\\nPlan: DO NOT'\nBlank prediction after stripping: 'Assessment: It is hard, so it may be a viral like HSV and some time bacterial li'\nBlank prediction after stripping: 'Assessment: With a very low sperm count the only way to get pregnant is through '\nBlank prediction after stripping: 'Assessment: Itching around anus could be due to two reasons: 1\\nPlan: All the bes'\nBlank prediction after stripping: 'Assessment: I can understand your concern. By your history and description, poss'\nBlank prediction after stripping: 'Assessment: This will help you to get rid of the disease, and you get a diagnosi'\nBlank prediction after stripping: 'Assessment: I can understand your concern. You may have some kidney disorder\\nPla'\nBlank prediction after stripping: 'Assessment: Naturally that area become infected and in many instances the infect'\nBlank prediction after stripping: 'Assessment: Blood is usually sterile I\\nPlan: The treatment of infective endocard'\nBlank prediction after stripping: 'Assessment: When the doctor performed the laparoscopy, I would think that they w'\nBlank prediction after stripping: 'Assessment: High fever must be controlled irrespective of diagnosis with paracet'\nBlank prediction after stripping: 'Assessment: But I would alert you to rule out TB with Erythema Nodes which may b'\nBlank prediction after stripping: 'Assessment: Decision to start antibiotics is taken by proper examination and his'\nBlank prediction after stripping: 'Assessment: HI, thanks for using Chat Doctor It may be difficult to change the d'\nBlank prediction after stripping: 'Assessment: I read and understand your concern. I am Chat Doctor answering your '\nBlank prediction after stripping: 'Assessment: Thanks for the query to my Chat Doctor online-clinic. I studied your'\nBlank prediction after stripping: 'Assessment: The twitching over the facial muscles and Erasmus can be because of '\nBlank prediction after stripping: 'Assessment: Since, your child is just started on treatment of infantile spasms, '\nBlank prediction after stripping: 'Assessment: Thanks for your questioning my opinion you had tissue / muscle injur'\nBlank prediction after stripping: 'Assessment: I would suggest you to consult an Allergist and get evaluated and a '\nBlank prediction after stripping: 'Assessment: Firstly psoriasis is just a skin condition, and it is not a hindranc'\nBlank prediction after stripping: 'Assessment: Avoid stress and stop smoking and alcohol. Try keel exercise\\nPlan: L'\nBlank prediction after stripping: 'Assessment: Then take treatment according to your diagnosis\\nPlan: You should go '\nBlank prediction after stripping: 'Assessment: All other symptoms like nausea & vomiting, frequent urination and th'\nBlank prediction after stripping: 'Assessment: The fell out tooth structure could be only the crown portion\\nPlan: I'\nBlank prediction after stripping: 'Assessment: He prescribed it to cover other non-viral organisms. Cronin is fine '\nBlank prediction after stripping: 'Assessment: Mass in neck have many causes. And it can be due to either benign or'\nBlank prediction after stripping: 'Assessment: Herniated disc could be treated conservatively or surgically. If the'\nBlank prediction after stripping: 'Assessment: Hello, From the description, it appears like a fairly severe hypogly'\nBlank prediction after stripping: 'Assessment: Brief answer palpitation warrants cardiologists opinionDetailed answ'\nBlank prediction after stripping: 'Assessment: If reports are normal diagnosis is anxiety disorder\\nPlan: For DM YOU'\nBlank prediction after stripping: 'Assessment: By your history and symptoms I think that, your dizziness and black-'\nBlank prediction after stripping: 'Assessment: com I am Chat Doctor. Mariano Into Bruno Mascaras\\nPlan: The history '\nBlank prediction after stripping: 'Assessment: Hello, I would explain that a heart murmur could be related a struct'\nBlank prediction after stripping: 'Assessment: You do is do warm saline gargle two - three times a day Application '\nBlank prediction after stripping: 'Assessment: First, there is no medication that can be taken by the father that h'\nBlank prediction after stripping: 'Assessment: But you say that still you are having anemia which might be due to s'\nBlank prediction after stripping: 'Assessment: It is treated with antibiotic therapy. The infection can sometimes b'\nBlank prediction after stripping: 'Assessment: I have gone through your Query & I really appreciate your concern ab'\nBlank prediction after stripping: 'Assessment: Though you are having regular cycles you may be having ovulatory cyc'\nBlank prediction after stripping: 'Assessment: One small thing that may be overlooked many times is a worm infestat'\nBlank prediction after stripping: 'Assessment: Tiny boils may be folliculitis\\nPlan: Take oral treatment like tab fi'\nBlank prediction after stripping: 'Assessment: Persistent cough may be because of allergy or flue like condition\\nPl'\nBlank prediction after stripping: 'Assessment: As you mentioned that you are left-handed by nature to perform all a'\nBlank prediction after stripping: 'Assessment: As Metoprolol is started fresh, its blood level will rise day by day'\nBlank prediction after stripping: 'Assessment: Welcome to Chat Doctor. I have gone through your query and can under'\nBlank prediction after stripping: 'Assessment: It is necessary to perform a comprehensive differential diagnosis be'\nBlank prediction after stripping: 'Assessment: Thank you for writing to us at Chat Doctor This condition is known a'\nBlank prediction after stripping: 'Assessment: Cancer is a difficult condition to treat, but one should fight it ti'\nBlank prediction after stripping: 'Assessment: Due to which you are facing difficulty in getting pain while perform'\nBlank prediction after stripping: 'Assessment: I can understand your concern. Black colored stuff can be degenerate'\nBlank prediction after stripping: 'Assessment: I can understand your situation and problem. By your history and des'\nBlank prediction after stripping: 'Assessment: Other possible cause could be lipoma (fat cells) lesion\\nPlan: The hi'\nBlank prediction after stripping: 'Assessment: Stone (calculus) in the urinary system causes sever discomfort but w'\nBlank prediction after stripping: 'Assessment: But if it is due to bacterial infection(diagnosed after thorough cli'\nBlank prediction after stripping: 'Assessment: \\nPlan: The bleeding which you had on 2nd June, need to be known whet'\nBlank prediction after stripping: 'Assessment: It will tell you that you are pregnant or not. If positive consult a'\nBlank prediction after stripping: 'Assessment: I read your query. You have not mentioned the location of swollen gl'\nBlank prediction after stripping: \"Assessment: So these should be consumed in moderate amount, since we can't stop \"\nBlank prediction after stripping: 'Assessment: I have gone through your question. First let me give you basic physi'\nBlank prediction after stripping: 'Assessment: So if you are not taking chemotherapy, then in your condition you wi'\nBlank prediction after stripping: 'Assessment: thanks for trusting the Chat Doctor doctors for your health related '\nBlank prediction after stripping: 'Assessment: In my opinion you should not worry about relapse. Partial hip replac'\nBlank prediction after stripping: 'Assessment: After going through your query I think your mother is suffering from'\nBlank prediction after stripping: 'Assessment: Hi, if the filling was done without a proper cavity preparation, whi'\nBlank prediction after stripping: 'Assessment: I suggest you not to worry much. Anxiety is a disorder and not a dis'\nBlank prediction after stripping: 'Assessment: , the above report stating the measurement of the largest cyst is 37'\nBlank prediction after stripping: 'Assessment: Newborn brains are in the process of development especially in prete'\nBlank prediction after stripping: 'Assessment: I can understand your situation and problem. You are having too many'\nBlank prediction after stripping: 'Assessment: It could be a blood collection due to minor injury or a vein rupture'\nBlank prediction after stripping: 'Assessment: Hello, Tachycardia (Increased heart rate) can have many reasons, and'\nBlank prediction after stripping: 'Assessment: There might be having osteomalacia or osteoporosis as well. Go for x'\nBlank prediction after stripping: 'Assessment: The pain in the upper jaw and headache can be secondary to tooth inf'\nBlank prediction after stripping: 'Assessment: A one and a half year old baby, if he passes less than 4 motions/wee'\nBlank prediction after stripping: 'Assessment: From your side you should be remained calm and see his behavior as a'\nBlank prediction after stripping: 'Assessment: He is taking medicines that can cause mild sedation, but sleepiness '\nBlank prediction after stripping: 'Assessment: Hello and welcome to Chat Doctor, A history of purple mottled legs a'\nBlank prediction after stripping: 'Assessment: You have not provided your age on the details. I would have been abl'\nBlank prediction after stripping: 'Assessment: I will do my best to help yours around the anus could be due to reas'\nBlank prediction after stripping: \"Assessment: However, you need not worry about it as till now you haven't done a \"\nBlank prediction after stripping: 'Assessment: However, there are several causes that the tongue may swell and that'\nBlank prediction after stripping: 'Assessment: If you are bitten by a suspected rabid dog, according to WHO categor'\nBlank prediction after stripping: 'Assessment: As a Urologist, i understand your anxiety. CKD is possible, if tests'\nBlank prediction after stripping: 'Assessment: In my opinion the symptoms you describe may be related to anxiety\\nPl'\nBlank prediction after stripping: 'Assessment: By your history and description, possibility of asthma is more likel'\nBlank prediction after stripping: 'Assessment: Thanks for your query. I understand that you are quite distressed ab'\nBlank prediction after stripping: 'Assessment: 2-The cause of this cyst and the facts told in your query, exactly m'\nBlank prediction after stripping: 'Assessment: It is important to know your age. Cardiopulmonary and musculoskeleta'\n","output_type":"stream"}],"execution_count":58},{"cell_type":"code","source":"tsoutput = \"student_eval_output.csv\"\n\nprint(\"Starting test inference\")\ninfer_and_save(tsdata, tsoutput)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T04:27:02.975588Z","iopub.execute_input":"2025-11-03T04:27:02.975784Z","iopub.status.idle":"2025-11-03T04:36:07.328329Z","shell.execute_reply.started":"2025-11-03T04:27:02.975769Z","shell.execute_reply":"2025-11-03T04:36:07.327532Z"}},"outputs":[{"name":"stdout","text":"Starting test inference\nBlank prediction after stripping: 'Assessment: He might be having Pharyngitis/Tonsillitis?  You can do the followin'\nBlank prediction after stripping: 'Assessment: This means that it increases when there is any inflammation such as '\nBlank prediction after stripping: 'Assessment: Understanding your concern. As per your query you have psoriasis art'\nBlank prediction after stripping: 'Assessment: You can be advised treatments like Root Canal Treatment or Extractio'\nBlank prediction after stripping: 'Assessment: -Once the diagnosis has been made, there may be a need of surgery, a'\nBlank prediction after stripping: 'Assessment: Dear mam, pill has to be taken within 72 hours the earlier, the bett'\nBlank prediction after stripping: 'Assessment: By your history, in my opinion you are having dyspnea on minimal exe'\nBlank prediction after stripping: 'Assessment: Your description is telling that you have chronic allergy problem. Y'\nBlank prediction after stripping: \"Assessment: The chances of conceiving still exist with irregular periods. I don'\"\nBlank prediction after stripping: 'Assessment: Once oral infection is cleared, he will be able to eat better. Meanw'\nBlank prediction after stripping: 'Assessment: -CBC and peripheral smear examination, If target cells seen hemoglob'\nBlank prediction after stripping: 'Assessment: I shall prescribe some psychotherapy techniques which should help yo'\nBlank prediction after stripping: 'Assessment: MRI gives details of disc, nerves and even bones. So visit a radiolo'\nBlank prediction after stripping: 'Assessment: Wat u have mentioned was intro cerebral indicating with injury with '\nBlank prediction after stripping: 'Assessment: Antifungal medications ketoconazole and ciclopirox are both effectiv'\nBlank prediction after stripping: 'Assessment: I can suggest you to try to find some solution that can help in the '\nBlank prediction after stripping: 'Assessment: Pain traveling down your leg and increasing in intensity means advan'\nBlank prediction after stripping: 'Assessment: thanks for posting query. take medicine as advised advise:1\\nPlan: Vi'\nBlank prediction after stripping: 'Assessment: I am not sure whether they are in frontal lobe or deep white matter,'\nBlank prediction after stripping: 'Assessment: It seems you have got intertwine. Intertrigo is an infection which o'\nBlank prediction after stripping: 'Assessment: I would like to clarify at the outset itself that when a liver funct'\nBlank prediction after stripping: 'Assessment: Take some medicines like camphene for the growth of your follicles a'\nBlank prediction after stripping: 'Assessment: which may be the reason for your symptoms\\nPlan: Treatment depend upo'\nBlank prediction after stripping: 'Assessment: Hope this will help you. Wishing you good health\\nPlan: Wishing you g'\nBlank prediction after stripping: 'Assessment: I carefully read your question and I understand your concern. I will'\nBlank prediction after stripping: \"Assessment: volume range is 1\\nPlan: Motility also should be at least >60%, it's \"\nBlank prediction after stripping: 'Assessment: If it is not reducing in a week, consult an oral physician and take '\nBlank prediction after stripping: 'Assessment: Hi, It seems that probably you may be having candida intertwine. Con'\nBlank prediction after stripping: 'Assessment: Now, in many cases people are not able to change jobs or circumstanc'\nBlank prediction after stripping: 'Assessment: So consult an oral physician and get it evaluated and take a radiogr'\nBlank prediction after stripping: 'Assessment: As post angioplasty surgery there is a proper vascular flow and this'\nBlank prediction after stripping: 'Assessment: Also, it could be a common perineal nerve entrapment\\nPlan: Physical '\nBlank prediction after stripping: 'Assessment: Your food habits and your stressed life and work pattern has a role '\nBlank prediction after stripping: 'Assessment: Delayed positive test could be due to exceptional additional delayed'\nBlank prediction after stripping: 'Assessment: Thanks for your query. Understood your concern for your daughter\\nPla'\nBlank prediction after stripping: 'Assessment: Further treatment will depend upon result of these tests and final d'\nBlank prediction after stripping: 'Assessment: Sometimes it is necessary to take 2 or more medicines to control you'\nBlank prediction after stripping: 'Assessment: HelloThanks for your query, based on the facts that you have posted '\nBlank prediction after stripping: 'Assessment: there are multiple organisms causing this infection it may be viral '\nBlank prediction after stripping: 'Assessment: Thank you for your query. I am Chat Doctor, I understand your concer'\nBlank prediction after stripping: \"Assessment: As a Urologist, i can understand your anxiety. You've two issues:1\\nP\"\nBlank prediction after stripping: 'Assessment: Certain viral infections that cause sore throat may also cause plate'\nBlank prediction after stripping: 'Assessment: Most likely thoracic and lumbar region problems are contributing to '\nBlank prediction after stripping: 'Assessment: Wrong diagnosis\\nPlan: Improper treatment. Inadequate treatment3\\nRewr'\nBlank prediction after stripping: 'Assessment: In your semen analysis report the sperm motility is less. Normally i'\nBlank prediction after stripping: 'Assessment: Un rhythmic heart (Atrial Fibrillation) is usually treated with Amio'\nBlank prediction after stripping: 'Assessment: Hello! Welcome on Chat Doctor ! Your symptoms could be related to a '\nBlank prediction after stripping: 'Assessment: Thank you for writing to you seem to have halogen effluvium. Underly'\nBlank prediction after stripping: 'Assessment: The result will likely say that the organisms is resistant to the ce'\nBlank prediction after stripping: 'Assessment: These symptoms will subside in a few days as Chat Doctor. In case of'\nBlank prediction after stripping: 'Assessment: Hello! Welcome on Chat Doctor! I passed carefully through your conce'\nBlank prediction after stripping: 'Assessment: I can understand your concern. It needs examination first\\nPlan: Afte'\nBlank prediction after stripping: 'Assessment: As your child is having vomiting from long time, it is likely to be '\nBlank prediction after stripping: 'Assessment: If your PP sugar has suddenly risen to 174 it must be due to the foo'\nBlank prediction after stripping: 'Assessment: I can understand your concern. Stress, anxiety and panic attacks its'\nBlank prediction after stripping: \"Assessment: So likely it's all due to anxiety\\nPlan: So I think you should not wo\"\nBlank prediction after stripping: 'Assessment: From the history of your symptoms, it could be a gall stone disease,'\nBlank prediction after stripping: 'Assessment: I will advise you today to do x-ray lower back and hip joint after o'\nBlank prediction after stripping: 'Assessment: I have gone through your symptoms, and in my opinion there is no rea'\nBlank prediction after stripping: \"Assessment: Some time its too early to diagnose someone as somatoform. If let's \"\nBlank prediction after stripping: 'Assessment: Joint soreness can be related to rheumatoid arthritis. It should be '\nBlank prediction after stripping: 'Assessment: I can understand your concern. Treatment of pancreatic cancer depend'\nBlank prediction after stripping: 'Assessment: I can understand your concern. No need to worry about heart diseases'\nBlank prediction after stripping: \"Assessment: Next time onwards don't use Novocaine\\nPlan: If your dizziness caused\"\nBlank prediction after stripping: 'Assessment: In my opinion the emergency contraception can be taken in cases of s'\nBlank prediction after stripping: 'Assessment: Increased platelets level is good and goes against the diagnosis of '\nBlank prediction after stripping: 'Assessment: The itching is still persistent and leads to bump formation, on itch'\nBlank prediction after stripping: 'Assessment: Based on the facts of your query, You seem to suffer from-Lump on ri'\nBlank prediction after stripping: 'Assessment: Remedy and cause-of your problem-In my opinion your lumps under rib '\nBlank prediction after stripping: 'Assessment: These could be psychological\\nPlan: ly/ Chat Doctor\\nRewrite the above'\nBlank prediction after stripping: 'Assessment: MRI brain findings of your mother are common findings, and can occur'\nBlank prediction after stripping: 'Assessment: I can understand your concern. Your dad has esophageal cancer stage '\nBlank prediction after stripping: 'Assessment: I studied your query with available details given in your query. I u'\nBlank prediction after stripping: 'Assessment: In my opinion you should start oral bronchodilators. There are many '\nBlank prediction after stripping: 'Assessment: I can certainly understand your concern. I have worked through your '\nBlank prediction after stripping: 'Assessment: I am Chat Doctor. I will be answering your concerns\\nPlan: Since your'\nBlank prediction after stripping: 'Assessment: For that urine routine microscopic examination is advisable. Underly'\nBlank prediction after stripping: 'Assessment: Hi, I am Chat Doctor, I have read your query in detail, I understand'\nBlank prediction after stripping: 'Assessment: Only after these examinations the right strategy may be found out. Y'\nBlank prediction after stripping: 'Assessment: Symptomatic relief can be obtained with intake of Vestibular sedativ'\nBlank prediction after stripping: 'Assessment: In chickenpox, there are crops of pa pules and vesicles which result'\nBlank prediction after stripping: 'Assessment: Treatment varies from organ to organ. Site can be known by doing a b'\nBlank prediction after stripping: 'Assessment: Hello, If you have ruled out a fracture, you most likely put stress '\nBlank prediction after stripping: 'Assessment: I can understand your concern. You are mostly trying to say congesti'\nBlank prediction after stripping: 'Assessment: Rabies is 100 % fatal, but it is 100% preventable by proper and adeq'\nBlank prediction after stripping: 'Assessment: Hi! I have understood your query, the pain which is coming to your t'\nBlank prediction after stripping: 'Assessment: It is desirable to consider the both the nose & sinus got involved w'\nBlank prediction after stripping: 'Assessment: The redness that your have noticed on the labia major of your child '\nBlank prediction after stripping: 'Assessment: The common causes for ED at this age are (1) Diabetes, (2) Hypertens'\nBlank prediction after stripping: 'Assessment: Hello! Welcome on Chat Doctor! I passed carefully through your quest'\nBlank prediction after stripping: 'Assessment: Hence, considering your menstrual cycle to be of 28 days, it would b'\nBlank prediction after stripping: 'Assessment: Hello, Thanks for consulting Chat Doctor, Read your query, as you ha'\nBlank prediction after stripping: 'Assessment: I would recommend you to consult a gynecologist to properly evaluate'\nBlank prediction after stripping: 'Assessment: Fistula is very notorious in recurrence. If it recurred it is very d'\nBlank prediction after stripping: 'Assessment: Other conditions can be brain injury, brain tumor, weakness of facia'\nBlank prediction after stripping: 'Assessment: You need consultation of your physician for detailed evaluation as s'\nBlank prediction after stripping: 'Assessment: antidepressant\\nPlan: I advise you to continue circle as told by your'\nBlank prediction after stripping: 'Assessment: In my opinion giddiness alone might not suggest pregnancy. The pregn'\nBlank prediction after stripping: 'Assessment: I can understand your concern. By your history and description, poss'\nBlank prediction after stripping: 'Assessment: A low attenuation lesion again is most likely to be a cyst\\nPlan: Ple'\nBlank prediction after stripping: 'Assessment: I am not sure of the exact reason, but may be related to noise or hi'\nBlank prediction after stripping: 'Assessment: Needs to be gone for the tap water sponging vigorously when is requi'\nBlank prediction after stripping: 'Assessment: You can apply clotrimazole cream twice daily followed by application'\nBlank prediction after stripping: 'Assessment: So, Muscle relaxant with a painkiller will help you a lot. Regarding'\nBlank prediction after stripping: 'Assessment: However, usually spinal hernias can have neurological symptoms such '\nBlank prediction after stripping: 'Assessment: Few reasons could be an injury, infection or an inflammation leads t'\nBlank prediction after stripping: 'Assessment: First I want to make sure you have scabies. Sometimes patients get s'\nBlank prediction after stripping: 'Assessment: inability to fertilize an egg under physiological conditions\\nPlan: 5'\nBlank prediction after stripping: 'Assessment: This will help to get a diagnosis whether this is intestinal infecti'\nBlank prediction after stripping: 'Assessment: Yes, CT will definitely help the diagnosis. The treatment will depen'\nBlank prediction after stripping: 'Assessment: Does this heaviness occur on heavy activities and subsides with rest'\nBlank prediction after stripping: 'Assessment: CSI has a success rate of 35-40% in each cycle, if all the other med'\nBlank prediction after stripping: 'Assessment: Now he is in habit of support for passing motions. But he can overco'\nBlank prediction after stripping: 'Assessment: Dear miss, Positive blood culture is a serious finding. And first th'\nBlank prediction after stripping: 'Assessment: We call this machine IT [inter-ferential therapy]. An experienced Ph'\nBlank prediction after stripping: 'Assessment: Very true that the symptoms of abdominal pain, nausea and fatty stoo'\nBlank prediction after stripping: 'Assessment: Diagnosis needs to be confirmed by doing x-ray of the involved foot '\nBlank prediction after stripping: 'Assessment: However, it could be very interesting and informative doing an ultra'\nBlank prediction after stripping: 'Assessment: Many times we see that person with stroke will present with behavior'\nBlank prediction after stripping: 'Assessment: Hi, It seems that you may be having purity or lichen ultimatum. Ther'\nBlank prediction after stripping: 'Assessment: Understanding your concern. As per your query you have burning sensa'\nBlank prediction after stripping: 'Assessment: I can understand your concern. Food poisoning causes gastritis, vomi'\nBlank prediction after stripping: 'Assessment: During breathing or coughing, raised intrathoracic pressure also cau'\nBlank prediction after stripping: 'Assessment: I can understand your concern. Such kind of sports injuries can caus'\nBlank prediction after stripping: 'Assessment: Muscle aches may be due to no exercise or electrolytes imbalance\\nPla'\nBlank prediction after stripping: 'Assessment: They will help in the diagnosis and prognosis of the disease\\nPlan: W'\nBlank prediction after stripping: 'Assessment: I shall prescribe some psychotherapy techniques which should help yo'\nBlank prediction after stripping: 'Assessment: Hopefully you visited a physician to clarify your condition. After y'\nBlank prediction after stripping: 'Assessment: I am Chat Doctor answering your query. If I were your doctor, I woul'\nBlank prediction after stripping: 'Assessment: Also, such people have a number of vitamin deficiencies which may al'\nBlank prediction after stripping: 'Assessment: It contains Levetiracetam as a generic medicine. It falls into Pregn'\nBlank prediction after stripping: 'Assessment: There are tablets in allopatry for BPH that can shrink the prostate,'\nBlank prediction after stripping: 'Assessment: Organic cause seems less likely\\nPlan: Chat Doctor\\nRewrite the above '\nBlank prediction after stripping: 'Assessment: profit is nothing but erythropoietin which is released by kidney and'\nBlank prediction after stripping: 'Assessment: Hello, Dejavu can occur in mental disorders but as you said that she'\nBlank prediction after stripping: 'Assessment: Hormone test and Fallopian tube patency test for you and semen analy'\nBlank prediction after stripping: 'Assessment: Any way it may be tried,--\\nPlan: This treatment is being suggested o'\nBlank prediction after stripping: 'Assessment: In my opinion you are having skin allergy moat, probably. Better to '\nBlank prediction after stripping: 'Assessment: If those conservative therapies fail, then surgery may be of help\\nPl'\nBlank prediction after stripping: 'Assessment: It never causes blood in stool which can be due to pathology in GIT.'\nBlank prediction after stripping: 'Assessment: It will give you exact diagnosis\\nPlan: You should go for fine needle'\nBlank prediction after stripping: 'Assessment: Therefore, correct diagnosis is necessary. Consult a psychologist / '\nBlank prediction after stripping: 'Assessment: Due to the spinal nerve pathology, there are abnormal sensory change'\nBlank prediction after stripping: 'Assessment: Most probably he may be on anti-tubercular medicines (Rifampicin, Is'\nBlank prediction after stripping: \"Assessment: If condition doesn't get well then consult doctor for proper examina\"\nBlank prediction after stripping: 'Assessment: Your history indicates it may be a rib fracture\\nPlan: Subrahmanyam M'\nBlank prediction after stripping: 'Assessment: Possibility of heart burn related chest pain is more likely\\nPlan: If'\nBlank prediction after stripping: 'Assessment: I would suggest you following things:> Take acetaminophen thrice a d'\nBlank prediction after stripping: 'Assessment: Treatment mainly depends upon underlying conditions\\nPlan: Treatment '\nBlank prediction after stripping: 'Assessment: So nightfall is not the problem. It usually occurs in person who doe'\nBlank prediction after stripping: 'Assessment: It is not a serious condition, usually triple treatment is sufficien'\nBlank prediction after stripping: 'Assessment: I can understand your concern. You were having infected tooth and th'\nBlank prediction after stripping: 'Assessment: 5 months in conception is a very short time, and you would need to k'\nBlank prediction after stripping: 'Assessment: This would cause other muscles to get stiffed. When muscles of neck '\nBlank prediction after stripping: 'Assessment: Cerebral atrophy can not be reversed\\nPlan: Normal development as on '\nBlank prediction after stripping: 'Assessment: Your chest discomfort is associated with exertion (playing tennis), '\nBlank prediction after stripping: 'Assessment: In my opinion your husband is having malignant pleural effusion due '\nBlank prediction after stripping: 'Assessment: Its closer to being a night terror. Please understand that sucChatDo'\nBlank prediction after stripping: 'Assessment: Actually because of braces there is debris accumulation below the gu'\nBlank prediction after stripping: 'Assessment: It is possible that alcohol use can affect the liver particularly if'\nBlank prediction after stripping: 'Assessment: As you had cut of the skin on the ventral surface of the penis durin'\nBlank prediction after stripping: 'Assessment: Seizures are common in this condition\\nPlan: Best wishes, Chat Doctor'\nBlank prediction after stripping: 'Assessment: Here you might have persistent sinus infection\\nPlan: If needed endos'\nBlank prediction after stripping: 'Assessment: Semen Volume 2-10 ml normal (3 ml in your husband case)\\nPlan: Hope i'\nBlank prediction after stripping: 'Assessment: I am Chat Doctor, I will shortly try to help you with my opinion. Ye'\nBlank prediction after stripping: 'Assessment: Stolin gum paint helps improve your gum health. You did not mention '\nBlank prediction after stripping: 'Assessment: Thanks for writing in. You need to visit the surgeon who will advise'\nBlank prediction after stripping: 'Assessment: You also need to rule out a lung infection or an Please discuss thes'\nBlank prediction after stripping: 'Assessment: Cancer is the less common thing that could be\\nPlan: Wish you good he'\nBlank prediction after stripping: 'Assessment: headache after masturbation is possible when there is a stress angle'\nBlank prediction after stripping: 'Assessment: Allergic reaction takes some time to settle. In the meanwhile you ca'\nBlank prediction after stripping: 'Assessment: But unfortunately pancreatic cancer is not of this category. Stage 4'\nBlank prediction after stripping: 'Assessment: However, as you are very concerned, it will help you to get an ECG d'\nBlank prediction after stripping: 'Assessment: Most common muscle group which causes such pain are hamstrung group '\nBlank prediction after stripping: \"Assessment: Please don't do that. Get your child properly investigated and start\"\nBlank prediction after stripping: 'Assessment: You can take it tomorrow Hope I have answered your question. If you '\nBlank prediction after stripping: 'Assessment: In other words, one of the blood vessels has ruptured, and the blood'\nBlank prediction after stripping: 'Assessment: fever could be due to Viral, Bacterial or Parasitic in origin. It wi'\nBlank prediction after stripping: 'Assessment: Chest x-rayECGonce these conditions are ruled out, it may be because'\nBlank prediction after stripping: 'Assessment: I want to warn you to not manipulate the doses of your daughter. If '\nBlank prediction after stripping: 'Assessment: Involvement of Autonomic Pathways in Subclinical and Lab Evaluation '\nBlank prediction after stripping: 'Assessment: Hello, Thanks for consulting ChatDoctorRead your query as you have i'\nBlank prediction after stripping: 'Assessment: Based on your statement I would state that you have developed staphy'\nBlank prediction after stripping: 'Assessment: Develop strong will power and self-confidence. You require few couns'\nBlank prediction after stripping: 'Assessment: It might not be possible to reach at a provisional diagnosis without'\nBlank prediction after stripping: 'Assessment: Tetanus is a bacterial disease by clostridium retain. There is no cu'\nBlank prediction after stripping: 'Assessment: HelloThanks for writing to ChatDoctorYellow color of semen may be du'\nBlank prediction after stripping: 'Assessment: I would suggest you to consult an Endodontist and get evaluated and '\nBlank prediction after stripping: 'Assessment: Of these only HSV 2 transmits through sexual contact. Hsv 1 can get '\nBlank prediction after stripping: 'Assessment: If you have itching, oral anti histamines may be taken. PUPPY rash i'\nBlank prediction after stripping: 'Assessment: Hi, How are you doing ? POD (Polycystic ovaries), This is a congenit'\nBlank prediction after stripping: 'Assessment: Yes, fibroid can cause a fall in hemoglobin values and parameters re'\nBlank prediction after stripping: 'Assessment: I also suspect the same in your child. For the confirmation of the d'\nBlank prediction after stripping: 'Assessment: Hi, The raised eosinophil count appears to be related to an allergic'\nBlank prediction after stripping: 'Assessment: I can understand your concern. She has well differentiated keratiniz'\nBlank prediction after stripping: 'Assessment: It will give you exact diagnosis\\nPlan: You should go for rectal exam'\nBlank prediction after stripping: 'Assessment: Also, there may be a possibility of female pattern hair loss\\nPlan: S'\nBlank prediction after stripping: 'Assessment: Your symptoms can be due to bronchitis or asthma. The fact that you '\nBlank prediction after stripping: 'Assessment: Your lesion may be a reaction to the new shoes and that can cause su'\nBlank prediction after stripping: 'Assessment: From what you have described, I think his condition is deteriorating'\nBlank prediction after stripping: 'Assessment: Your husbands symptoms could be indicative of a mood disorder. Howev'\nBlank prediction after stripping: 'Assessment: Currently, from your symptoms I can judge that you are having depres'\nBlank prediction after stripping: 'Assessment: For this reason, I recommend consulting with his cardiologist and pe'\nBlank prediction after stripping: 'Assessment: For the hernia there are a number of symptoms which mainly is the ex'\nBlank prediction after stripping: 'Assessment: Any incision made in the muscle will lead to the stimulation of the '\nBlank prediction after stripping: 'Assessment: Confirm diagnosis can be done after seeing the lesion. But if u took'\nBlank prediction after stripping: 'Assessment: I would recommend you to get some rest. Take hot water fomentation i'\n","output_type":"stream"}],"execution_count":59},{"cell_type":"markdown","source":"## Metrics\n### Use predictions and references from saved files\n### A good practice seen in literature","metadata":{"_kg_hide-input":true}},{"cell_type":"code","source":"%%capture\n!pip install evaluate bert_score rouge_score\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T03:20:44.988406Z","iopub.execute_input":"2025-11-03T03:20:44.989206Z","iopub.status.idle":"2025-11-03T03:22:21.525745Z","shell.execute_reply.started":"2025-11-03T03:20:44.989181Z","shell.execute_reply":"2025-11-03T03:22:21.524990Z"},"_kg_hide-input":true},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":42},{"cell_type":"markdown","source":"## Eval and metrics","metadata":{"_kg_hide-input":true}},{"cell_type":"markdown","source":"### first check for empty predictions","metadata":{"_kg_hide-input":true}},{"cell_type":"code","source":"import pandas as pd\n\nresults = pd.read_csv(\"student_eval_output.csv\")\n\n# Identify NaN predictions  - there are indeed quite a few...\nnan_mask = results[\"prediction\"].isna()\nnum_nan = nan_mask.sum()\ntotal = len(results)\nprint(f\"NaN (missing) predictions: {num_nan} / {total} ({num_nan/total:.1%})\")\n\n# Mark, but keep in dataset for full analysis\nresults[\"is_nan\"] = nan_mask\n\n# Drop NaNs before metric calculation\nnon_nan_results = results[~nan_mask]\npreds = non_nan_results[\"prediction\"].tolist()\nrefs = non_nan_results[\"reference\"].tolist()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T04:36:07.329240Z","iopub.execute_input":"2025-11-03T04:36:07.329530Z","iopub.status.idle":"2025-11-03T04:36:07.347993Z","shell.execute_reply.started":"2025-11-03T04:36:07.329506Z","shell.execute_reply":"2025-11-03T04:36:07.347407Z"},"_kg_hide-input":true},"outputs":[{"name":"stdout","text":"NaN (missing) predictions: 207 / 472 (43.9%)\n","output_type":"stream"}],"execution_count":60},{"cell_type":"markdown","source":"### Do simple metrics","metadata":{"_kg_hide-input":true}},{"cell_type":"code","source":"from evaluate import load\nrouge = load(\"rouge\")\nbertscore = load(\"bertscore\")\n\nif len(preds) > 0:\n    rouge_scores = rouge.compute(predictions=preds, references=refs)\n    bertscore_out = bertscore.compute(predictions=preds, references=refs, lang=\"en\")\n    print(\"ROUGE-L:\", rouge_scores[\"rougeL\"])\n    print(\"Mean BERTScore F1:\", sum(bertscore_out['f1']) / len(bertscore_out['f1']))\nelse:\n    print(\"No non-blank predictions to score.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T04:36:07.349297Z","iopub.execute_input":"2025-11-03T04:36:07.349494Z","iopub.status.idle":"2025-11-03T04:36:26.458200Z","shell.execute_reply.started":"2025-11-03T04:36:07.349480Z","shell.execute_reply":"2025-11-03T04:36:26.457418Z"},"_kg_hide-input":true},"outputs":[{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"ROUGE-L: 0.23268499823369582\nMean BERTScore F1: 0.8752007360728282\n","output_type":"stream"}],"execution_count":61},{"cell_type":"code","source":"import numpy as np\nexact_match_rate = np.mean([p.strip() == r.strip() for p, r in zip(preds, refs)])\nprint(f\"Exact Match %: {100 * exact_match_rate:.2f}%\")\n","metadata":{"trusted":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2025-11-03T03:24:56.627898Z","iopub.execute_input":"2025-11-03T03:24:56.628447Z","iopub.status.idle":"2025-11-03T03:24:56.633122Z","shell.execute_reply.started":"2025-11-03T03:24:56.628424Z","shell.execute_reply":"2025-11-03T03:24:56.632356Z"}},"outputs":[{"name":"stdout","text":"Exact Match %: 0.00%\n","output_type":"stream"}],"execution_count":48},{"cell_type":"code","source":"#optional - medical term retention check\n# needs domain specific list of terms\n# sample is below...\n\nmedical_terms = {\"creatinine\", \"dialysis\", \"kidney\", \"diabetes\", \"nephrologyst\", \"GFR\",\n                 \"urea\", \"uric acid\", \"anion gap\", \"electrolytes\", \"renal biopsy\"}\n\ndef retain_count(pred, ref, terms):\n    ref_terms = [term for term in terms if term in ref.lower()]\n    pred_terms = [term for term in ref_terms if term in pred.lower()]\n    return len(pred_terms), len(ref_terms)\n\nretained, total = 0, 0\nfor p, r in zip(preds, refs):\n    rc, tc = retain_count(p, r, medical_terms)\n    retained += rc\n    total += tc\nretain_rate = retained / total if total > 0 else 1.0\nprint(f\"Medical term retain rate: {100 * retain_rate:.2f}%\")\n","metadata":{"trusted":true,"_kg_hide-input":true},"outputs":[],"execution_count":62},{"cell_type":"markdown","source":"### Interpretation of the above can be mixed\n### We trained the model to give plain output\n### It won't be surprising if some medical terms are simplified","metadata":{}},{"cell_type":"markdown","source":"# Full Metric Comparison\n\n## Done for train and test data","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom evaluate import load\n\n# needs domain specific list of terms\n# sample is below...\nmedical_terms = {\"creatinine\", \"dialysis\", \"kidney\", \"diabetes\", \"nephrologyst\", \"GFR\",\n                 \"urea\", \"uric acid\", \"anion gap\", \"electrolytes\", \"renal biopsy\"}\n\ndef load_and_prepare(fname):\n    df = pd.read_csv(fname)\n    preds = df[\"prediction\"].fillna(\"\").astype(str).tolist()\n    refs = df[\"reference\"].fillna(\"\").astype(str).tolist()\n    nan_mask = (df[\"prediction\"].isna() | (df[\"prediction\"].str.strip() == \"\"))\n    return preds, refs, nan_mask\n\ndef compute_metrics(preds, refs, nan_mask, terms):\n    metrics = {}\n    # filter for empty predictions\n    preds_nonan = [p for i,p in enumerate(preds) if not nan_mask.iloc[i]]\n    refs_nonan = [r for i,r in enumerate(refs) if not nan_mask.iloc[i]]\n    count = len(preds_nonan)\n    metrics[\"Total\"] = len(preds)\n    metrics[\"Valid\"] = count\n    metrics[\"NaN_rate\"] = 1 - count/len(preds) if len(preds) else 0\n    # calculate metrics ignoring empty\n    if count > 0:\n        rouge = load(\"rouge\")\n        rouge_scores = rouge.compute(predictions=preds_nonan, references=refs_nonan)\n        metrics[\"ROUGE-L\"] = rouge_scores[\"rougeL\"]\n        bertscore = load(\"bertscore\")\n        bs_scores = bertscore.compute(predictions=preds_nonan, references=refs_nonan, lang=\"en\")\n        metrics[\"BERTScore_F1\"] = float(np.mean(bs_scores['f1']))\n        metrics[\"Exact_Match\"] = float(np.mean([p.strip()==r.strip() for p,r in zip(preds_nonan,refs_nonan)]))\n        # medical terms retention\n        def retain_count(pred, ref, terms):\n            ref_terms = [term for term in terms if term in ref.lower()]\n            pred_terms = [term for term in ref_terms if term in pred.lower()]\n            return len(pred_terms), len(ref_terms)\n        retained, total = 0, 0\n        for p, r in zip(preds_nonan, refs_nonan):\n            rc, tc = retain_count(p, r, terms)\n            retained += rc\n            total += tc\n        metrics[\"Medical_Retain\"] = retained/total if total>0 else 1.0\n        # Hallucination detection   - code leveraged; metric to be interpreted\n        def find_hallucinated(pred, ref, terms):\n            pred_terms = {term for term in terms if term in pred.lower()}\n            ref_terms = {term for term in terms if term in ref.lower()}\n            return len(pred_terms - ref_terms)\n        halluc_count = sum(find_hallucinated(p,r,terms)>0 for p,r in zip(preds_nonan,refs_nonan))\n        metrics[\"Hallucination_rate\"] = halluc_count/count if count>0 else 0\n    else:\n        metrics.update({\"ROUGE-L\":0,\"BERTScore_F1\":0,\"Exact_Match\":0,\"Medical_Retain\":0,\"Hallucination_rate\":0})\n    return metrics\n\n# Load train set\ntrain_preds, train_refs, train_nan = load_and_prepare(\"/kaggle/working/student_train_output.csv\")\ntrain_metrics = compute_metrics(train_preds, train_refs, train_nan, medical_terms)\n\n# Load test set\ntest_preds, test_refs, test_nan = load_and_prepare(\"/kaggle/working/student_eval_output.csv\")\ntest_metrics = compute_metrics(test_preds, test_refs, test_nan, medical_terms)\n\n# Pretty print as a table\nimport pandas as pd\nreport = pd.DataFrame([train_metrics, test_metrics], index=[\"Train\",\"Test\"]).T\nprint(report)\n","metadata":{"trusted":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2025-11-03T04:36:26.463674Z","iopub.execute_input":"2025-11-03T04:36:26.463890Z","iopub.status.idle":"2025-11-03T04:37:05.082867Z","shell.execute_reply.started":"2025-11-03T04:36:26.463867Z","shell.execute_reply":"2025-11-03T04:37:05.082239Z"}},"outputs":[{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"                         Train        Test\nTotal               471.000000  472.000000\nValid               278.000000  265.000000\nNaN_rate              0.409766    0.438559\nROUGE-L               0.235115    0.232685\nBERTScore_F1          0.875770    0.875201\nExact_Match           0.000000    0.000000\nMedical_Retain        0.320000    0.642857\nHallucination_rate    0.014388    0.011321\n","output_type":"stream"}],"execution_count":63},{"cell_type":"markdown","source":"#### perplexity's comments on the metrics!","metadata":{"_kg_hide-input":true}},{"cell_type":"markdown","source":"Interpreting  Model's Evaluation Results\nLet's walk through how you might interpret these results, especially for an academic healthcare NLP project:\n\nROUGE-L: 0.266\nModerate recall for medical summarization. For abstractive summaries of clinical notes, a ROUGE-L score of about 0.27 suggests your model is picking up relevant sequences but isn't perfectly aligned with reference summaries.\n\nTypical range for strong models in medical summarization can be 0.3‚Äì0.4+, though this depends on dataset complexity and how much reference and prediction texts diverge. This means your model performs reasonably but could benefit from further tuning.\n\nMean BERTScore F1: 0.89\nHigh semantic similarity! This score is strong‚ÄîBERTScore captures meaning rather than just word overlap, and scores above 0.85 typically suggest outputs remain faithful in meaning to the references, even if phrasing differs.\n\nEven with a moderate ROUGE-L, a high BERTScore suggests the model paraphrases well, retaining important clinical concepts and descriptions.\n\n56% NaN (Missing Predictions)\nThis is concerning. More than half your samples resulted in missing predictions, which is not typical. In practice, you'd want this to be under 5‚Äì10%.\n\nCommon causes:\n\nModel or tokenization errors on certain prompts.\n\nVery long inputs or aggressive truncation.\n\nModel or hardware resource limits (memory, timeouts).\n\nBugs in inference (e.g., some prompts producing no output, or mishandling the results after generation).\n\nNext Action: Investigate why predictions are missing. Check logs, input formatting, and try generating a small batch of the problematic prompts directly to debug.\n\nIf you fix this, you'll likely see your ROUGE-L and BERTScore change‚Äîin particular, removing missing outputs will give you metrics that better reflect overall quality.\n\nQuick Summary\nBERTScore F1 (0.89): Semantic meaning is well preserved where the model produces output.\n\nROUGE-L (0.27): Linguistic overlap is moderate‚Äîparaphrasing or missing references may account for the gap.\n\nNaN rate (56%): Suggests the evaluation is incomplete; addressing this will be your next critical step.\n","metadata":{"_kg_hide-input":true}},{"cell_type":"markdown","source":"## Reference code - for later usage","metadata":{"_kg_hide-input":true}},{"cell_type":"code","source":"# One set of evaluation...\n\n\n# ============================================================================\n# COMPREHENSIVE EVALUATION FRAMEWORK FOR TEACHER-STUDENT MEDICAL NLP MODEL\n# ============================================================================\n# This framework evaluates a TinyLlama student model fine-tuned with LoRA\n# for converting clinical SOAP notes to patient-friendly summaries\n\nimport torch\nimport numpy as np\nfrom typing import Dict, List, Tuple\nfrom dataclasses import dataclass\nimport json\nfrom datetime import datetime\n\n# Required libraries\n# pip install rouge-score\n# pip install bert-score\n# pip install evaluate\n# pip install transformers\n# pip install torch\n\nfrom rouge_score import rouge_scorer\nfrom bert_score import score as bert_score\nimport evaluate\n\n# ============================================================================\n# 1. AUTOMATED METRICS CLASS\n# ============================================================================\n\n@dataclass\nclass AutomatedMetrics:\n    \"\"\"Compute automated evaluation metrics for text generation tasks\"\"\"\n\n    def __init__(self, use_stemmer: bool = True):\n        self.rouge_scorer = rouge_scorer.RougeScorer(\n            ['rouge1', 'rouge2', 'rougeL'], \n            use_stemmer=use_stemmer\n        )\n        self.perplexity_metric = evaluate.load(\"perplexity\", module_type=\"metric\")\n        self.bleu_metric = evaluate.load(\"bleu\")\n\n    def compute_rouge(self, generated: str, reference: str) -> Dict[str, float]:\n        \"\"\"\n        Compute ROUGE scores (F1-score focused)\n        Best for: Medical note summarization tasks\n\n        ROUGE-1: Unigram overlap (individual words)\n        ROUGE-2: Bigram overlap (word pairs)  \n        ROUGE-L: Longest common subsequence (semantic coherence)\n        \"\"\"\n        scores = self.rouge_scorer.score(reference, generated)\n\n        return {\n            'rouge1_f1': scores['rouge1'].fmeasure,\n            'rouge2_f1': scores['rouge2'].fmeasure,\n            'rougeL_f1': scores['rougeL'].fmeasure,\n       ","metadata":{"trusted":true,"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from rouge_score import rouge_scorer\nfrom sacrebleu import corpus_bleu\nfrom bert_score import score as bert_score_fn\nimport textstat\n\nrefs = [row[\"output\"] for row in train_data]\npreds = small_df[\"student_output\"].tolist()\n\nrouge = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\nr1, rL = [], []\nfor pred, ref in zip(preds, refs):\n    scores = rouge.score(str(pred), str(ref))\n    r1.append(scores[\"rouge1\"].fmeasure)\n    rL.append(scores[\"rougeL\"].fmeasure)\nprint(\"ROUGE-1 avg:\", sum(r1)/len(r1))\nprint(\"ROUGE-L avg:\", sum(rL)/len(rL))\nbleu = corpus_bleu(preds, [refs])\nprint(\"BLEU:\", bleu.score)\nP, R, F1 = bert_score_fn(preds, refs, lang=\"en\")\nprint(\"BERTScore F1 avg:\", F1.mean().item())\nread_ease = [textstat.flesch_reading_ease(txt) for txt in preds]\nfk_grade = [textstat.flesch_kincaid_grade(txt) for txt in preds]\nprint(\"Avg Flesch Ease:\", sum(read_ease)/len(read_ease))\nprint(\"Avg FK Grade:\", sum(fk_grade)/len(fk_grade))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T09:34:13.964665Z","iopub.status.idle":"2025-10-31T09:34:13.965010Z","shell.execute_reply.started":"2025-10-31T09:34:13.964852Z","shell.execute_reply":"2025-10-31T09:34:13.964866Z"},"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import bitsandbytes as bnb\nprint(bnb.__version__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T09:34:13.974389Z","iopub.status.idle":"2025-10-31T09:34:13.974709Z","shell.execute_reply.started":"2025-10-31T09:34:13.974546Z","shell.execute_reply":"2025-10-31T09:34:13.974559Z"},"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true,"_kg_hide-input":true},"outputs":[],"execution_count":null}]}