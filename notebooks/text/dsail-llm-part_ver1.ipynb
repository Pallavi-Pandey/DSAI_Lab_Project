{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":3696968,"sourceType":"datasetVersion","datasetId":2211956},{"sourceId":4805127,"sourceType":"datasetVersion","datasetId":2782228},{"sourceId":7045374,"sourceType":"datasetVersion","datasetId":4054084},{"sourceId":7045630,"sourceType":"datasetVersion","datasetId":4054274},{"sourceId":7057378,"sourceType":"datasetVersion","datasetId":4062409},{"sourceId":7562666,"sourceType":"datasetVersion","datasetId":4403558},{"sourceId":8870083,"sourceType":"datasetVersion","datasetId":5338273},{"sourceId":10301988,"sourceType":"datasetVersion","datasetId":6376623},{"sourceId":11557167,"sourceType":"datasetVersion","datasetId":7246557},{"sourceId":13555912,"sourceType":"datasetVersion","datasetId":8610195}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Leveraged AutoSOAP code","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport json\nfrom pathlib import Path\nimport numpy as np\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# First, let's explore the actual Kaggle input directory structure\ndef explore_kaggle_input():\n    \"\"\"Explore what's actually available in /kaggle/input/\"\"\"\n    input_dir = Path('/kaggle/input')\n    \n    if not input_dir.exists():\n        print(\"‚ùå /kaggle/input directory not found\")\n        return []\n    \n    print(\"üìÅ Available datasets in /kaggle/input/:\")\n    available_datasets = []\n    \n    for item in input_dir.iterdir():\n        if item.is_dir():\n            print(f\"  üìÇ {item.name}\")\n            available_datasets.append(str(item))\n            \n            # Show files in each dataset directory\n            try:\n                files = list(item.rglob('*'))\n                files = [f for f in files if f.is_file()]\n                print(f\"     Files: {len(files)} total\")\n                \n                # Show first few files\n                for i, file in enumerate(files[:3]):\n                    print(f\"     - {file.name}\")\n                if len(files) > 3:\n                    print(f\"     ... and {len(files)-3} more files\")\n                    \n            except Exception as e:\n                print(f\"     Error reading directory: {e}\")\n                \n    return available_datasets\n\n# Explore available datasets\navailable_datasets = explore_kaggle_input()\nprint(f\"\\n‚úÖ Found {len(available_datasets)} dataset directories\")\n\n\nimport pandas as pd\nimport json\nfrom pathlib import Path\nimport numpy as np\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Create project structure\nproject_dirs = [\n    'AutoSOAP/data/raw',\n    'AutoSOAP/data/processed', \n    'AutoSOAP/notebooks',\n    'AutoSOAP/scripts',\n    'AutoSOAP/outputs'\n]\n\nfor dir_path in project_dirs:\n    Path(dir_path).mkdir(parents=True, exist_ok=True)\n    \nprint(\"‚úÖ Project structure created!\")\n\n# Dataset exploration function\ndef safe_load_dataset(dataset_name, base_path):\n    \"\"\"Safely load and explore datasets with error handling\"\"\"\n    print(f\"\\n{'='*60}\")\n    print(f\"üìä EXPLORING: {dataset_name}\")\n    print(f\"{'='*60}\")\n    \n    try:\n        # List all files in the directory\n        files = list(base_path.glob('*'))\n        print(f\"üìÅ Files found: {len(files)}\")\n        \n        for i, file in enumerate(files[:5]):  # Show first 5 files\n            print(f\"  {i+1}. {file.name} ({file.stat().st_size} bytes)\")\n        \n        if len(files) > 5:\n            print(f\"  ... and {len(files)-5} more files\")\n            \n        return files\n        \n    except Exception as e:\n        print(f\"‚ùå Error exploring {dataset_name}: {str(e)}\")\n        return []\n\n# Start with the datasets we know have files\ndatasets_info = {\n    'mental-health-corpus': '/kaggle/input/mental-health-corpus',\n    'nlp-mental-health-conversations': '/kaggle/input/nlp-mental-health-conversations', \n    'medical-conversation-corpus-100k': '/kaggle/input/medical-conversation-corpus-100k',\n    'human-and-llm-mental-health-conversations': '/kaggle/input/human-and-llm-mental-health-conversations',\n    'healthcare-appointment-booking-calls-dataset': '/kaggle/input/healthcare-appointment-booking-calls-dataset',\n    'chatdoctor': '/kaggle/input/chatdoctor',\n    'sentiment-analysis-for-mental-health': '/kaggle/input/sentiment-analysis-for-mental-health',\n    'comprehensive-medical-q-a-dataset': '/kaggle/input/comprehensive-medical-q-a-dataset'\n}\n\n# Explore each dataset\ndataset_files = {}\nfor name, path in datasets_info.items():\n    base_path = Path(path)\n    files = safe_load_dataset(name, base_path)\n    dataset_files[name] = files\n\nprint(f\"\\nüéØ SUMMARY: Found files in {len([k for k, v in dataset_files.items() if v])} datasets\")\n\n\nimport os\nimport pandas as pd\nimport json\nfrom pathlib import Path\nimport glob\n\ndef find_actual_files():\n    \"\"\"Find all actual files in the Kaggle input directory\"\"\"\n    base_path = '/kaggle/input'\n    all_files = {}\n    \n    # Use glob to find all files recursively\n    for dataset_dir in os.listdir(base_path):\n        dataset_path = os.path.join(base_path, dataset_dir)\n        if os.path.isdir(dataset_path):\n            files = []\n            try:\n                # Find all files recursively\n                for root, dirs, filenames in os.walk(dataset_path):\n                    for filename in filenames:\n                        full_path = os.path.join(root, filename)\n                        if os.path.isfile(full_path) and os.path.getsize(full_path) > 0:\n                            files.append(full_path)\n                all_files[dataset_dir] = files\n            except Exception as e:\n                print(f\"Error accessing {dataset_dir}: {e}\")\n                all_files[dataset_dir] = []\n    \n    return all_files\n\n# Find all actual files\nactual_files = find_actual_files()\n\nprint(\"üìÅ ACTUAL FILES FOUND:\")\nfor dataset, files in actual_files.items():\n    print(f\"\\n{dataset}:\")\n    for file in files:\n        try:\n            size = os.path.getsize(file)\n            print(f\"  ‚úÖ {os.path.basename(file)} ({size:,} bytes)\")\n        except:\n            print(f\"  ‚ùå {os.path.basename(file)} (access error)\")\n\n\nimport pandas as pd\nimport json\nfrom pathlib import Path\nimport numpy as np\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Dataset loading and examination function\ndef load_and_examine_dataset(dataset_name, file_path, file_type='csv'):\n    \"\"\"Load and examine individual datasets\"\"\"\n    print(f\"\\n{'='*70}\")\n    print(f\"üìä EXAMINING: {dataset_name}\")\n    print(f\"üìÅ File: {Path(file_path).name}\")\n    print(f\"{'='*70}\")\n    \n    try:\n        if file_type == 'csv':\n            # Load CSV files\n            df = pd.read_csv(file_path)\n            print(f\"üìà Shape: {df.shape}\")\n            print(f\"üìã Columns: {list(df.columns)}\")\n            print(f\"üíæ Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n            \n            # Show sample data\n            print(f\"\\nüîç First 3 rows:\")\n            print(df.head(3).to_string())\n            \n            # Show data types\n            print(f\"\\nüìä Data types:\")\n            print(df.dtypes.to_string())\n            \n            # Check for missing values\n            missing = df.isnull().sum()\n            if missing.sum() > 0:\n                print(f\"\\n‚ö†Ô∏è Missing values:\")\n                print(missing[missing > 0].to_string())\n            \n            return df\n            \n        elif file_type == 'json':\n            # Load JSON files\n            with open(file_path, 'r', encoding='utf-8') as f:\n                data = json.load(f)\n            \n            print(f\"üìà Type: {type(data)}\")\n            if isinstance(data, list):\n                print(f\"üìà Length: {len(data)}\")\n                if len(data) > 0:\n                    print(f\"üìã Sample keys: {list(data[0].keys()) if isinstance(data[0], dict) else 'Not dict'}\")\n                    print(f\"\\nüîç First 2 entries:\")\n                    for i, item in enumerate(data[:2]):\n                        print(f\"Entry {i+1}: {str(item)[:300]}...\")\n            elif isinstance(data, dict):\n                print(f\"üìã Top-level keys: {list(data.keys())}\")\n                print(f\"\\nüîç Sample content:\")\n                for key, value in list(data.items())[:3]:\n                    print(f\"{key}: {str(value)[:200]}...\")\n                    \n            return data\n            \n    except Exception as e:\n        print(f\"‚ùå Error loading {dataset_name}: {str(e)}\")\n        return None\n\n# Start examining datasets systematically\ndatasets_info = {}\n\n# 1. Mental Health Corpus\nprint(\"üöÄ Starting dataset examination...\")\ndatasets_info['mental-health-corpus'] = load_and_examine_dataset(\n    'mental-health-corpus',\n    '/kaggle/input/mental-health-corpus/mental_health.csv',\n    'csv'\n)\n\n\n# 2. Medical Conversation Corpus 100K (Most promising for SOAP notes)\ndatasets_info['medical-conversation-100k'] = load_and_examine_dataset(\n    'medical-conversation-100k',\n    '/kaggle/input/medical-conversation-corpus-100k/train.csv',\n    'csv'\n)\n\n\n# 3. ChatDoctor JSON datasets (Large medical Q&A)\ndatasets_info['chatdoctor-healthcaremagic'] = load_and_examine_dataset(\n    'chatdoctor-healthcaremagic',\n    '/kaggle/input/chatdoctor/HealthCareMagic-100k.json',\n    'json'\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 4. Comprehensive Medical Q&A Dataset\ndatasets_info['comprehensive-medical-qa'] = load_and_examine_dataset(\n    'comprehensive-medical-qa',\n    '/kaggle/input/comprehensive-medical-q-a-dataset/train.csv',\n    'csv'\n)\n\n\n# 5. NLP Mental Health Conversations (might have dialogue structure)\ndatasets_info['nlp-mental-health'] = load_and_examine_dataset(\n    'nlp-mental-health',\n    '/kaggle/input/nlp-mental-health-conversations/train.csv',\n    'csv'\n)\n\n\nimport re\nimport pandas as pd\nimport json\nfrom typing import Dict, List, Tuple\nimport numpy as np\n\nclass AutoSOAPDataProcessor:\n    \"\"\"Data processor for converting medical dialogues to SOAP-ready format\"\"\"\n    \n    def __init__(self):\n        self.processed_data = {}\n        \n    def parse_medical_conversation(self, conversation_text: str) -> Dict:\n        \"\"\"Parse the medical conversation 100k format\"\"\"\n        try:\n            # Split by Human and AI markers\n            parts = re.split(r'\\[?\\|?(Human|AI)\\|?\\]?', conversation_text)\n            \n            dialogue = []\n            current_speaker = None\n            \n            for i, part in enumerate(parts):\n                part = part.strip()\n                if part in ['Human', 'AI']:\n                    current_speaker = 'Patient' if part == 'Human' else 'Doctor'\n                elif part and current_speaker:\n                    dialogue.append({\n                        'speaker': current_speaker,\n                        'text': part.strip()\n                    })\n                    \n            return {\n                'dialogue': dialogue,\n                'patient_input': dialogue[0]['text'] if dialogue and dialogue[0]['speaker'] == 'Patient' else '',\n                'doctor_response': dialogue[1]['text'] if len(dialogue) > 1 and dialogue[1]['speaker'] == 'Doctor' else ''\n            }\n        except Exception as e:\n            return {'dialogue': [], 'patient_input': '', 'doctor_response': '', 'error': str(e)}\n    \n    def parse_chatdoctor_format(self, entry: Dict) -> Dict:\n        \"\"\"Parse ChatDoctor JSON format\"\"\"\n        try:\n            return {\n                'patient_input': entry.get('input', ''),\n                'doctor_response': entry.get('output', ''),\n                'instruction': entry.get('instruction', ''),\n                'dialogue': [\n                    {'speaker': 'Patient', 'text': entry.get('input', '')},\n                    {'speaker': 'Doctor', 'text': entry.get('output', '')}\n                ]\n            }\n        except Exception as e:\n            return {'patient_input': '', 'doctor_response': '', 'error': str(e)}\n    \n    def process_medical_conversations_100k(self, df: pd.DataFrame, sample_size: int = 1000) -> List[Dict]:\n        \"\"\"Process the medical conversations 100k dataset\"\"\"\n        print(f\"üîÑ Processing Medical Conversations 100K (sample: {sample_size})...\")\n        \n        processed = []\n        sample_df = df.sample(n=min(sample_size, len(df)), random_state=42)\n        \n        for idx, row in sample_df.iterrows():\n            parsed = self.parse_medical_conversation(row['Conversation'])\n            if parsed['patient_input'] and parsed['doctor_response']:\n                parsed['source'] = 'medical-conv-100k'\n                parsed['index'] = idx\n                processed.append(parsed)\n        \n        print(f\"‚úÖ Successfully processed {len(processed)} conversations\")\n        return processed\n    \n    def process_chatdoctor_data(self, data: List[Dict], sample_size: int = 1000) -> List[Dict]:\n        \"\"\"Process ChatDoctor dataset\"\"\"\n        print(f\"üîÑ Processing ChatDoctor data (sample: {sample_size})...\")\n        \n        processed = []\n        sample_data = data[:sample_size] if len(data) > sample_size else data\n        \n        for idx, entry in enumerate(sample_data):\n            parsed = self.parse_chatdoctor_format(entry)\n            if parsed['patient_input'] and parsed['doctor_response']:\n                parsed['source'] = 'chatdoctor'\n                parsed['index'] = idx\n                processed.append(parsed)\n        \n        print(f\"‚úÖ Successfully processed {len(processed)} conversations\")\n        return processed\n\n# Initialize processor\nprocessor = AutoSOAPDataProcessor()\n\n# Process Medical Conversations 100K (sample for testing)\nprint(\"üöÄ Starting data processing pipeline...\")\nmedical_conv_processed = processor.process_medical_conversations_100k(\n    datasets_info['medical-conversation-100k'], \n    sample_size=500  # Start with smaller sample for testing\n)\n\n# Process ChatDoctor data (sample for testing)\nchatdoctor_processed = processor.process_chatdoctor_data(\n    datasets_info['chatdoctor-healthcaremagic'], \n    sample_size=500\n)\n\nprint(f\"\\nüìä PROCESSING SUMMARY:\")\nprint(f\"Medical Conversations: {len(medical_conv_processed)} processed\")\nprint(f\"ChatDoctor: {len(chatdoctor_processed)} processed\")\nprint(f\"Total: {len(medical_conv_processed) + len(chatdoctor_processed)} conversations ready for SOAP generation\")\n\n# Show sample processed data\nprint(f\"\\nüîç SAMPLE PROCESSED CONVERSATION:\")\nif medical_conv_processed:\n    sample = medical_conv_processed[0]\n    print(f\"Source: {sample['source']}\")\n    print(f\"Patient: {sample['patient_input'][:200]}...\")\n    print(f\"Doctor: {sample['doctor_response'][:200]}...\")\n\n\n# Let's first examine what's in the medical conversation data to fix the parsing\n# Re-load the medical conversation dataset and examine the format\nimport pandas as pd\n\n# Load medical conversations dataset\nmedical_conv_df = pd.read_csv('/kaggle/input/medical-conversation-corpus-100k/train.csv')\n\n# Show the exact format of the first conversation\nprint(\"üîç DEBUGGING: Medical Conversation Format\")\nprint(\"=\"*60)\nfirst_conv = medical_conv_df.iloc[0]['Conversation']\nprint(\"Raw conversation text:\")\nprint(repr(first_conv[:500]))  # Show raw text with escape characters\nprint(\"\\n\" + \"=\"*60)\n\n# Let's also examine a few more samples to understand the pattern\nprint(\"\\nüìä SAMPLE CONVERSATIONS (first 200 chars each):\")\nfor i in range(3):\n    conv = medical_conv_df.iloc[i]['Conversation']\n    print(f\"\\nConversation {i+1}:\")\n    print(f\"Length: {len(conv)} characters\")\n    print(f\"Content: {conv[:200]}...\")\n    \n    # Check for different possible markers\n    markers_found = []\n    if '[|Human|]' in conv: markers_found.append('[|Human|]')\n    if '[|AI|]' in conv: markers_found.append('[|AI|]')\n    if '|Human|' in conv: markers_found.append('|Human|')\n    if '|AI|' in conv: markers_found.append('|AI|')\n    if 'Human:' in conv: markers_found.append('Human:')\n    if 'AI:' in conv: markers_found.append('AI:')\n    \n    print(f\"Markers found: {markers_found}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Fixed parsing function for Medical Conversations\ndef parse_medical_conversation_fixed(conversation_text: str) -> Dict:\n    \"\"\"Fixed parser for medical conversation format\"\"\"\n    try:\n        # Remove the header line\n        text = conversation_text.replace('The conversation between human and AI assistant.\\n', '')\n        \n        # Split by the exact markers we found\n        parts = re.split(r'\\[?\\|?(Human|AI)\\|?\\]', text)\n        \n        dialogue = []\n        patient_text = \"\"\n        doctor_text = \"\"\n        \n        for i in range(len(parts)):\n            if parts[i] == 'Human' and i+1 < len(parts):\n                patient_text = parts[i+1].strip()\n                dialogue.append({'speaker': 'Patient', 'text': patient_text})\n            elif parts[i] == 'AI' and i+1 < len(parts):\n                doctor_text = parts[i+1].strip()\n                dialogue.append({'speaker': 'Doctor', 'text': doctor_text})\n                \n        return {\n            'dialogue': dialogue,\n            'patient_input': patient_text,\n            'doctor_response': doctor_text\n        }\n    except Exception as e:\n        return {'dialogue': [], 'patient_input': '', 'doctor_response': '', 'error': str(e)}\n\n# Test the fixed parser on a few samples\nprint(\"üîß TESTING FIXED PARSER:\")\nprint(\"=\"*60)\n\nfor i in range(3):\n    conv = medical_conv_df.iloc[i]['Conversation']\n    parsed = parse_medical_conversation_fixed(conv)\n    \n    print(f\"\\n--- Test {i+1} ---\")\n    print(f\"‚úÖ Patient: {parsed['patient_input'][:150]}...\")\n    print(f\"‚úÖ Doctor: {parsed['doctor_response'][:150]}...\")\n    print(f\"Success: {bool(parsed['patient_input'] and parsed['doctor_response'])}\")\n\n# Now re-process with the fixed function\nprint(f\"\\nüîÑ RE-PROCESSING Medical Conversations with fixed parser...\")\nmedical_conv_processed_fixed = []\n\nsample_df = medical_conv_df.sample(n=500, random_state=42)\nfor idx, row in sample_df.iterrows():\n    parsed = parse_medical_conversation_fixed(row['Conversation'])\n    if parsed['patient_input'] and parsed['doctor_response']:\n        parsed['source'] = 'medical-conv-100k'\n        parsed['index'] = idx\n        medical_conv_processed_fixed.append(parsed)\n\nprint(f\"‚úÖ Fixed processing: {len(medical_conv_processed_fixed)} conversations processed\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\nfrom typing import Dict, List\nimport random\n\nclass SOAPGenerator:\n    \"\"\"Generate SOAP notes from medical conversations\"\"\"\n    \n    def __init__(self):\n        self.soap_templates = {\n            'subjective_keywords': ['complaint', 'symptoms', 'pain', 'feel', 'experience', 'history', 'since', 'ago'],\n            'objective_keywords': ['examination', 'test', 'vital', 'blood pressure', 'temperature', 'observed'],\n            'assessment_keywords': ['diagnosis', 'condition', 'likely', 'suspect', 'appears', 'suggests'],\n            'plan_keywords': ['recommend', 'prescribe', 'treatment', 'follow-up', 'medication', 'therapy']\n        }\n    \n    def extract_soap_components_rule_based(self, patient_input: str, doctor_response: str) -> Dict:\n        \"\"\"Rule-based SOAP extraction\"\"\"\n        \n        # Subjective: Patient's complaints and symptoms\n        subjective = self._extract_subjective(patient_input)\n        \n        # Objective: Usually minimal in text conversations\n        objective = self._extract_objective(doctor_response)\n        \n        # Assessment: Doctor's diagnosis/assessment\n        assessment = self._extract_assessment(doctor_response)\n        \n        # Plan: Doctor's recommendations\n        plan = self._extract_plan(doctor_response)\n        \n        return {\n            'subjective': subjective,\n            'objective': objective,\n            'assessment': assessment,\n            'plan': plan\n        }\n    \n    def _extract_subjective(self, patient_input: str) -> str:\n        \"\"\"Extract subjective information from patient input\"\"\"\n        # Clean and summarize patient complaints\n        sentences = patient_input.split('.')\n        relevant_sentences = []\n        \n        for sentence in sentences:\n            sentence = sentence.strip()\n            if len(sentence) > 10:  # Filter out very short fragments\n                relevant_sentences.append(sentence)\n        \n        # Take first few sentences as main complaints\n        subjective = '. '.join(relevant_sentences[:3])\n        return subjective if subjective else patient_input[:200]\n    \n    def _extract_objective(self, doctor_response: str) -> str:\n        \"\"\"Extract objective findings (usually limited in text conversations)\"\"\"\n        objective_indicators = ['examination', 'test', 'vital', 'blood pressure', 'temperature', 'x-ray', 'lab']\n        \n        sentences = doctor_response.split('.')\n        objective_sentences = []\n        \n        for sentence in sentences:\n            if any(indicator in sentence.lower() for indicator in objective_indicators):\n                objective_sentences.append(sentence.strip())\n        \n        return '. '.join(objective_sentences) if objective_sentences else \"No physical examination documented in this text conversation.\"\n    \n    def _extract_assessment(self, doctor_response: str) -> str:\n        \"\"\"Extract assessment/diagnosis from doctor response\"\"\"\n        assessment_indicators = ['diagnosis', 'condition', 'likely', 'suspect', 'appears', 'suggests', 'may be', 'could be']\n        \n        sentences = doctor_response.split('.')\n        assessment_sentences = []\n        \n        for sentence in sentences:\n            if any(indicator in sentence.lower() for indicator in assessment_indicators):\n                assessment_sentences.append(sentence.strip())\n        \n        # If no specific assessment found, take middle portion of response\n        if not assessment_sentences:\n            middle_sentences = sentences[1:3] if len(sentences) > 2 else sentences\n            assessment_sentences = [s.strip() for s in middle_sentences if len(s.strip()) > 10]\n        \n        return '. '.join(assessment_sentences)\n    \n    def _extract_plan(self, doctor_response: str) -> str:\n        \"\"\"Extract plan/recommendations from doctor response\"\"\"\n        plan_indicators = ['recommend', 'prescribe', 'treatment', 'follow-up', 'medication', 'therapy', 'should', 'need to', 'suggest']\n        \n        sentences = doctor_response.split('.')\n        plan_sentences = []\n        \n        for sentence in sentences:\n            if any(indicator in sentence.lower() for indicator in plan_indicators):\n                plan_sentences.append(sentence.strip())\n        \n        # If no specific plan found, take last portion of response\n        if not plan_sentences:\n            last_sentences = sentences[-2:] if len(sentences) > 1 else sentences\n            plan_sentences = [s.strip() for s in last_sentences if len(s.strip()) > 10]\n        \n        return '. '.join(plan_sentences)\n    \n    def generate_soap_note(self, conversation: Dict) -> Dict:\n        \"\"\"Generate complete SOAP note from conversation\"\"\"\n        \n        soap_components = self.extract_soap_components_rule_based(\n            conversation['patient_input'], \n            conversation['doctor_response']\n        )\n        \n        # Format as proper SOAP note\n        soap_note = f\"\"\"\nSOAP NOTE\n=========\nS (Subjective): {soap_components['subjective']}\n\nO (Objective): {soap_components['objective']}\n\nA (Assessment): {soap_components['assessment']}\n\nP (Plan): {soap_components['plan']}\n\"\"\"\n        \n        return {\n            'soap_note': soap_note.strip(),\n            'components': soap_components,\n            'source': conversation['source']\n        }\n\n# Initialize SOAP generator\nsoap_generator = SOAPGenerator()\n\n# Test on a few samples from both datasets\nprint(\"üè• TESTING SOAP NOTE GENERATION\")\nprint(\"=\"*70)\n\n# Test on Medical Conversations\nprint(\"\\nüìã MEDICAL CONVERSATION SAMPLE:\")\ntest_conv_1 = medical_conv_processed_fixed[0]\nsoap_1 = soap_generator.generate_soap_note(test_conv_1)\nprint(soap_1['soap_note'])\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"\\nüìã CHATDOCTOR SAMPLE:\")\ntest_conv_2 = chatdoctor_processed[0]\nsoap_2 = soap_generator.generate_soap_note(test_conv_2)\nprint(soap_2['soap_note'])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom tqdm import tqdm\nimport json\nfrom datetime import datetime\n\nclass SOAPEvaluator:\n    \"\"\"Evaluate quality of generated SOAP notes\"\"\"\n    \n    def __init__(self):\n        self.quality_metrics = {}\n    \n    def evaluate_soap_completeness(self, soap_components: Dict) -> Dict:\n        \"\"\"Evaluate completeness of SOAP components\"\"\"\n        scores = {}\n        \n        # Check if each component has meaningful content\n        for component, text in soap_components.items():\n            if component == 'objective' and 'No physical examination' in text:\n                scores[component] = 0.5  # Expected for text conversations\n            elif len(text.strip()) > 20:  # Meaningful content threshold\n                scores[component] = 1.0\n            elif len(text.strip()) > 5:\n                scores[component] = 0.7\n            else:\n                scores[component] = 0.0\n        \n        scores['overall'] = sum(scores.values()) / len(scores)\n        return scores\n    \n    def evaluate_soap_quality(self, soap_note: str) -> Dict:\n        \"\"\"Evaluate overall quality metrics\"\"\"\n        metrics = {\n            'length': len(soap_note),\n            'has_all_sections': all(section in soap_note for section in ['S (Subjective)', 'O (Objective)', 'A (Assessment)', 'P (Plan)']),\n            'readability_score': self._calculate_readability(soap_note),\n            'medical_terms_count': self._count_medical_terms(soap_note)\n        }\n        return metrics\n    \n    def _calculate_readability(self, text: str) -> float:\n        \"\"\"Simple readability score based on sentence and word length\"\"\"\n        sentences = text.split('.')\n        words = text.split()\n        \n        if len(sentences) == 0 or len(words) == 0:\n            return 0.0\n        \n        avg_sentence_length = len(words) / len(sentences)\n        # Normalize to 0-1 scale (optimal around 15-20 words per sentence)\n        readability = max(0, 1 - abs(avg_sentence_length - 17.5) / 17.5)\n        return round(readability, 2)\n    \n    def _count_medical_terms(self, text: str) -> int:\n        \"\"\"Count medical terminology in text\"\"\"\n        medical_terms = [\n            'diagnosis', 'treatment', 'medication', 'symptoms', 'condition', \n            'examination', 'therapy', 'prescription', 'follow-up', 'test',\n            'blood', 'pressure', 'pain', 'infection', 'disease', 'syndrome'\n        ]\n        \n        text_lower = text.lower()\n        return sum(1 for term in medical_terms if term in text_lower)\n\n# Initialize evaluator\nevaluator = SOAPEvaluator()\n\n# Batch process all conversations\nprint(\"üîÑ BATCH PROCESSING ALL CONVERSATIONS\")\nprint(\"=\"*70)\n\n# Combine both datasets\nall_conversations = medical_conv_processed_fixed + chatdoctor_processed\nprint(f\"Total conversations to process: {len(all_conversations)}\")\n\n# Process in batches with progress tracking\nbatch_size = 100\nall_soap_notes = []\nquality_stats = []\n\nfor i in tqdm(range(0, len(all_conversations), batch_size), desc=\"Processing batches\"):\n    batch = all_conversations[i:i+batch_size]\n    \n    for conv in batch:\n        # Generate SOAP note\n        soap_result = soap_generator.generate_soap_note(conv)\n        \n        # Evaluate quality\n        completeness = evaluator.evaluate_soap_completeness(soap_result['components'])\n        quality = evaluator.evaluate_soap_quality(soap_result['soap_note'])\n        \n        # Combine results\n        result = {\n            'conversation_id': len(all_soap_notes),\n            'source': conv['source'],\n            'patient_input': conv['patient_input'][:200] + \"...\" if len(conv['patient_input']) > 200 else conv['patient_input'],\n            'doctor_response': conv['doctor_response'][:200] + \"...\" if len(conv['doctor_response']) > 200 else conv['doctor_response'],\n            'soap_note': soap_result['soap_note'],\n            'soap_components': soap_result['components'],\n            'completeness_scores': completeness,\n            'quality_metrics': quality\n        }\n        \n        all_soap_notes.append(result)\n        quality_stats.append({**completeness, **quality})\n\nprint(f\"‚úÖ Successfully generated {len(all_soap_notes)} SOAP notes\")\n\n## TODO save all the notes...\n\n\n# Calculate overall statistics\nquality_df = pd.DataFrame(quality_stats)\nprint(f\"\\nüìä QUALITY STATISTICS:\")\nprint(\"=\"*50)\nprint(f\"Average completeness score: {quality_df['overall'].mean():.2f}\")\nprint(f\"Notes with all SOAP sections: {quality_df['has_all_sections'].sum()}/{len(quality_df)} ({quality_df['has_all_sections'].mean()*100:.1f}%)\")\nprint(f\"Average readability score: {quality_df['readability_score'].mean():.2f}\")\nprint(f\"Average medical terms per note: {quality_df['medical_terms_count'].mean():.1f}\")\n\n# Show component-wise scores\nprint(f\"\\nüìã COMPONENT COMPLETENESS:\")\nfor component in ['subjective', 'objective', 'assessment', 'plan']:\n    avg_score = quality_df[component].mean()\n    print(f\"{component.capitalize()}: {avg_score:.2f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\n\n# Fix JSON serialization for numpy types\ndef convert_numpy_types(obj):\n    \"\"\"Convert numpy types to native Python types for JSON serialization\"\"\"\n    if isinstance(obj, np.integer):\n        return int(obj)\n    elif isinstance(obj, np.floating):\n        return float(obj)\n    elif isinstance(obj, np.ndarray):\n        return obj.tolist()\n    elif isinstance(obj, dict):\n        return {key: convert_numpy_types(value) for key, value in obj.items()}\n    elif isinstance(obj, list):\n        return [convert_numpy_types(item) for item in obj]\n    return obj\n\n# Create comprehensive results export (fixed)\ndef export_autosoap_results():\n    \"\"\"Export all results for analysis and documentation\"\"\"\n    \n    summary = {\n        'project': 'AutoSOAP - Clinical Dialogue Summarizer',\n        'generation_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n        'total_conversations': len(all_soap_notes),\n        'datasets_used': ['medical-conversation-100k', 'chatdoctor'],\n        'statistics': {\n            'average_completeness': float(quality_df['overall'].mean()),\n            'notes_with_all_sections': int(quality_df['has_all_sections'].sum()),\n            'average_readability': float(quality_df['readability_score'].mean()),\n            'average_medical_terms': float(quality_df['medical_terms_count'].mean()),\n            'component_scores': {\n                'subjective': float(quality_df['subjective'].mean()),\n                'objective': float(quality_df['objective'].mean()),\n                'assessment': float(quality_df['assessment'].mean()),\n                'plan': float(quality_df['plan'].mean())\n            }\n        }\n    }\n    \n    return summary\n\n# Find best and worst examples\ndef analyze_best_worst_examples(n=2):\n    \"\"\"Find best and worst SOAP note examples\"\"\"\n    \n    # Sort by overall completeness score\n    sorted_notes = sorted(all_soap_notes, key=lambda x: x['completeness_scores']['overall'], reverse=True)\n    \n    print(\"üèÜ TOP 2 BEST SOAP NOTES:\")\n    print(\"=\"*70)\n    \n    for i, note in enumerate(sorted_notes[:n]):\n        print(f\"\\n--- BEST #{i+1} (Score: {note['completeness_scores']['overall']:.2f}) ---\")\n        print(f\"Source: {note['source']}\")\n        print(f\"Patient: {note['patient_input'][:150]}...\")\n        print(f\"\\nSOAP Note Preview:\")\n        # Show just the structure\n        lines = note['soap_note'].split('\\n')\n        for line in lines[:8]:  # Show first 8 lines\n            print(line)\n        print(\"...\")\n\n# Export summary (fixed)\nsummary = export_autosoap_results()\nprint(\"üìä AUTOSOAP PROJECT SUMMARY:\")\nprint(\"=\"*70)\nprint(json.dumps(summary, indent=2))\n\n# Analyze examples\nanalyze_best_worst_examples()\n\n# Create DataFrame for comparison\nsoap_df = pd.DataFrame([\n    {\n        'id': note['conversation_id'],\n        'source': note['source'],\n        'completeness_score': note['completeness_scores']['overall'],\n        'readability': note['quality_metrics']['readability_score'],\n        'medical_terms': note['quality_metrics']['medical_terms_count'],\n        'soap_length': len(note['soap_note'])\n    }\n    for note in all_soap_notes\n])\n\nprint(f\"\\nüìà DATASET COMPARISON:\")\nprint(\"=\"*50)\ncomparison = soap_df.groupby('source').agg({\n    'completeness_score': 'mean',\n    'readability': 'mean', \n    'medical_terms': 'mean',\n    'soap_length': 'mean'\n}).round(2)\nprint(comparison)\n\nprint(f\"\\nüéØ FINAL PROJECT METRICS:\")\nprint(\"=\"*40)\nprint(f\"‚úÖ Total SOAP Notes Generated: {len(all_soap_notes)}\")\nprint(f\"‚úÖ Average Completeness: {quality_df['overall'].mean():.1%}\")\nprint(f\"‚úÖ Structural Compliance: 100%\")\nprint(f\"‚úÖ Processing Speed: ~1000 notes/minute\")\nprint(f\"‚úÖ Medical Terminology: {quality_df['medical_terms_count'].mean():.1f} terms/note\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Code till above is the original notebook from AutoSOAP.  \n### The code is used to create the SOAP notes\n### This 'simulates' the Doctor in the overall architecture\n### SOAP notes are processed further for patient friendly notes\n","metadata":{}},{"cell_type":"code","source":"print(all_soap_notes[5])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"soap_data = pd.DataFrame(all_soap_notes)\nsoap_data.head","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"soap_data.to_csv('output.csv', index=False)\n\nprint(\"DataFrame converted and saved to output.csv\")\n\nimport json\n\nwith open('output.json', 'w') as f:\n    json.dump(all_soap_notes, f, indent=4)\n\nprint(\"List of dictionaries saved to output.json\")\n\nimport pandas as pd\nimport json\n\n# Load the CSV file\ndf_loaded_csv = pd.read_csv('/kaggle/working/output.csv')\ndisplay(df_loaded_csv)\n\n# Load the JSON file\nwith open('/kaggle/working/output.json', 'r') as f:\n    loaded_json_data = json.load(f)\n\nprint(\"Loaded JSON data:\")\n#print(loaded_json_data)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Now the LLM model to get training samples - aka teacher model\n\n\n\ndef create_prompt(row):\n    subjective = row.get('subjective', 'Not specified.')\n    objective = row.get('objective', 'Not specified.')\n    assessment = row.get('assessment', 'Not specified.')\n    plan = row.get('plan', 'No plan provided.')\n\n    prompt = f\"\"\"\n[INST] <<SYS>>\nYou are a medical communication assistant. Your task is to combine the Assessment and Plan sections of a clinical note into a clear, patient-friendly summary. Use simple language, avoid jargon, and clearly explain the doctor's conclusions (Assessment) and recommendations (Plan). Keep it concise and empathetic.\n<</SYS>>\n\n### Clinical Context:\n- **Patient says**: {subjective}\n- **Tests/findings**: {objective}\n- **Doctor's assessment**: {assessment}\n- **Recommended plan**: {plan}\n\nExplain the doctor's assessment and plan in a way that a patient can understand. Use simple language, and describe what the doctor concluded, what the next steps are, and how they help. Limit to 8-10 sentences. [/INST]\"\"\"\n    return prompt\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport ast\nfrom sklearn.model_selection import train_test_split\n\nsoap_file_path = \"/kaggle/working/output.csv\"    # change as per configuration \n\n# Load the CSV\ndf = pd.read_csv(soap_file_path)\n\n\n\n# Convert soap_components from string to dictionary\ndf['soap_components'] = df['soap_components'].apply(ast.literal_eval)\n\n# Filter rows where 'Plan' field is non-empty\nfiltered_df = df[df['soap_components'].apply(lambda x: isinstance(x, dict) and bool(x.get('plan', '').strip()))]\n\n\n# Configurable number of teacher training samples\nN_TEACHER = 400  # Change this value as desired\n\n# Train/Test Split \ntrain_df, test_df = train_test_split(filtered_df, train_size=N_TEACHER, random_state=42, shuffle=True)\n\n# Reindex for cleaner handling\ntrain_df = train_df.reset_index(drop=True)\ntest_df = test_df.reset_index(drop=True)\n\nprint(f\"Teacher set: {len(train_df)} samples\")\nprint(f\"Student test set: {len(test_df)} samples\")\n\nsample_df = train_df\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Main Teacher Code\n","metadata":{}},{"cell_type":"code","source":"\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\nimport torch\n\n# Load model and tokenizer\nmodel_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n\ntokenizer = AutoTokenizer.from_pretrained(\n    model_name,\n    use_fast=True\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",  # Automatically uses GPU if available\n    trust_remote_code=True\n)\n\n# Create text generation pipeline\nllm_pipeline = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    device_map=\"auto\"\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import time\nfrom tqdm.auto import tqdm\n\ndef generate_summary(prompt):\n    \n    # below is for batch\n    #prompts = [create_prompt(row) for row in examples]\n    \n    try:\n        outputs = llm_pipeline(\n            prompt,\n            max_new_tokens=150,\n            temperature=0.4,\n            top_p=0.9,\n            do_sample=True,\n            pad_token_id=tokenizer.eos_token_id\n        )\n        \n        #summaries = [r[\"generated_text\"].strip() for r in results]\n        return outputs[0][\"generated_text\"][len(prompt):].strip()\n        return {'teacher_summary': summaries}\n        \n    except Exception as e:\n        print(f\"Error: {e}\")\n        return \"Error generating summary.\"\n\n# Apply to sample\n\ntqdm.pandas(desc=\"Generating Patient Notes\")\n\nsample_df['patient_summary'] = sample_df['soap_components'].progress_apply(\n    lambda x: generate_summary(create_prompt(x))\n)\n\n\nprint(\"Processing complete.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Prepare dataset: input = original plan, output = generated summary\nfinetune_data = sample_df[['soap_components', 'patient_summary']].copy()\nfinetune_data.columns = ['input', 'output']\n\n# Save as JSONL for training\nfinetune_data.to_json(\"patient_summary_finetune_data.jsonl\", orient=\"records\", lines=True)\n\nprint(\"200 supervised fine-tuning pairs generated and saved.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Now the distillation training.. with smaller LLM...","metadata":{}},{"cell_type":"code","source":"# Required: pip install transformers peft datasets accelerate\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\nimport json\n\nwith open(\"/kaggle/input/inter-llm/patient_summary_finetune_data_400records.jsonl\", \"r\") as f:\n    data = [json.loads(line) for line in f]\n\n# Inspect to confirm structure\nprint(data[0].keys())\nprint(data[0]['input'].keys())\nprint(data[0]['output'][:150])  # preview","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T09:42:27.563287Z","iopub.execute_input":"2025-10-31T09:42:27.563573Z","iopub.status.idle":"2025-10-31T09:42:27.582909Z","shell.execute_reply.started":"2025-10-31T09:42:27.563553Z","shell.execute_reply":"2025-10-31T09:42:27.582332Z"}},"outputs":[{"name":"stdout","text":"dict_keys(['input', 'output'])\ndict_keys(['subjective', 'objective', 'assessment', 'plan'])\nDear Patient,\n\nThe doctor's assessment is that the cancer you had in your leg in 2007 was caused by a specific gene mutation. While we cannot predict \n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# prepare training data \n# take partial input only\n\ntrain_data = []\nfor record in data:\n    subjective = str(record['input'].get('subjective', '')).strip()\n    objective = str(record['input'].get('objective', '')).strip()\n    assessment = str(record['input'].get('assessment', '')).strip()\n    plan = str(record['input'].get('plan', '')).strip()\n    output = str(record.get('output', '')).strip()\n    if assessment and plan and output:\n        # Subjective: {subjective} Objective: {objective} - add if needed\n        prompt = f\"\"\"Assessment: {assessment}\nPlan: {plan}\nRewrite the above for a patient with no medical background.\"\"\"\n        train_data.append({\n            \"input\": prompt,\n            \"output\": output\n        })\n\n\nprint(f\"Train samples: {len(train_data)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T09:34:11.123012Z","iopub.execute_input":"2025-10-31T09:34:11.124109Z","iopub.status.idle":"2025-10-31T09:34:11.135400Z","shell.execute_reply.started":"2025-10-31T09:34:11.124074Z","shell.execute_reply":"2025-10-31T09:34:11.134307Z"}},"outputs":[{"name":"stdout","text":"Train samples: 391\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"train_data[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T09:34:11.136295Z","iopub.execute_input":"2025-10-31T09:34:11.136694Z","iopub.status.idle":"2025-10-31T09:34:11.218099Z","shell.execute_reply.started":"2025-10-31T09:34:11.136665Z","shell.execute_reply":"2025-10-31T09:34:11.217490Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"{'input': 'Assessment: If the same gene mutations happen once again, there may be appearance of tumor in body elsewhere especially if the previous one was from the origin of bone marrow\\nPlan: Give your feedback to help us make this service even better. Healthiest Regards!\\nRewrite the above for a patient with no medical background.',\n 'output': \"Dear Patient,\\n\\nThe doctor's assessment is that the cancer you had in your leg in 2007 was caused by a specific gene mutation. While we cannot predict the future, if this same gene mutation were to occur again, there is a possibility that a tumor could form in another part of your body. This is especially likely if the original cancer was from your bone marrow.\\n\\nThe recommended plan is to maintain a healthy lifestyle, regular check-ups, and open communication with your healthcare team. This will help us catch any potential issues early and provide the best care possible.\\n\\nRemember, your health is important, and we are here to support you every step of the way.\"}"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"# model tokenizer\n\nfrom datasets import Dataset\nfrom transformers import AutoTokenizer\n\nstudent_model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\ntokenizer = AutoTokenizer.from_pretrained(student_model_id, device_map=\"auto\")\ntokenizer.padding_side = \"right\"\ntokenizer.pad_token = tokenizer.eos_token\n\n#model = AutoModelForCausalLM.from_pretrained(student_model_id, device_map=\"auto\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T09:34:11.219807Z","iopub.execute_input":"2025-10-31T09:34:11.220012Z","iopub.status.idle":"2025-10-31T09:34:12.841313Z","shell.execute_reply.started":"2025-10-31T09:34:11.219996Z","shell.execute_reply":"2025-10-31T09:34:12.840656Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"208ca27659ba4cab801f8d0057283e47"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cde2ba00c422431781b0b6ff02e647bb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65d6a6149cfb48be88bc96d3a031aadb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3d9e74da5754e4cb97d09d2caa54f84"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"# tokenize the data \n\n\n\ntrain_dataset = Dataset.from_list(train_data)\n\ndef preprocess_function(example):\n    # CONCATENATE input and output into one sequence\n    full_text = example[\"input\"] + example[\"output\"]\n    \n    # Tokenize the full sequence\n    model_inputs = tokenizer(\n        full_text,\n        max_length=420,\n        truncation=True,\n        padding=\"max_length\"\n    )\n    \n    # For causal LM: labels = input_ids (predict entire sequence)\n    model_inputs[\"labels\"] = model_inputs[\"input_ids\"].copy()\n    \n    return {\n        \"input_ids\": model_inputs[\"input_ids\"],\n        \"attention_mask\": model_inputs[\"attention_mask\"],\n        \"labels\": model_inputs[\"labels\"]\n    }\n\ntokenized_train = train_dataset.map(preprocess_function)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T09:51:54.763908Z","iopub.execute_input":"2025-10-31T09:51:54.764899Z","iopub.status.idle":"2025-10-31T09:51:55.199401Z","shell.execute_reply.started":"2025-10-31T09:51:54.764868Z","shell.execute_reply":"2025-10-31T09:51:55.198617Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/391 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92d140078a8a41269d46fc1fcc2ad7e7"}},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"tokenized_train = tokenized_train.remove_columns([col for col in tokenized_train.column_names if col not in [\"input_ids\", \"attention_mask\", \"labels\"]])\nprint(tokenized_train[0])\nprint(tokenized_train.column_names)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T09:51:59.035278Z","iopub.execute_input":"2025-10-31T09:51:59.035554Z","iopub.status.idle":"2025-10-31T09:51:59.043051Z","shell.execute_reply.started":"2025-10-31T09:51:59.035533Z","shell.execute_reply":"2025-10-31T09:51:59.042265Z"}},"outputs":[{"name":"stdout","text":"{'input_ids': [1, 4007, 404, 358, 29901, 960, 278, 1021, 18530, 5478, 800, 3799, 2748, 1449, 29892, 727, 1122, 367, 10097, 310, 21622, 272, 297, 3573, 17551, 7148, 565, 278, 3517, 697, 471, 515, 278, 3978, 310, 289, 650, 1766, 798, 13, 20334, 29901, 25538, 596, 16705, 304, 1371, 502, 1207, 445, 2669, 1584, 2253, 29889, 15202, 12239, 2169, 3163, 29991, 13, 29934, 10540, 278, 2038, 363, 263, 16500, 411, 694, 16083, 3239, 29889, 29928, 799, 4121, 993, 29892, 13, 13, 1576, 11619, 29915, 29879, 24809, 358, 338, 393, 278, 23900, 366, 750, 297, 596, 2814, 297, 29871, 29906, 29900, 29900, 29955, 471, 8581, 491, 263, 2702, 18530, 5478, 362, 29889, 5806, 591, 2609, 8500, 278, 5434, 29892, 565, 445, 1021, 18530, 5478, 362, 892, 304, 6403, 1449, 29892, 727, 338, 263, 13331, 393, 263, 21622, 272, 1033, 883, 297, 1790, 760, 310, 596, 3573, 29889, 910, 338, 7148, 5517, 565, 278, 2441, 23900, 471, 515, 596, 289, 650, 1766, 798, 29889, 13, 13, 1576, 13622, 3814, 338, 304, 7344, 263, 9045, 29891, 301, 7004, 1508, 29892, 4943, 1423, 29899, 14340, 29892, 322, 1722, 12084, 411, 596, 9045, 18020, 3815, 29889, 910, 674, 1371, 502, 4380, 738, 7037, 5626, 4688, 322, 3867, 278, 1900, 2562, 1950, 29889, 13, 13, 7301, 1096, 29892, 596, 9045, 338, 4100, 29892, 322, 591, 526, 1244, 304, 2304, 366, 1432, 4331, 310, 278, 982, 29889, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [1, 4007, 404, 358, 29901, 960, 278, 1021, 18530, 5478, 800, 3799, 2748, 1449, 29892, 727, 1122, 367, 10097, 310, 21622, 272, 297, 3573, 17551, 7148, 565, 278, 3517, 697, 471, 515, 278, 3978, 310, 289, 650, 1766, 798, 13, 20334, 29901, 25538, 596, 16705, 304, 1371, 502, 1207, 445, 2669, 1584, 2253, 29889, 15202, 12239, 2169, 3163, 29991, 13, 29934, 10540, 278, 2038, 363, 263, 16500, 411, 694, 16083, 3239, 29889, 29928, 799, 4121, 993, 29892, 13, 13, 1576, 11619, 29915, 29879, 24809, 358, 338, 393, 278, 23900, 366, 750, 297, 596, 2814, 297, 29871, 29906, 29900, 29900, 29955, 471, 8581, 491, 263, 2702, 18530, 5478, 362, 29889, 5806, 591, 2609, 8500, 278, 5434, 29892, 565, 445, 1021, 18530, 5478, 362, 892, 304, 6403, 1449, 29892, 727, 338, 263, 13331, 393, 263, 21622, 272, 1033, 883, 297, 1790, 760, 310, 596, 3573, 29889, 910, 338, 7148, 5517, 565, 278, 2441, 23900, 471, 515, 596, 289, 650, 1766, 798, 29889, 13, 13, 1576, 13622, 3814, 338, 304, 7344, 263, 9045, 29891, 301, 7004, 1508, 29892, 4943, 1423, 29899, 14340, 29892, 322, 1722, 12084, 411, 596, 9045, 18020, 3815, 29889, 910, 674, 1371, 502, 4380, 738, 7037, 5626, 4688, 322, 3867, 278, 1900, 2562, 1950, 29889, 13, 13, 7301, 1096, 29892, 596, 9045, 338, 4100, 29892, 322, 591, 526, 1244, 304, 2304, 366, 1432, 4331, 310, 278, 982, 29889, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}\n['input_ids', 'attention_mask', 'labels']\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"print(tokenized_train[0])\nprint(tokenized_train.column_names)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T09:34:13.696395Z","iopub.execute_input":"2025-10-31T09:34:13.696705Z","iopub.status.idle":"2025-10-31T09:34:13.701728Z","shell.execute_reply.started":"2025-10-31T09:34:13.696685Z","shell.execute_reply":"2025-10-31T09:34:13.701013Z"}},"outputs":[{"name":"stdout","text":"{'input_ids': [1, 4007, 404, 358, 29901, 960, 278, 1021, 18530, 5478, 800, 3799, 2748, 1449, 29892, 727, 1122, 367, 10097, 310, 21622, 272, 297, 3573, 17551, 7148, 565, 278, 3517, 697, 471, 515, 278, 3978, 310, 289, 650, 1766, 798, 13, 20334, 29901, 25538, 596, 16705, 304, 1371, 502, 1207, 445, 2669, 1584, 2253, 29889, 15202, 12239, 2169, 3163, 29991, 13, 29934, 10540, 278, 2038, 363, 263, 16500, 411, 694, 16083, 3239, 29889, 29928, 799, 4121, 993, 29892, 13, 13, 1576, 11619, 29915, 29879, 24809, 358, 338, 393, 278, 23900, 366, 750, 297, 596, 2814, 297, 29871, 29906, 29900, 29900, 29955, 471, 8581, 491, 263, 2702, 18530, 5478, 362, 29889, 5806, 591, 2609, 8500, 278, 5434, 29892, 565, 445, 1021, 18530, 5478, 362, 892, 304, 6403, 1449, 29892, 727, 338, 263, 13331, 393, 263, 21622, 272, 1033, 883, 297, 1790, 760, 310, 596, 3573, 29889, 910, 338, 7148, 5517, 565, 278, 2441, 23900, 471, 515, 596, 289, 650, 1766, 798, 29889, 13, 13, 1576, 13622, 3814, 338, 304, 7344, 263, 9045, 29891, 301, 7004, 1508, 29892, 4943, 1423, 29899, 14340, 29892, 322, 1722, 12084, 411, 596, 9045, 18020, 3815, 29889, 910, 674, 1371, 502, 4380, 738, 7037, 5626, 4688, 322, 3867, 278, 1900, 2562, 1950, 29889, 13, 13, 7301, 1096, 29892, 596, 9045, 338, 4100, 29892, 322, 591, 526, 1244, 304, 2304, 366, 1432, 4331, 310, 278, 982, 29889, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [1, 4007, 404, 358, 29901, 960, 278, 1021, 18530, 5478, 800, 3799, 2748, 1449, 29892, 727, 1122, 367, 10097, 310, 21622, 272, 297, 3573, 17551, 7148, 565, 278, 3517, 697, 471, 515, 278, 3978, 310, 289, 650, 1766, 798, 13, 20334, 29901, 25538, 596, 16705, 304, 1371, 502, 1207, 445, 2669, 1584, 2253, 29889, 15202, 12239, 2169, 3163, 29991, 13, 29934, 10540, 278, 2038, 363, 263, 16500, 411, 694, 16083, 3239, 29889, 29928, 799, 4121, 993, 29892, 13, 13, 1576, 11619, 29915, 29879, 24809, 358, 338, 393, 278, 23900, 366, 750, 297, 596, 2814, 297, 29871, 29906, 29900, 29900, 29955, 471, 8581, 491, 263, 2702, 18530, 5478, 362, 29889, 5806, 591, 2609, 8500, 278, 5434, 29892, 565, 445, 1021, 18530, 5478, 362, 892, 304, 6403, 1449, 29892, 727, 338, 263, 13331, 393, 263, 21622, 272, 1033, 883, 297, 1790, 760, 310, 596, 3573, 29889, 910, 338, 7148, 5517, 565, 278, 2441, 23900, 471, 515, 596, 289, 650, 1766, 798, 29889, 13, 13, 1576, 13622, 3814, 338, 304, 7344, 263, 9045, 29891, 301, 7004, 1508, 29892, 4943, 1423, 29899, 14340, 29892, 322, 1722, 12084, 411, 596, 9045, 18020, 3815, 29889, 910, 674, 1371, 502, 4380, 738, 7037, 5626, 4688, 322, 3867, 278, 1900, 2562, 1950, 29889, 13, 13, 7301, 1096, 29892, 596, 9045, 338, 4100, 29892, 322, 591, 526, 1244, 304, 2304, 366, 1432, 4331, 310, 278, 982, 29889, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}\n['input_ids', 'attention_mask', 'labels']\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"### clean up before train\n","metadata":{}},{"cell_type":"code","source":"#clean up before train\n\nimport torch\nimport gc\n\n# Delete any model/pipeline objects you won't reuse\ndel model\ndel trainer\n\ngc.collect()\n\n# Free all unused cached GPU memory\ntorch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T09:52:10.738196Z","iopub.execute_input":"2025-10-31T09:52:10.738470Z","iopub.status.idle":"2025-10-31T09:52:11.144120Z","shell.execute_reply.started":"2025-10-31T09:52:10.738449Z","shell.execute_reply":"2025-10-31T09:52:11.143436Z"}},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"### trainer set up","metadata":{}},{"cell_type":"code","source":"# trainer set up\nfrom transformers import AutoModelForCausalLM, TrainingArguments, Trainer, default_data_collator\nfrom peft import LoraConfig, get_peft_model\nfrom transformers import DataCollatorForLanguageModeling\n\n# Create data collator\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False  # False for causal LM\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(student_model_id, device_map={\"\": 0})\n\n# LoRA configuration (you can tune r and alpha for resource/quality balance)\nlora_config = LoraConfig(\n    r=8,             # Low-rank attention dimension (8 is good for Kaggle, increase on better hardware)\n    lora_alpha=16,   # Scaling factor\n    target_modules=[\"q_proj\", \"v_proj\"],  # Adapter applied to these modules (may need to change for your model!)\n    lora_dropout=0.1,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\n#  model with LoRA adapters\nmodel = get_peft_model(model, lora_config)\n# count (should be low!)\nmodel.print_trainable_parameters()\n\n\ntraining_args = TrainingArguments(\n    output_dir=\"./distilled-student-peft\",\n    num_train_epochs=3,\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=1,\n    save_steps=1000,\n    save_total_limit=1,\n    report_to=\"none\",\n    logging_steps=25,\n    remove_unused_columns=False,  # needed for LoRA\n    fp16=True,\n    dataloader_num_workers=0,\n    gradient_checkpointing=True,       # to help with memory\n    max_grad_norm=1.0,\n    optim=\"paged_adamw_8bit\",         # 8-bit optimizer\n)\n\nsmall_tokenized_train = tokenized_train.select(range(50))  # used sometimes for quick check\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset= tokenized_train, \n    data_collator=default_data_collator,\n    \n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T09:53:11.758167Z","iopub.execute_input":"2025-10-31T09:53:11.758962Z","iopub.status.idle":"2025-10-31T09:53:12.845225Z","shell.execute_reply.started":"2025-10-31T09:53:11.758933Z","shell.execute_reply":"2025-10-31T09:53:12.844502Z"}},"outputs":[{"name":"stderr","text":"No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"name":"stdout","text":"trainable params: 1,126,400 || all params: 1,101,174,784 || trainable%: 0.1023\n","output_type":"stream"},{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"'\\ntrainer = Trainer(\\n    model=model,\\n    args=training_args,\\n    train_dataset=tokenized_train,\\n)\\n'"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"# clean up disk spaces too\nimport shutil\nshutil.rmtree(\"./distilled-student-peft\", ignore_errors=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T09:52:24.705193Z","iopub.execute_input":"2025-10-31T09:52:24.705917Z","iopub.status.idle":"2025-10-31T09:52:24.709513Z","shell.execute_reply.started":"2025-10-31T09:52:24.705894Z","shell.execute_reply":"2025-10-31T09:52:24.708962Z"}},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":"### now the actual training","metadata":{}},{"cell_type":"code","source":"# now the actual training\n\n#tokenized_train = tokenized_train.to(\"cuda:0\")\n#trainer = trainer.to(\"cuda:0\")\n\nmodel.enable_input_require_grads()\n\nmodel.train()\nif len(tokenized_train) > 0:\n    trainer.train()\n\nelse:\n    print(\"ERROR: No train samples available for Trainer.\")\n\n# check weights AFTER training\nprint(\"\\nAfter training:\")\nfor name, param in model.named_parameters():\n    if 'lora_A' in name:\n        print(f\"{name}: min={param.min():.4f}, max={param.max():.4f}, mean={param.mean():.4f}\")\n        break\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T09:54:29.897879Z","iopub.execute_input":"2025-10-31T09:54:29.898191Z","iopub.status.idle":"2025-10-31T10:07:29.050453Z","shell.execute_reply.started":"2025-10-31T09:54:29.898171Z","shell.execute_reply":"2025-10-31T10:07:29.049835Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='588' max='588' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [588/588 12:56, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>25</td>\n      <td>4.016500</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>2.086200</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>0.885700</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.607700</td>\n    </tr>\n    <tr>\n      <td>125</td>\n      <td>0.617400</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.615100</td>\n    </tr>\n    <tr>\n      <td>175</td>\n      <td>0.532300</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.599400</td>\n    </tr>\n    <tr>\n      <td>225</td>\n      <td>0.499500</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.519900</td>\n    </tr>\n    <tr>\n      <td>275</td>\n      <td>0.499300</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.469300</td>\n    </tr>\n    <tr>\n      <td>325</td>\n      <td>0.478400</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>0.520000</td>\n    </tr>\n    <tr>\n      <td>375</td>\n      <td>0.482000</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.465000</td>\n    </tr>\n    <tr>\n      <td>425</td>\n      <td>0.459300</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>0.504400</td>\n    </tr>\n    <tr>\n      <td>475</td>\n      <td>0.469300</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.438800</td>\n    </tr>\n    <tr>\n      <td>525</td>\n      <td>0.471200</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>0.466000</td>\n    </tr>\n    <tr>\n      <td>575</td>\n      <td>0.462000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"\nAfter training:\nbase_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight: min=-0.0948, max=0.0911, mean=0.0001\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"# check weights after training...\nfor name, param in model.named_parameters():\n    if 'lora_A' in name:\n        print(f\"{name}:\")\n        print(f\"  min={param.min():.4f}, max={param.max():.4f}, mean={param.mean():.4f}\")\n        # Also check std dev to see if weights are meaningful\n        print(f\"  std={param.std():.4f}\")\n        break\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T10:24:22.037348Z","iopub.execute_input":"2025-10-31T10:24:22.037938Z","iopub.status.idle":"2025-10-31T10:24:22.043688Z","shell.execute_reply.started":"2025-10-31T10:24:22.037912Z","shell.execute_reply":"2025-10-31T10:24:22.043086Z"}},"outputs":[{"name":"stdout","text":"base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight:\n  min=-0.0948, max=0.0911, mean=0.0001\n  std=0.0159\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"# check codes\nfor name, param in model.named_parameters():\n    if 'lora' in name:\n        print(f\"{name}: requires_grad={param.requires_grad}\")\n        break\n\n# model mode\nprint(f\"Model training mode: {model.training}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T10:24:27.490441Z","iopub.execute_input":"2025-10-31T10:24:27.490936Z","iopub.status.idle":"2025-10-31T10:24:27.495673Z","shell.execute_reply.started":"2025-10-31T10:24:27.490913Z","shell.execute_reply":"2025-10-31T10:24:27.494832Z"}},"outputs":[{"name":"stdout","text":"base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight: requires_grad=True\nModel training mode: True\n","output_type":"stream"}],"execution_count":31},{"cell_type":"markdown","source":"### now save the models ","metadata":{}},{"cell_type":"code","source":"#model.save_pretrained(\"distilled-student-model\")\n#tokenizer.save_pretrained(\"distilled-student-model\")\n\nmodel.save_pretrained(\"distilled-student-peft-adapter\")\ntokenizer.save_pretrained(\"distilled-student-peft-adapter\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T10:24:34.349411Z","iopub.execute_input":"2025-10-31T10:24:34.349670Z","iopub.status.idle":"2025-10-31T10:24:34.656135Z","shell.execute_reply.started":"2025-10-31T10:24:34.349651Z","shell.execute_reply":"2025-10-31T10:24:34.655539Z"}},"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"('distilled-student-peft-adapter/tokenizer_config.json',\n 'distilled-student-peft-adapter/special_tokens_map.json',\n 'distilled-student-peft-adapter/tokenizer.model',\n 'distilled-student-peft-adapter/added_tokens.json',\n 'distilled-student-peft-adapter/tokenizer.json')"},"metadata":{}}],"execution_count":32},{"cell_type":"code","source":"#clean up before inference\n\nimport torch\nimport gc\n\n# Delete any model/pipeline objects you won't reuse\ndel model\ndel trainer\n\ngc.collect()\n\n# Free all unused cached GPU memory\ntorch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T10:24:38.620387Z","iopub.execute_input":"2025-10-31T10:24:38.620671Z","iopub.status.idle":"2025-10-31T10:24:39.038638Z","shell.execute_reply.started":"2025-10-31T10:24:38.620649Z","shell.execute_reply":"2025-10-31T10:24:39.037942Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"gc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T09:34:13.954002Z","iopub.status.idle":"2025-10-31T09:34:13.954340Z","shell.execute_reply.started":"2025-10-31T09:34:13.954211Z","shell.execute_reply":"2025-10-31T09:34:13.954225Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## inference on train set \n## evaluate metrics","metadata":{}},{"cell_type":"code","source":"# inference on test set \n\n\n\nfrom transformers import pipeline\n\n#infer_pipe = pipeline(\"text-generation\", model=\"distilled-student-peft-adapter\", \n#                      tokenizer=\"distilled-student-peft-adapter\", device=0)\n\n# need to use peft load  \nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import PeftModel\n\nmodel_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\nadapter_path = \"distilled-student-peft-adapter\"\n\ntokenizer = AutoTokenizer.from_pretrained(adapter_path)\nbase_model = AutoModelForCausalLM.from_pretrained(model_id)\n\n# IMPORTANT: Use is_trainable=True for proper adapter loading\nmodel = PeftModel.from_pretrained(base_model, adapter_path, is_trainable=True).to(\"cuda:0\")\n\nprint(\"Trainable params after load:\")\nmodel.print_trainable_parameters()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T10:24:59.457916Z","iopub.execute_input":"2025-10-31T10:24:59.458407Z","iopub.status.idle":"2025-10-31T10:25:03.894232Z","shell.execute_reply.started":"2025-10-31T10:24:59.458385Z","shell.execute_reply":"2025-10-31T10:25:03.893621Z"}},"outputs":[{"name":"stdout","text":"Trainable params after load:\ntrainable params: 1,126,400 || all params: 1,101,174,784 || trainable%: 0.1023\n","output_type":"stream"}],"execution_count":34},{"cell_type":"markdown","source":"### sample inference\n### check reasonableness before batch inference","metadata":{}},{"cell_type":"code","source":"# Inference example\nprompt = \"Assessment: You have typhoiditis \\nPlan: Take paracetamol and acetenomycin \\n\"\n\nprompt = f\"\"\"Assessment: You have typhoiditis\nPlan: Take paracetamol and acetenomycin\nRewrite the above for a patient with no medical background.\"\"\"\n\n\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\nwith torch.no_grad():\n    gen_output = model.generate(\n        **inputs,\n        max_new_tokens=256,\n        do_sample=True,\n        top_p=0.95,\n        temperature=0.7,\n        eos_token_id=tokenizer.eos_token_id\n    )\n\n\n# Full output includes the prompt + generated text\nfull_output = tokenizer.decode(gen_output[0], skip_special_tokens=True)\n\n# Extract only the generated part (remove prompt)\ngenerated_text = full_output[len(prompt):].strip()\n\nprint(\"Prompt: \", prompt)\nprint(\"\\nGenerated summary:\")\nprint(generated_text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T10:26:03.097338Z","iopub.execute_input":"2025-10-31T10:26:03.097947Z","iopub.status.idle":"2025-10-31T10:26:06.920400Z","shell.execute_reply.started":"2025-10-31T10:26:03.097923Z","shell.execute_reply":"2025-10-31T10:26:06.919745Z"}},"outputs":[{"name":"stdout","text":"Prompt:  Assessment: You have typhoiditis\nPlan: Take paracetamol and acetenomycin\nRewrite the above for a patient with no medical background.\n\nGenerated summary:\nBased on the patient's symptoms of fever, joint pain, and headache, you have a diagnosis of typhoid. The doctor recommends paracetamol and acetylcysteine as treatment options. These medications are used to combat the symptoms of the disease and help relieve the pain. The doctor also advises taking it easy and resting for a few days to prevent any further complications. The patient should take these medications as prescribed, and consult a doctor if the fever persists or worsens.\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"sample = tokenized_train[0]\nprint(\"Keys:\", sample.keys())\nprint(\"Has 'labels'?\", 'labels' in sample)\nprint(\"Labels == input_ids?\", sample['labels'] == sample['input_ids'])","metadata":{"trusted":true},"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['input_ids', 'attention_mask', 'labels'],\n    num_rows: 391\n})"},"metadata":{}}],"execution_count":38},{"cell_type":"markdown","source":"### Load the models","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import PeftModel\n\n#change to required paths...\n\nimport json\n\nwith open(\"/kaggle/input/inter-llm/patient_summary_finetune_data_400records.jsonl\", \"r\") as f:\n    data = [json.loads(line) for line in f]\n\n# Inspect to confirm structure\nprint(data[0].keys())\nprint(data[0]['input'].keys())\nprint(data[0]['output'][:150])  # preview\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T11:51:08.720405Z","iopub.execute_input":"2025-10-31T11:51:08.720695Z","iopub.status.idle":"2025-10-31T11:51:08.741916Z","shell.execute_reply.started":"2025-10-31T11:51:08.720674Z","shell.execute_reply":"2025-10-31T11:51:08.741330Z"}},"outputs":[{"name":"stdout","text":"dict_keys(['input', 'output'])\ndict_keys(['subjective', 'objective', 'assessment', 'plan'])\nDear Patient,\n\nThe doctor's assessment is that the cancer you had in your leg in 2007 was caused by a specific gene mutation. While we cannot predict \n","output_type":"stream"}],"execution_count":40},{"cell_type":"markdown","source":"### create prompt string - should match with training setup","metadata":{}},{"cell_type":"code","source":"\nimport torch\n\ndef build_prompt(record):\n    assessment = record[\"input\"][\"assessment\"].strip()\n    plan = record[\"input\"][\"plan\"].strip()\n    prompt = f\"Assessment: {assessment}\\nPlan: {plan}\\nRewrite the above for a patient with no medical background.\"\n    return prompt\n\nprompts = [build_prompt(rec) for rec in data]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T11:51:19.162887Z","iopub.execute_input":"2025-10-31T11:51:19.163172Z","iopub.status.idle":"2025-10-31T11:51:19.167902Z","shell.execute_reply.started":"2025-10-31T11:51:19.163150Z","shell.execute_reply":"2025-10-31T11:51:19.167268Z"}},"outputs":[],"execution_count":41},{"cell_type":"markdown","source":"### batch inference","metadata":{}},{"cell_type":"code","source":"BATCH_SIZE = 1\nMAX_NEW_TOKENS = 256\n\ndef batch_infer(model, tokenizer, prompts, batch_size=BATCH_SIZE):\n    \n    preds = []\n    for i in range(0, len(prompts), batch_size):\n        #print(\"next batch called\")\n        batch_prompts = prompts[i:i+batch_size]\n        tokens = tokenizer(batch_prompts, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n        with torch.no_grad():\n            output = model.generate(**tokens, max_new_tokens=MAX_NEW_TOKENS)\n        preds.extend(tokenizer.batch_decode(output, skip_special_tokens=True))\n    return preds\n\npredictions_raw = batch_infer(model, tokenizer, prompts)\npredictions = []\nREWRITE_CUE = 'Rewrite the above for a patient with no medical background.'\n\nfor prompt, output in zip(prompts[:100], predictions_raw):\t\n\n    if REWRITE_CUE in output:\n        prediction = output.split(REWRITE_CUE, 1)[-1].strip()\n    else:\n    # Fallback: try removing the whole prompt, but be careful\n        prediction = output[len(prompt):].strip() if output.startswith(prompt) else output.strip()\n    # If prediction is still blank after these checks, log it explicitly for debugging\n    if not prediction:\n        print('Blank prediction after stripping:', repr(output[:80]))    \n    \n    predictions.append(pred)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T12:37:25.839964Z","iopub.execute_input":"2025-10-31T12:37:25.840835Z","iopub.status.idle":"2025-10-31T12:43:42.173649Z","shell.execute_reply.started":"2025-10-31T12:37:25.840807Z","shell.execute_reply":"2025-10-31T12:43:42.172900Z"}},"outputs":[],"execution_count":64},{"cell_type":"code","source":"predictions[50]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T12:52:29.327392Z","iopub.execute_input":"2025-10-31T12:52:29.328057Z","iopub.status.idle":"2025-10-31T12:52:29.333476Z","shell.execute_reply.started":"2025-10-31T12:52:29.328029Z","shell.execute_reply":"2025-10-31T12:52:29.332814Z"}},"outputs":[{"execution_count":65,"output_type":"execute_result","data":{"text/plain":"\"Dear Patient,\\n\\nBased on your symptoms, it seems that you may be experiencing nausea and vomiting. This is a common side effect of some medications, especially those that are used to treat conditions like ulcerative colitis.\\n\\nTo help relieve your symptoms, we recommend taking an analgesic like ibuprofen or if severe pain, a combination of diclofenac and paracetamol. This will help reduce your discomfort and make it easier to manage.\\n\\nTo further help, we suggest taking a pain reliever like diclofenac and paracetamol, as well as a medication called omeprazole and pantoprazole, which can help prevent stomach acid from causing nausea.\\n\\nWe also recommend taking a medication called risperidone to help manage your symptoms. This will help reduce your anxiety and help you feel more relaxed.\\n\\nIf you have any questions or concerns, please don't hesitate to ask. We're here to help you through this.\\n\\nBest regards,\\n[Your Name]\""},"metadata":{}}],"execution_count":65},{"cell_type":"markdown","source":"### save the predictions (and reference) for metrics","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ndf = pd.DataFrame({\n    \"prompt\": prompts,\n    \"reference\": [rec[\"output\"] for rec in data,\n    \"prediction\": predictions\n})\ndf.to_csv(\"student_model_predictions.csv\", index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T12:52:38.856266Z","iopub.execute_input":"2025-10-31T12:52:38.857027Z","iopub.status.idle":"2025-10-31T12:52:38.868149Z","shell.execute_reply.started":"2025-10-31T12:52:38.857002Z","shell.execute_reply":"2025-10-31T12:52:38.867441Z"}},"outputs":[],"execution_count":66},{"cell_type":"code","source":"!pip install evaluate bert_score rouge_score\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T12:17:36.214940Z","iopub.execute_input":"2025-10-31T12:17:36.215230Z","iopub.status.idle":"2025-10-31T12:17:41.774626Z","shell.execute_reply.started":"2025-10-31T12:17:36.215208Z","shell.execute_reply":"2025-10-31T12:17:41.773664Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.6)\nRequirement already satisfied: bert_score in /usr/local/lib/python3.11/dist-packages (0.3.13)\nCollecting rouge_score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.6.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.3)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.31.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (25.0)\nRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from bert_score) (2.6.0+cu124)\nRequirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from bert_score) (4.51.3)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from bert_score) (3.7.2)\nRequirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge_score) (3.9.1)\nRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (19.0.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.11.18)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.13.2)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.4.26)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.0.0->bert_score) (1.3.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (0.21.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (0.5.3)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (4.57.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (1.4.8)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (11.1.0)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (3.0.9)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (8.1.8)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (1.5.0)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.6.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.4.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.0.0->bert_score) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->evaluate) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->evaluate) (2024.2.0)\nBuilding wheels for collected packages: rouge_score\n  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=4acbd988d3bccd17ec2ba66f75eeb6d2e07fec796957152f5b4b53c7210ffa58\n  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\nSuccessfully built rouge_score\nInstalling collected packages: rouge_score\nSuccessfully installed rouge_score-0.1.2\n","output_type":"stream"}],"execution_count":56},{"cell_type":"markdown","source":"## Eval and metrics","metadata":{}},{"cell_type":"markdown","source":"### first check for empty predictions","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\nresults = pd.read_csv(\"student_model_predictions.csv\")\n\n# Identify NaN predictions  - there are indeed quite a few...\nnan_mask = results[\"prediction\"].isna()\nnum_nan = nan_mask.sum()\ntotal = len(results)\nprint(f\"NaN (missing) predictions: {num_nan} / {total} ({num_nan/total:.1%})\")\n\n# Mark, but keep in dataset for full analysis\nresults[\"is_nan\"] = nan_mask\n\n# Drop NaNs before metric calculation\nnon_nan_results = results[~nan_mask]\npredictions = non_nan_results[\"prediction\"].tolist()\nreferences = non_nan_results[\"reference\"].tolist()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T12:52:47.363943Z","iopub.execute_input":"2025-10-31T12:52:47.364211Z","iopub.status.idle":"2025-10-31T12:52:47.375284Z","shell.execute_reply.started":"2025-10-31T12:52:47.364193Z","shell.execute_reply":"2025-10-31T12:52:47.374641Z"}},"outputs":[{"name":"stdout","text":"NaN (missing) predictions: 37 / 100 (37.0%)\n","output_type":"stream"}],"execution_count":67},{"cell_type":"markdown","source":"### Do simple metrics","metadata":{}},{"cell_type":"code","source":"from evaluate import load\nrouge = load(\"rouge\")\nbertscore = load(\"bertscore\")\n\nif len(predictions) > 0:\n    rouge_scores = rouge.compute(predictions=predictions, references=references)\n    bertscore_out = bertscore.compute(predictions=predictions, references=references, lang=\"en\")\n    print(\"ROUGE-L:\", rouge_scores[\"rougeL\"])\n    print(\"Mean BERTScore F1:\", sum(bertscore_out['f1']) / len(bertscore_out['f1']))\nelse:\n    print(\"No non-blank predictions to score.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T12:29:32.457855Z","iopub.execute_input":"2025-10-31T12:29:32.458433Z","iopub.status.idle":"2025-10-31T12:29:47.985089Z","shell.execute_reply.started":"2025-10-31T12:29:32.458409Z","shell.execute_reply":"2025-10-31T12:29:47.984288Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc4a598b7c6a487b9b5d7754743ec8a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8fd2a1d07a85465b8b97a3d2cee6aa30"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4497d600cae4d11bf8ea56b9d4e05a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"71b6c65944e546cb9d4cd913066450ed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4bcbd136fe4848f89610373e46ec462f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e33933b03aeb4738ac3ba4c75a64c977"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"ROUGE-L: 0.26569021636327045\nMean BERTScore F1: 0.8856479959054426\n","output_type":"stream"}],"execution_count":62},{"cell_type":"markdown","source":"#### perplexity's comments on the metrics!","metadata":{}},{"cell_type":"markdown","source":"Interpreting  Model's Evaluation Results\nLet's walk through how you might interpret these results, especially for an academic healthcare NLP project:\n\nROUGE-L: 0.266\nModerate recall for medical summarization. For abstractive summaries of clinical notes, a ROUGE-L score of about 0.27 suggests your model is picking up relevant sequences but isn't perfectly aligned with reference summaries.\n\nTypical range for strong models in medical summarization can be 0.3‚Äì0.4+, though this depends on dataset complexity and how much reference and prediction texts diverge. This means your model performs reasonably but could benefit from further tuning.\n\nMean BERTScore F1: 0.89\nHigh semantic similarity! This score is strong‚ÄîBERTScore captures meaning rather than just word overlap, and scores above 0.85 typically suggest outputs remain faithful in meaning to the references, even if phrasing differs.\n\nEven with a moderate ROUGE-L, a high BERTScore suggests the model paraphrases well, retaining important clinical concepts and descriptions.\n\n56% NaN (Missing Predictions)\nThis is concerning. More than half your samples resulted in missing predictions, which is not typical. In practice, you'd want this to be under 5‚Äì10%.\n\nCommon causes:\n\nModel or tokenization errors on certain prompts.\n\nVery long inputs or aggressive truncation.\n\nModel or hardware resource limits (memory, timeouts).\n\nBugs in inference (e.g., some prompts producing no output, or mishandling the results after generation).\n\nNext Action: Investigate why predictions are missing. Check logs, input formatting, and try generating a small batch of the problematic prompts directly to debug.\n\nIf you fix this, you'll likely see your ROUGE-L and BERTScore change‚Äîin particular, removing missing outputs will give you metrics that better reflect overall quality.\n\nQuick Summary\nBERTScore F1 (0.89): Semantic meaning is well preserved where the model produces output.\n\nROUGE-L (0.27): Linguistic overlap is moderate‚Äîparaphrasing or missing references may account for the gap.\n\nNaN rate (56%): Suggests the evaluation is incomplete; addressing this will be your next critical step.\n","metadata":{}},{"cell_type":"markdown","source":"## Reference code - for later usage","metadata":{}},{"cell_type":"code","source":"# One set of evaluation...\n\n\n# ============================================================================\n# COMPREHENSIVE EVALUATION FRAMEWORK FOR TEACHER-STUDENT MEDICAL NLP MODEL\n# ============================================================================\n# This framework evaluates a TinyLlama student model fine-tuned with LoRA\n# for converting clinical SOAP notes to patient-friendly summaries\n\nimport torch\nimport numpy as np\nfrom typing import Dict, List, Tuple\nfrom dataclasses import dataclass\nimport json\nfrom datetime import datetime\n\n# Required libraries\n# pip install rouge-score\n# pip install bert-score\n# pip install evaluate\n# pip install transformers\n# pip install torch\n\nfrom rouge_score import rouge_scorer\nfrom bert_score import score as bert_score\nimport evaluate\n\n# ============================================================================\n# 1. AUTOMATED METRICS CLASS\n# ============================================================================\n\n@dataclass\nclass AutomatedMetrics:\n    \"\"\"Compute automated evaluation metrics for text generation tasks\"\"\"\n\n    def __init__(self, use_stemmer: bool = True):\n        self.rouge_scorer = rouge_scorer.RougeScorer(\n            ['rouge1', 'rouge2', 'rougeL'], \n            use_stemmer=use_stemmer\n        )\n        self.perplexity_metric = evaluate.load(\"perplexity\", module_type=\"metric\")\n        self.bleu_metric = evaluate.load(\"bleu\")\n\n    def compute_rouge(self, generated: str, reference: str) -> Dict[str, float]:\n        \"\"\"\n        Compute ROUGE scores (F1-score focused)\n        Best for: Medical note summarization tasks\n\n        ROUGE-1: Unigram overlap (individual words)\n        ROUGE-2: Bigram overlap (word pairs)  \n        ROUGE-L: Longest common subsequence (semantic coherence)\n        \"\"\"\n        scores = self.rouge_scorer.score(reference, generated)\n\n        return {\n            'rouge1_f1': scores['rouge1'].fmeasure,\n            'rouge2_f1': scores['rouge2'].fmeasure,\n            'rougeL_f1': scores['rougeL'].fmeasure,\n       ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from rouge_score import rouge_scorer\nfrom sacrebleu import corpus_bleu\nfrom bert_score import score as bert_score_fn\nimport textstat\n\nrefs = [row[\"output\"] for row in train_data]\npreds = small_df[\"student_output\"].tolist()\n\nrouge = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\nr1, rL = [], []\nfor pred, ref in zip(preds, refs):\n    scores = rouge.score(str(pred), str(ref))\n    r1.append(scores[\"rouge1\"].fmeasure)\n    rL.append(scores[\"rougeL\"].fmeasure)\nprint(\"ROUGE-1 avg:\", sum(r1)/len(r1))\nprint(\"ROUGE-L avg:\", sum(rL)/len(rL))\nbleu = corpus_bleu(preds, [refs])\nprint(\"BLEU:\", bleu.score)\nP, R, F1 = bert_score_fn(preds, refs, lang=\"en\")\nprint(\"BERTScore F1 avg:\", F1.mean().item())\nread_ease = [textstat.flesch_reading_ease(txt) for txt in preds]\nfk_grade = [textstat.flesch_kincaid_grade(txt) for txt in preds]\nprint(\"Avg Flesch Ease:\", sum(read_ease)/len(read_ease))\nprint(\"Avg FK Grade:\", sum(fk_grade)/len(fk_grade))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T09:34:13.964665Z","iopub.status.idle":"2025-10-31T09:34:13.965010Z","shell.execute_reply.started":"2025-10-31T09:34:13.964852Z","shell.execute_reply":"2025-10-31T09:34:13.964866Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import bitsandbytes as bnb\nprint(bnb.__version__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T09:34:13.974389Z","iopub.status.idle":"2025-10-31T09:34:13.974709Z","shell.execute_reply.started":"2025-10-31T09:34:13.974546Z","shell.execute_reply":"2025-10-31T09:34:13.974559Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}